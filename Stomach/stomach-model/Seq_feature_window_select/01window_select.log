nohup: ignoring input
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Using TensorFlow backend.
样本个数 3154
样本个数 6308
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f24cb109350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f24cb109350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f2502bb0c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f2502bb0c10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24fa64d510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24fa64d510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24fa67be50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24fa67be50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24cb09bd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24cb09bd10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24fa75f210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24fa75f210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24cb0db990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24cb0db990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24fa6ef650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24fa6ef650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24caf8a450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24caf8a450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24cad40810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24cad40810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24fa67b510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24fa67b510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24caf8a1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24caf8a1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24caeb0a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24caeb0a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24caca2410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24caca2410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24caa59750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24caa59750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24cab83dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24cab83dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24cac969d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24cac969d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24caa98090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24caa98090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24c29a16d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24c29a16d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24c2918bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24c2918bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24c2871f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24c2871f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24c29a1d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24c29a1d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24c27441d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24c27441d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24c24e8150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24c24e8150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24c24197d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24c24197d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24c2588bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24c2588bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24c285dbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24c285dbd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24c24588d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24c24588d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24c21f2950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24c21f2950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24c24583d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24c24583d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24c2468ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24c2468ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24c21f2250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24c21f2250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24c1f2e550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24c1f2e550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24c1f51f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24c1f51f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24c1dc64d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24c1dc64d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24c220e0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24c220e0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24c1f2ef10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24c1f2ef10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f250cda1590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f250cda1590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24c1baf710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24c1baf710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24c1a69510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24c1a69510>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24c1ccab10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24c1ccab10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24c1bafe90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24c1bafe90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24c1f514d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24c1f514d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24c1b1e050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24c1b1e050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24b995b7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24b995b7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b97c9790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b97c9790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24b98b3690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24b98b3690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b99de150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b99de150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24b9594410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24b9594410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24b957dfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24b957dfd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b9503310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b9503310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24b9583510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24b9583510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b94654d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b94654d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24b922c3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24b922c3d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24b113f750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24b113f750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b92a39d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b92a39d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24b922ce10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24b922ce10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b117fcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b117fcd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24b0f26f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24b0f26f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24b11a1c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24b11a1c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b12130d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b12130d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24b0e22c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24b0e22c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b0cd3710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b0cd3710>>: AttributeError: module 'gast' has no attribute 'Str'
2022-11-28 22:30:08.920415: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2022-11-28 22:30:09.048982: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2400000000 Hz
2022-11-28 22:30:09.149690: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55f030d05590 executing computations on platform Host. Devices:
2022-11-28 22:30:09.149829: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2022-11-28 22:30:10.392432: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
01window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])
WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.

WARNING:tensorflow:From /home/lyli/miniconda3/envs/SeqPose/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 28:07 - loss: 0.8545 - acc: 0.4688
 128/5677 [..............................] - ETA: 20:18 - loss: 0.8850 - acc: 0.4609
 192/5677 [>.............................] - ETA: 16:11 - loss: 0.8169 - acc: 0.5208
 256/5677 [>.............................] - ETA: 14:27 - loss: 0.7857 - acc: 0.5312
 320/5677 [>.............................] - ETA: 13:03 - loss: 0.7910 - acc: 0.5281
 384/5677 [=>............................] - ETA: 11:56 - loss: 0.7957 - acc: 0.5260
 448/5677 [=>............................] - ETA: 11:15 - loss: 0.7785 - acc: 0.5357
 512/5677 [=>............................] - ETA: 10:56 - loss: 0.7674 - acc: 0.5293
 576/5677 [==>...........................] - ETA: 10:38 - loss: 0.7584 - acc: 0.5191
 640/5677 [==>...........................] - ETA: 10:08 - loss: 0.7627 - acc: 0.5047
 704/5677 [==>...........................] - ETA: 9:42 - loss: 0.7593 - acc: 0.5014 
 768/5677 [===>..........................] - ETA: 9:32 - loss: 0.7568 - acc: 0.5039
 832/5677 [===>..........................] - ETA: 9:17 - loss: 0.7548 - acc: 0.5036
 896/5677 [===>..........................] - ETA: 9:01 - loss: 0.7535 - acc: 0.5033
 960/5677 [====>.........................] - ETA: 8:42 - loss: 0.7506 - acc: 0.5052
1024/5677 [====>.........................] - ETA: 8:25 - loss: 0.7501 - acc: 0.5039
1088/5677 [====>.........................] - ETA: 8:09 - loss: 0.7489 - acc: 0.5037
1152/5677 [=====>........................] - ETA: 7:59 - loss: 0.7462 - acc: 0.5052
1216/5677 [=====>........................] - ETA: 7:51 - loss: 0.7435 - acc: 0.5066
1280/5677 [=====>........................] - ETA: 7:47 - loss: 0.7411 - acc: 0.5094
1344/5677 [======>.......................] - ETA: 7:38 - loss: 0.7409 - acc: 0.5082
1408/5677 [======>.......................] - ETA: 7:25 - loss: 0.7396 - acc: 0.5078
1472/5677 [======>.......................] - ETA: 7:13 - loss: 0.7411 - acc: 0.5061
1536/5677 [=======>......................] - ETA: 7:02 - loss: 0.7406 - acc: 0.5033
1600/5677 [=======>......................] - ETA: 6:50 - loss: 0.7390 - acc: 0.5031
1664/5677 [=======>......................] - ETA: 6:41 - loss: 0.7357 - acc: 0.5108
1728/5677 [========>.....................] - ETA: 6:36 - loss: 0.7372 - acc: 0.5081
1792/5677 [========>.....................] - ETA: 6:29 - loss: 0.7355 - acc: 0.5084
1856/5677 [========>.....................] - ETA: 6:22 - loss: 0.7360 - acc: 0.5070
1920/5677 [=========>....................] - ETA: 6:14 - loss: 0.7340 - acc: 0.5094
1984/5677 [=========>....................] - ETA: 6:07 - loss: 0.7345 - acc: 0.5081
2048/5677 [=========>....................] - ETA: 5:58 - loss: 0.7339 - acc: 0.5083
2112/5677 [==========>...................] - ETA: 5:49 - loss: 0.7327 - acc: 0.5076
2176/5677 [==========>...................] - ETA: 5:40 - loss: 0.7311 - acc: 0.5092
2240/5677 [==========>...................] - ETA: 5:32 - loss: 0.7300 - acc: 0.5094
2304/5677 [===========>..................] - ETA: 5:25 - loss: 0.7292 - acc: 0.5104
2368/5677 [===========>..................] - ETA: 5:19 - loss: 0.7297 - acc: 0.5093
2432/5677 [===========>..................] - ETA: 5:13 - loss: 0.7296 - acc: 0.5086
2496/5677 [============>.................] - ETA: 5:06 - loss: 0.7291 - acc: 0.5092
2560/5677 [============>.................] - ETA: 5:00 - loss: 0.7278 - acc: 0.5109
2624/5677 [============>.................] - ETA: 4:52 - loss: 0.7263 - acc: 0.5126
2688/5677 [=============>................] - ETA: 4:44 - loss: 0.7269 - acc: 0.5100
2752/5677 [=============>................] - ETA: 4:37 - loss: 0.7267 - acc: 0.5094
2816/5677 [=============>................] - ETA: 4:30 - loss: 0.7268 - acc: 0.5085
2880/5677 [==============>...............] - ETA: 4:23 - loss: 0.7266 - acc: 0.5073
2944/5677 [==============>...............] - ETA: 4:16 - loss: 0.7266 - acc: 0.5058
3008/5677 [==============>...............] - ETA: 4:10 - loss: 0.7259 - acc: 0.5053
3072/5677 [===============>..............] - ETA: 4:04 - loss: 0.7254 - acc: 0.5046
3136/5677 [===============>..............] - ETA: 3:58 - loss: 0.7256 - acc: 0.5029
3200/5677 [===============>..............] - ETA: 3:52 - loss: 0.7257 - acc: 0.5012
3264/5677 [================>.............] - ETA: 3:46 - loss: 0.7257 - acc: 0.5015
3328/5677 [================>.............] - ETA: 3:39 - loss: 0.7245 - acc: 0.5033
3392/5677 [================>.............] - ETA: 3:32 - loss: 0.7238 - acc: 0.5047
3456/5677 [=================>............] - ETA: 3:26 - loss: 0.7235 - acc: 0.5041
3520/5677 [=================>............] - ETA: 3:19 - loss: 0.7235 - acc: 0.5037
3584/5677 [=================>............] - ETA: 3:12 - loss: 0.7228 - acc: 0.5056
3648/5677 [==================>...........] - ETA: 3:07 - loss: 0.7220 - acc: 0.5058
3712/5677 [==================>...........] - ETA: 3:01 - loss: 0.7217 - acc: 0.5062
3776/5677 [==================>...........] - ETA: 2:55 - loss: 0.7208 - acc: 0.5074
3840/5677 [===================>..........] - ETA: 2:49 - loss: 0.7205 - acc: 0.5062
3904/5677 [===================>..........] - ETA: 2:43 - loss: 0.7197 - acc: 0.5079
3968/5677 [===================>..........] - ETA: 2:37 - loss: 0.7201 - acc: 0.5063
4032/5677 [====================>.........] - ETA: 2:31 - loss: 0.7196 - acc: 0.5067
4096/5677 [====================>.........] - ETA: 2:25 - loss: 0.7194 - acc: 0.5081
4160/5677 [====================>.........] - ETA: 2:19 - loss: 0.7196 - acc: 0.5070
4224/5677 [=====================>........] - ETA: 2:14 - loss: 0.7185 - acc: 0.5088
4288/5677 [=====================>........] - ETA: 2:08 - loss: 0.7179 - acc: 0.5091
4352/5677 [=====================>........] - ETA: 2:02 - loss: 0.7175 - acc: 0.5092
4416/5677 [======================>.......] - ETA: 1:56 - loss: 0.7174 - acc: 0.5102
4480/5677 [======================>.......] - ETA: 1:50 - loss: 0.7168 - acc: 0.5112
4544/5677 [=======================>......] - ETA: 1:45 - loss: 0.7165 - acc: 0.5117
4608/5677 [=======================>......] - ETA: 1:39 - loss: 0.7157 - acc: 0.5130
4672/5677 [=======================>......] - ETA: 1:33 - loss: 0.7160 - acc: 0.5128
4736/5677 [========================>.....] - ETA: 1:27 - loss: 0.7158 - acc: 0.5135
4800/5677 [========================>.....] - ETA: 1:21 - loss: 0.7154 - acc: 0.5144
4864/5677 [========================>.....] - ETA: 1:15 - loss: 0.7149 - acc: 0.5156
4928/5677 [=========================>....] - ETA: 1:09 - loss: 0.7154 - acc: 0.5152
4992/5677 [=========================>....] - ETA: 1:03 - loss: 0.7158 - acc: 0.5142
5056/5677 [=========================>....] - ETA: 57s - loss: 0.7152 - acc: 0.5150 
5120/5677 [==========================>...] - ETA: 51s - loss: 0.7155 - acc: 0.5143
5184/5677 [==========================>...] - ETA: 46s - loss: 0.7155 - acc: 0.5139
5248/5677 [==========================>...] - ETA: 39s - loss: 0.7148 - acc: 0.5154
5312/5677 [===========================>..] - ETA: 33s - loss: 0.7145 - acc: 0.5160
5376/5677 [===========================>..] - ETA: 27s - loss: 0.7141 - acc: 0.5166
5440/5677 [===========================>..] - ETA: 21s - loss: 0.7139 - acc: 0.5165
5504/5677 [============================>.] - ETA: 16s - loss: 0.7139 - acc: 0.5173
5568/5677 [============================>.] - ETA: 10s - loss: 0.7138 - acc: 0.5171
5632/5677 [============================>.] - ETA: 4s - loss: 0.7140 - acc: 0.5172 
5677/5677 [==============================] - 540s 95ms/step - loss: 0.7139 - acc: 0.5174 - val_loss: 0.6846 - val_acc: 0.5468

Epoch 00001: val_acc improved from -inf to 0.54675, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window01/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 6:22 - loss: 0.6973 - acc: 0.5625
 128/5677 [..............................] - ETA: 6:35 - loss: 0.7026 - acc: 0.4844
 192/5677 [>.............................] - ETA: 6:20 - loss: 0.6877 - acc: 0.5156
 256/5677 [>.............................] - ETA: 6:21 - loss: 0.6906 - acc: 0.5234
 320/5677 [>.............................] - ETA: 6:36 - loss: 0.6822 - acc: 0.5437
 384/5677 [=>............................] - ETA: 6:36 - loss: 0.6862 - acc: 0.5260
 448/5677 [=>............................] - ETA: 6:29 - loss: 0.6857 - acc: 0.5424
 512/5677 [=>............................] - ETA: 6:26 - loss: 0.6870 - acc: 0.5410
 576/5677 [==>...........................] - ETA: 6:21 - loss: 0.6906 - acc: 0.5330
 640/5677 [==>...........................] - ETA: 6:15 - loss: 0.6875 - acc: 0.5375
 704/5677 [==>...........................] - ETA: 6:10 - loss: 0.6939 - acc: 0.5241
 768/5677 [===>..........................] - ETA: 6:02 - loss: 0.6961 - acc: 0.5247
 832/5677 [===>..........................] - ETA: 5:58 - loss: 0.6972 - acc: 0.5252
 896/5677 [===>..........................] - ETA: 5:51 - loss: 0.6960 - acc: 0.5290
 960/5677 [====>.........................] - ETA: 5:47 - loss: 0.6945 - acc: 0.5365
1024/5677 [====>.........................] - ETA: 5:43 - loss: 0.6938 - acc: 0.5352
1088/5677 [====>.........................] - ETA: 5:39 - loss: 0.6920 - acc: 0.5386
1152/5677 [=====>........................] - ETA: 5:34 - loss: 0.6940 - acc: 0.5347
1216/5677 [=====>........................] - ETA: 5:30 - loss: 0.6929 - acc: 0.5337
1280/5677 [=====>........................] - ETA: 5:25 - loss: 0.6953 - acc: 0.5289
1344/5677 [======>.......................] - ETA: 5:21 - loss: 0.6963 - acc: 0.5275
1408/5677 [======>.......................] - ETA: 5:15 - loss: 0.6970 - acc: 0.5256
1472/5677 [======>.......................] - ETA: 5:09 - loss: 0.6974 - acc: 0.5251
1536/5677 [=======>......................] - ETA: 5:05 - loss: 0.6976 - acc: 0.5267
1600/5677 [=======>......................] - ETA: 5:02 - loss: 0.6980 - acc: 0.5250
1664/5677 [=======>......................] - ETA: 4:57 - loss: 0.6990 - acc: 0.5258
1728/5677 [========>.....................] - ETA: 4:52 - loss: 0.6987 - acc: 0.5266
1792/5677 [========>.....................] - ETA: 4:48 - loss: 0.6981 - acc: 0.5273
1856/5677 [========>.....................] - ETA: 4:43 - loss: 0.6981 - acc: 0.5248
1920/5677 [=========>....................] - ETA: 4:39 - loss: 0.6974 - acc: 0.5266
1984/5677 [=========>....................] - ETA: 4:34 - loss: 0.6968 - acc: 0.5297
2048/5677 [=========>....................] - ETA: 4:30 - loss: 0.6972 - acc: 0.5283
2112/5677 [==========>...................] - ETA: 4:25 - loss: 0.6967 - acc: 0.5279
2176/5677 [==========>...................] - ETA: 4:21 - loss: 0.6973 - acc: 0.5267
2240/5677 [==========>...................] - ETA: 4:16 - loss: 0.6971 - acc: 0.5254
2304/5677 [===========>..................] - ETA: 4:11 - loss: 0.6983 - acc: 0.5234
2368/5677 [===========>..................] - ETA: 4:06 - loss: 0.6982 - acc: 0.5215
2432/5677 [===========>..................] - ETA: 4:02 - loss: 0.6982 - acc: 0.5218
2496/5677 [============>.................] - ETA: 3:56 - loss: 0.6985 - acc: 0.5188
2560/5677 [============>.................] - ETA: 3:51 - loss: 0.6979 - acc: 0.5207
2624/5677 [============>.................] - ETA: 3:46 - loss: 0.6973 - acc: 0.5210
2688/5677 [=============>................] - ETA: 3:42 - loss: 0.6983 - acc: 0.5186
2752/5677 [=============>................] - ETA: 3:37 - loss: 0.6984 - acc: 0.5196
2816/5677 [=============>................] - ETA: 3:33 - loss: 0.6978 - acc: 0.5217
2880/5677 [==============>...............] - ETA: 3:28 - loss: 0.6973 - acc: 0.5240
2944/5677 [==============>...............] - ETA: 3:24 - loss: 0.6976 - acc: 0.5238
3008/5677 [==============>...............] - ETA: 3:19 - loss: 0.6968 - acc: 0.5253
3072/5677 [===============>..............] - ETA: 3:14 - loss: 0.6971 - acc: 0.5247
3136/5677 [===============>..............] - ETA: 3:09 - loss: 0.6972 - acc: 0.5242
3200/5677 [===============>..............] - ETA: 3:04 - loss: 0.6967 - acc: 0.5250
3264/5677 [================>.............] - ETA: 2:59 - loss: 0.6964 - acc: 0.5260
3328/5677 [================>.............] - ETA: 2:54 - loss: 0.6965 - acc: 0.5267
3392/5677 [================>.............] - ETA: 2:49 - loss: 0.6957 - acc: 0.5295
3456/5677 [=================>............] - ETA: 2:44 - loss: 0.6955 - acc: 0.5312
3520/5677 [=================>............] - ETA: 2:40 - loss: 0.6953 - acc: 0.5301
3584/5677 [=================>............] - ETA: 2:35 - loss: 0.6960 - acc: 0.5290
3648/5677 [==================>...........] - ETA: 2:30 - loss: 0.6958 - acc: 0.5293
3712/5677 [==================>...........] - ETA: 2:25 - loss: 0.6957 - acc: 0.5296
3776/5677 [==================>...........] - ETA: 2:20 - loss: 0.6953 - acc: 0.5297
3840/5677 [===================>..........] - ETA: 2:16 - loss: 0.6955 - acc: 0.5292
3904/5677 [===================>..........] - ETA: 2:11 - loss: 0.6956 - acc: 0.5292
3968/5677 [===================>..........] - ETA: 2:06 - loss: 0.6957 - acc: 0.5292
4032/5677 [====================>.........] - ETA: 2:01 - loss: 0.6954 - acc: 0.5295
4096/5677 [====================>.........] - ETA: 1:56 - loss: 0.6957 - acc: 0.5295
4160/5677 [====================>.........] - ETA: 1:52 - loss: 0.6957 - acc: 0.5296
4224/5677 [=====================>........] - ETA: 1:47 - loss: 0.6956 - acc: 0.5286
4288/5677 [=====================>........] - ETA: 1:42 - loss: 0.6955 - acc: 0.5275
4352/5677 [=====================>........] - ETA: 1:37 - loss: 0.6956 - acc: 0.5276
4416/5677 [======================>.......] - ETA: 1:33 - loss: 0.6952 - acc: 0.5281
4480/5677 [======================>.......] - ETA: 1:28 - loss: 0.6949 - acc: 0.5277
4544/5677 [=======================>......] - ETA: 1:23 - loss: 0.6950 - acc: 0.5284
4608/5677 [=======================>......] - ETA: 1:18 - loss: 0.6955 - acc: 0.5267
4672/5677 [=======================>......] - ETA: 1:14 - loss: 0.6961 - acc: 0.5265
4736/5677 [========================>.....] - ETA: 1:09 - loss: 0.6959 - acc: 0.5272
4800/5677 [========================>.....] - ETA: 1:04 - loss: 0.6957 - acc: 0.5275
4864/5677 [========================>.....] - ETA: 1:00 - loss: 0.6951 - acc: 0.5294
4928/5677 [=========================>....] - ETA: 55s - loss: 0.6954 - acc: 0.5278 
4992/5677 [=========================>....] - ETA: 50s - loss: 0.6955 - acc: 0.5270
5056/5677 [=========================>....] - ETA: 45s - loss: 0.6950 - acc: 0.5279
5120/5677 [==========================>...] - ETA: 41s - loss: 0.6947 - acc: 0.5283
5184/5677 [==========================>...] - ETA: 36s - loss: 0.6945 - acc: 0.5293
5248/5677 [==========================>...] - ETA: 31s - loss: 0.6945 - acc: 0.5305
5312/5677 [===========================>..] - ETA: 26s - loss: 0.6941 - acc: 0.5320
5376/5677 [===========================>..] - ETA: 22s - loss: 0.6944 - acc: 0.5320
5440/5677 [===========================>..] - ETA: 17s - loss: 0.6948 - acc: 0.5316
5504/5677 [============================>.] - ETA: 12s - loss: 0.6947 - acc: 0.5322
5568/5677 [============================>.] - ETA: 8s - loss: 0.6943 - acc: 0.5323 
5632/5677 [============================>.] - ETA: 3s - loss: 0.6944 - acc: 0.5321
5677/5677 [==============================] - 436s 77ms/step - loss: 0.6945 - acc: 0.5314 - val_loss: 0.6883 - val_acc: 0.5499

Epoch 00002: val_acc improved from 0.54675 to 0.54992, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window01/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 3/10

  64/5677 [..............................] - ETA: 6:57 - loss: 0.7399 - acc: 0.4531
 128/5677 [..............................] - ETA: 6:55 - loss: 0.7201 - acc: 0.4844
 192/5677 [>.............................] - ETA: 6:35 - loss: 0.7022 - acc: 0.4896
 256/5677 [>.............................] - ETA: 6:27 - loss: 0.7028 - acc: 0.4883
 320/5677 [>.............................] - ETA: 6:25 - loss: 0.6985 - acc: 0.5188
 384/5677 [=>............................] - ETA: 6:19 - loss: 0.6980 - acc: 0.5104
 448/5677 [=>............................] - ETA: 6:13 - loss: 0.6930 - acc: 0.5268
 512/5677 [=>............................] - ETA: 6:03 - loss: 0.6891 - acc: 0.5352
 576/5677 [==>...........................] - ETA: 5:54 - loss: 0.6857 - acc: 0.5434
 640/5677 [==>...........................] - ETA: 5:48 - loss: 0.6807 - acc: 0.5516
 704/5677 [==>...........................] - ETA: 5:44 - loss: 0.6857 - acc: 0.5384
 768/5677 [===>..........................] - ETA: 5:38 - loss: 0.6849 - acc: 0.5443
 832/5677 [===>..........................] - ETA: 5:34 - loss: 0.6847 - acc: 0.5481
 896/5677 [===>..........................] - ETA: 5:31 - loss: 0.6831 - acc: 0.5513
 960/5677 [====>.........................] - ETA: 5:26 - loss: 0.6845 - acc: 0.5521
1024/5677 [====>.........................] - ETA: 5:27 - loss: 0.6853 - acc: 0.5498
1088/5677 [====>.........................] - ETA: 5:24 - loss: 0.6872 - acc: 0.5441
1152/5677 [=====>........................] - ETA: 5:20 - loss: 0.6865 - acc: 0.5477
1216/5677 [=====>........................] - ETA: 5:15 - loss: 0.6887 - acc: 0.5428
1280/5677 [=====>........................] - ETA: 5:12 - loss: 0.6900 - acc: 0.5383
1344/5677 [======>.......................] - ETA: 5:07 - loss: 0.6899 - acc: 0.5379
1408/5677 [======>.......................] - ETA: 5:03 - loss: 0.6913 - acc: 0.5348
1472/5677 [======>.......................] - ETA: 4:58 - loss: 0.6902 - acc: 0.5387
1536/5677 [=======>......................] - ETA: 4:53 - loss: 0.6885 - acc: 0.5417
1600/5677 [=======>......................] - ETA: 4:49 - loss: 0.6907 - acc: 0.5375
1664/5677 [=======>......................] - ETA: 4:44 - loss: 0.6912 - acc: 0.5373
1728/5677 [========>.....................] - ETA: 4:40 - loss: 0.6919 - acc: 0.5370
1792/5677 [========>.....................] - ETA: 4:35 - loss: 0.6923 - acc: 0.5368
1856/5677 [========>.....................] - ETA: 4:30 - loss: 0.6926 - acc: 0.5361
1920/5677 [=========>....................] - ETA: 4:26 - loss: 0.6918 - acc: 0.5370
1984/5677 [=========>....................] - ETA: 4:22 - loss: 0.6916 - acc: 0.5398
2048/5677 [=========>....................] - ETA: 4:17 - loss: 0.6918 - acc: 0.5405
2112/5677 [==========>...................] - ETA: 4:12 - loss: 0.6925 - acc: 0.5407
2176/5677 [==========>...................] - ETA: 4:07 - loss: 0.6917 - acc: 0.5409
2240/5677 [==========>...................] - ETA: 4:03 - loss: 0.6910 - acc: 0.5429
2304/5677 [===========>..................] - ETA: 3:58 - loss: 0.6914 - acc: 0.5421
2368/5677 [===========>..................] - ETA: 3:54 - loss: 0.6905 - acc: 0.5427
2432/5677 [===========>..................] - ETA: 3:49 - loss: 0.6900 - acc: 0.5440
2496/5677 [============>.................] - ETA: 3:44 - loss: 0.6901 - acc: 0.5445
2560/5677 [============>.................] - ETA: 3:40 - loss: 0.6900 - acc: 0.5449
2624/5677 [============>.................] - ETA: 3:35 - loss: 0.6905 - acc: 0.5446
2688/5677 [=============>................] - ETA: 3:31 - loss: 0.6903 - acc: 0.5443
2752/5677 [=============>................] - ETA: 3:26 - loss: 0.6897 - acc: 0.5443
2816/5677 [=============>................] - ETA: 3:22 - loss: 0.6898 - acc: 0.5440
2880/5677 [==============>...............] - ETA: 3:18 - loss: 0.6899 - acc: 0.5417
2944/5677 [==============>...............] - ETA: 3:13 - loss: 0.6893 - acc: 0.5428
3008/5677 [==============>...............] - ETA: 3:08 - loss: 0.6897 - acc: 0.5429
3072/5677 [===============>..............] - ETA: 3:04 - loss: 0.6903 - acc: 0.5417
3136/5677 [===============>..............] - ETA: 2:59 - loss: 0.6908 - acc: 0.5402
3200/5677 [===============>..............] - ETA: 2:55 - loss: 0.6910 - acc: 0.5394
3264/5677 [================>.............] - ETA: 2:50 - loss: 0.6920 - acc: 0.5371
3328/5677 [================>.............] - ETA: 2:46 - loss: 0.6917 - acc: 0.5379
3392/5677 [================>.............] - ETA: 2:41 - loss: 0.6915 - acc: 0.5380
3456/5677 [=================>............] - ETA: 2:37 - loss: 0.6925 - acc: 0.5347
3520/5677 [=================>............] - ETA: 2:32 - loss: 0.6920 - acc: 0.5358
3584/5677 [=================>............] - ETA: 2:28 - loss: 0.6913 - acc: 0.5360
3648/5677 [==================>...........] - ETA: 2:24 - loss: 0.6916 - acc: 0.5356
3712/5677 [==================>...........] - ETA: 2:19 - loss: 0.6917 - acc: 0.5345
3776/5677 [==================>...........] - ETA: 2:15 - loss: 0.6915 - acc: 0.5355
3840/5677 [===================>..........] - ETA: 2:10 - loss: 0.6913 - acc: 0.5349
3904/5677 [===================>..........] - ETA: 2:05 - loss: 0.6909 - acc: 0.5361
3968/5677 [===================>..........] - ETA: 2:01 - loss: 0.6914 - acc: 0.5350
4032/5677 [====================>.........] - ETA: 1:56 - loss: 0.6916 - acc: 0.5347
4096/5677 [====================>.........] - ETA: 1:52 - loss: 0.6913 - acc: 0.5364
4160/5677 [====================>.........] - ETA: 1:47 - loss: 0.6909 - acc: 0.5368
4224/5677 [=====================>........] - ETA: 1:43 - loss: 0.6905 - acc: 0.5379
4288/5677 [=====================>........] - ETA: 1:38 - loss: 0.6907 - acc: 0.5371
4352/5677 [=====================>........] - ETA: 1:34 - loss: 0.6907 - acc: 0.5370
4416/5677 [======================>.......] - ETA: 1:29 - loss: 0.6906 - acc: 0.5380
4480/5677 [======================>.......] - ETA: 1:24 - loss: 0.6903 - acc: 0.5384
4544/5677 [=======================>......] - ETA: 1:20 - loss: 0.6907 - acc: 0.5370
4608/5677 [=======================>......] - ETA: 1:15 - loss: 0.6902 - acc: 0.5384
4672/5677 [=======================>......] - ETA: 1:11 - loss: 0.6896 - acc: 0.5392
4736/5677 [========================>.....] - ETA: 1:06 - loss: 0.6888 - acc: 0.5405
4800/5677 [========================>.....] - ETA: 1:01 - loss: 0.6889 - acc: 0.5404
4864/5677 [========================>.....] - ETA: 57s - loss: 0.6891 - acc: 0.5411 
4928/5677 [=========================>....] - ETA: 52s - loss: 0.6891 - acc: 0.5408
4992/5677 [=========================>....] - ETA: 48s - loss: 0.6887 - acc: 0.5417
5056/5677 [=========================>....] - ETA: 43s - loss: 0.6886 - acc: 0.5419
5120/5677 [==========================>...] - ETA: 39s - loss: 0.6883 - acc: 0.5434
5184/5677 [==========================>...] - ETA: 34s - loss: 0.6880 - acc: 0.5442
5248/5677 [==========================>...] - ETA: 30s - loss: 0.6883 - acc: 0.5431
5312/5677 [===========================>..] - ETA: 25s - loss: 0.6885 - acc: 0.5427
5376/5677 [===========================>..] - ETA: 21s - loss: 0.6886 - acc: 0.5426
5440/5677 [===========================>..] - ETA: 16s - loss: 0.6888 - acc: 0.5432
5504/5677 [============================>.] - ETA: 12s - loss: 0.6889 - acc: 0.5429
5568/5677 [============================>.] - ETA: 7s - loss: 0.6886 - acc: 0.5435 
5632/5677 [============================>.] - ETA: 3s - loss: 0.6887 - acc: 0.5437
5677/5677 [==============================] - 419s 74ms/step - loss: 0.6883 - acc: 0.5445 - val_loss: 0.6772 - val_acc: 0.5737

Epoch 00003: val_acc improved from 0.54992 to 0.57369, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window01/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 4/10

  64/5677 [..............................] - ETA: 8:07 - loss: 0.6852 - acc: 0.5938
 128/5677 [..............................] - ETA: 7:55 - loss: 0.6615 - acc: 0.6016
 192/5677 [>.............................] - ETA: 7:37 - loss: 0.6805 - acc: 0.5625
 256/5677 [>.............................] - ETA: 7:33 - loss: 0.6858 - acc: 0.5547
 320/5677 [>.............................] - ETA: 7:25 - loss: 0.6814 - acc: 0.5594
 384/5677 [=>............................] - ETA: 7:25 - loss: 0.6879 - acc: 0.5521
 448/5677 [=>............................] - ETA: 7:17 - loss: 0.6841 - acc: 0.5737
 512/5677 [=>............................] - ETA: 7:17 - loss: 0.6803 - acc: 0.5781
 576/5677 [==>...........................] - ETA: 7:14 - loss: 0.6794 - acc: 0.5781
 640/5677 [==>...........................] - ETA: 7:10 - loss: 0.6793 - acc: 0.5828
 704/5677 [==>...........................] - ETA: 7:10 - loss: 0.6794 - acc: 0.5810
 768/5677 [===>..........................] - ETA: 7:10 - loss: 0.6787 - acc: 0.5846
 832/5677 [===>..........................] - ETA: 7:05 - loss: 0.6817 - acc: 0.5817
 896/5677 [===>..........................] - ETA: 7:03 - loss: 0.6859 - acc: 0.5692
 960/5677 [====>.........................] - ETA: 7:01 - loss: 0.6844 - acc: 0.5719
1024/5677 [====>.........................] - ETA: 6:55 - loss: 0.6851 - acc: 0.5723
1088/5677 [====>.........................] - ETA: 6:56 - loss: 0.6841 - acc: 0.5699
1152/5677 [=====>........................] - ETA: 6:49 - loss: 0.6831 - acc: 0.5729
1216/5677 [=====>........................] - ETA: 6:46 - loss: 0.6836 - acc: 0.5683
1280/5677 [=====>........................] - ETA: 6:40 - loss: 0.6850 - acc: 0.5648
1344/5677 [======>.......................] - ETA: 6:33 - loss: 0.6841 - acc: 0.5647
1408/5677 [======>.......................] - ETA: 6:29 - loss: 0.6829 - acc: 0.5661
1472/5677 [======>.......................] - ETA: 6:23 - loss: 0.6836 - acc: 0.5652
1536/5677 [=======>......................] - ETA: 6:16 - loss: 0.6838 - acc: 0.5645
1600/5677 [=======>......................] - ETA: 6:11 - loss: 0.6842 - acc: 0.5625
1664/5677 [=======>......................] - ETA: 6:06 - loss: 0.6841 - acc: 0.5625
1728/5677 [========>.....................] - ETA: 5:58 - loss: 0.6833 - acc: 0.5637
1792/5677 [========>.....................] - ETA: 5:52 - loss: 0.6830 - acc: 0.5647
1856/5677 [========>.....................] - ETA: 5:46 - loss: 0.6832 - acc: 0.5641
1920/5677 [=========>....................] - ETA: 5:45 - loss: 0.6842 - acc: 0.5599
1984/5677 [=========>....................] - ETA: 5:39 - loss: 0.6845 - acc: 0.5585
2048/5677 [=========>....................] - ETA: 5:33 - loss: 0.6845 - acc: 0.5596
2112/5677 [==========>...................] - ETA: 5:26 - loss: 0.6839 - acc: 0.5611
2176/5677 [==========>...................] - ETA: 5:19 - loss: 0.6835 - acc: 0.5611
2240/5677 [==========>...................] - ETA: 5:16 - loss: 0.6826 - acc: 0.5647
2304/5677 [===========>..................] - ETA: 5:11 - loss: 0.6819 - acc: 0.5664
2368/5677 [===========>..................] - ETA: 5:05 - loss: 0.6835 - acc: 0.5629
2432/5677 [===========>..................] - ETA: 4:58 - loss: 0.6828 - acc: 0.5637
2496/5677 [============>.................] - ETA: 4:52 - loss: 0.6821 - acc: 0.5645
2560/5677 [============>.................] - ETA: 4:46 - loss: 0.6830 - acc: 0.5629
2624/5677 [============>.................] - ETA: 4:40 - loss: 0.6826 - acc: 0.5636
2688/5677 [=============>................] - ETA: 4:35 - loss: 0.6830 - acc: 0.5636
2752/5677 [=============>................] - ETA: 4:30 - loss: 0.6822 - acc: 0.5658
2816/5677 [=============>................] - ETA: 4:24 - loss: 0.6830 - acc: 0.5646
2880/5677 [==============>...............] - ETA: 4:18 - loss: 0.6832 - acc: 0.5639
2944/5677 [==============>...............] - ETA: 4:12 - loss: 0.6831 - acc: 0.5635
3008/5677 [==============>...............] - ETA: 4:05 - loss: 0.6823 - acc: 0.5662
3072/5677 [===============>..............] - ETA: 3:59 - loss: 0.6831 - acc: 0.5648
3136/5677 [===============>..............] - ETA: 3:55 - loss: 0.6829 - acc: 0.5654
3200/5677 [===============>..............] - ETA: 3:49 - loss: 0.6827 - acc: 0.5650
3264/5677 [================>.............] - ETA: 3:43 - loss: 0.6822 - acc: 0.5662
3328/5677 [================>.............] - ETA: 3:37 - loss: 0.6822 - acc: 0.5664
3392/5677 [================>.............] - ETA: 3:31 - loss: 0.6821 - acc: 0.5669
3456/5677 [=================>............] - ETA: 3:25 - loss: 0.6811 - acc: 0.5686
3520/5677 [=================>............] - ETA: 3:19 - loss: 0.6805 - acc: 0.5693
3584/5677 [=================>............] - ETA: 3:13 - loss: 0.6809 - acc: 0.5692
3648/5677 [==================>...........] - ETA: 3:07 - loss: 0.6805 - acc: 0.5691
3712/5677 [==================>...........] - ETA: 3:01 - loss: 0.6801 - acc: 0.5706
3776/5677 [==================>...........] - ETA: 2:56 - loss: 0.6800 - acc: 0.5720
3840/5677 [===================>..........] - ETA: 2:50 - loss: 0.6803 - acc: 0.5727
3904/5677 [===================>..........] - ETA: 2:44 - loss: 0.6799 - acc: 0.5733
3968/5677 [===================>..........] - ETA: 2:38 - loss: 0.6797 - acc: 0.5736
4032/5677 [====================>.........] - ETA: 2:32 - loss: 0.6800 - acc: 0.5724
4096/5677 [====================>.........] - ETA: 2:26 - loss: 0.6801 - acc: 0.5730
4160/5677 [====================>.........] - ETA: 2:19 - loss: 0.6797 - acc: 0.5731
4224/5677 [=====================>........] - ETA: 2:13 - loss: 0.6801 - acc: 0.5717
4288/5677 [=====================>........] - ETA: 2:07 - loss: 0.6793 - acc: 0.5728
4352/5677 [=====================>........] - ETA: 2:02 - loss: 0.6796 - acc: 0.5719
4416/5677 [======================>.......] - ETA: 1:56 - loss: 0.6798 - acc: 0.5713
4480/5677 [======================>.......] - ETA: 1:50 - loss: 0.6801 - acc: 0.5710
4544/5677 [=======================>......] - ETA: 1:44 - loss: 0.6807 - acc: 0.5695
4608/5677 [=======================>......] - ETA: 1:38 - loss: 0.6806 - acc: 0.5699
4672/5677 [=======================>......] - ETA: 1:32 - loss: 0.6815 - acc: 0.5687
4736/5677 [========================>.....] - ETA: 1:26 - loss: 0.6814 - acc: 0.5697
4800/5677 [========================>.....] - ETA: 1:20 - loss: 0.6815 - acc: 0.5694
4864/5677 [========================>.....] - ETA: 1:14 - loss: 0.6813 - acc: 0.5707
4928/5677 [=========================>....] - ETA: 1:08 - loss: 0.6813 - acc: 0.5702
4992/5677 [=========================>....] - ETA: 1:02 - loss: 0.6811 - acc: 0.5709
5056/5677 [=========================>....] - ETA: 57s - loss: 0.6815 - acc: 0.5696 
5120/5677 [==========================>...] - ETA: 51s - loss: 0.6820 - acc: 0.5689
5184/5677 [==========================>...] - ETA: 45s - loss: 0.6819 - acc: 0.5696
5248/5677 [==========================>...] - ETA: 39s - loss: 0.6818 - acc: 0.5699
5312/5677 [===========================>..] - ETA: 33s - loss: 0.6816 - acc: 0.5704
5376/5677 [===========================>..] - ETA: 27s - loss: 0.6819 - acc: 0.5703
5440/5677 [===========================>..] - ETA: 22s - loss: 0.6821 - acc: 0.5693
5504/5677 [============================>.] - ETA: 16s - loss: 0.6823 - acc: 0.5685
5568/5677 [============================>.] - ETA: 10s - loss: 0.6823 - acc: 0.5690
5632/5677 [============================>.] - ETA: 4s - loss: 0.6828 - acc: 0.5678 
5677/5677 [==============================] - 552s 97ms/step - loss: 0.6828 - acc: 0.5679 - val_loss: 0.6830 - val_acc: 0.5800

Epoch 00004: val_acc improved from 0.57369 to 0.58003, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window01/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 5/10

  64/5677 [..............................] - ETA: 8:50 - loss: 0.7119 - acc: 0.4844
 128/5677 [..............................] - ETA: 8:54 - loss: 0.6989 - acc: 0.5547
 192/5677 [>.............................] - ETA: 8:47 - loss: 0.6859 - acc: 0.5781
 256/5677 [>.............................] - ETA: 8:33 - loss: 0.6805 - acc: 0.5898
 320/5677 [>.............................] - ETA: 8:27 - loss: 0.6855 - acc: 0.5781
 384/5677 [=>............................] - ETA: 8:22 - loss: 0.6810 - acc: 0.5729
 448/5677 [=>............................] - ETA: 8:18 - loss: 0.6824 - acc: 0.5759
 512/5677 [=>............................] - ETA: 8:13 - loss: 0.6819 - acc: 0.5762
 576/5677 [==>...........................] - ETA: 8:05 - loss: 0.6781 - acc: 0.5833
 640/5677 [==>...........................] - ETA: 7:57 - loss: 0.6782 - acc: 0.5844
 704/5677 [==>...........................] - ETA: 7:49 - loss: 0.6801 - acc: 0.5795
 768/5677 [===>..........................] - ETA: 7:39 - loss: 0.6813 - acc: 0.5729
 832/5677 [===>..........................] - ETA: 7:33 - loss: 0.6831 - acc: 0.5673
 896/5677 [===>..........................] - ETA: 7:25 - loss: 0.6846 - acc: 0.5681
 960/5677 [====>.........................] - ETA: 7:18 - loss: 0.6843 - acc: 0.5677
1024/5677 [====>.........................] - ETA: 7:13 - loss: 0.6858 - acc: 0.5615
1088/5677 [====>.........................] - ETA: 7:07 - loss: 0.6854 - acc: 0.5643
1152/5677 [=====>........................] - ETA: 7:01 - loss: 0.6839 - acc: 0.5677
1216/5677 [=====>........................] - ETA: 6:54 - loss: 0.6858 - acc: 0.5600
1280/5677 [=====>........................] - ETA: 6:48 - loss: 0.6880 - acc: 0.5586
1344/5677 [======>.......................] - ETA: 6:39 - loss: 0.6874 - acc: 0.5625
1408/5677 [======>.......................] - ETA: 6:32 - loss: 0.6857 - acc: 0.5632
1472/5677 [======>.......................] - ETA: 6:24 - loss: 0.6854 - acc: 0.5618
1536/5677 [=======>......................] - ETA: 6:18 - loss: 0.6851 - acc: 0.5638
1600/5677 [=======>......................] - ETA: 6:15 - loss: 0.6848 - acc: 0.5644
1664/5677 [=======>......................] - ETA: 6:10 - loss: 0.6849 - acc: 0.5643
1728/5677 [========>.....................] - ETA: 6:04 - loss: 0.6850 - acc: 0.5637
1792/5677 [========>.....................] - ETA: 5:59 - loss: 0.6848 - acc: 0.5653
1856/5677 [========>.....................] - ETA: 5:53 - loss: 0.6847 - acc: 0.5625
1920/5677 [=========>....................] - ETA: 5:47 - loss: 0.6842 - acc: 0.5620
1984/5677 [=========>....................] - ETA: 5:40 - loss: 0.6845 - acc: 0.5640
2048/5677 [=========>....................] - ETA: 5:35 - loss: 0.6847 - acc: 0.5630
2112/5677 [==========>...................] - ETA: 5:29 - loss: 0.6839 - acc: 0.5677
2176/5677 [==========>...................] - ETA: 5:24 - loss: 0.6840 - acc: 0.5676
2240/5677 [==========>...................] - ETA: 5:19 - loss: 0.6837 - acc: 0.5656
2304/5677 [===========>..................] - ETA: 5:13 - loss: 0.6837 - acc: 0.5651
2368/5677 [===========>..................] - ETA: 5:08 - loss: 0.6830 - acc: 0.5676
2432/5677 [===========>..................] - ETA: 5:02 - loss: 0.6834 - acc: 0.5670
2496/5677 [============>.................] - ETA: 4:56 - loss: 0.6839 - acc: 0.5665
2560/5677 [============>.................] - ETA: 4:50 - loss: 0.6825 - acc: 0.5691
2624/5677 [============>.................] - ETA: 4:43 - loss: 0.6827 - acc: 0.5701
2688/5677 [=============>................] - ETA: 4:39 - loss: 0.6825 - acc: 0.5718
2752/5677 [=============>................] - ETA: 4:33 - loss: 0.6824 - acc: 0.5709
2816/5677 [=============>................] - ETA: 4:27 - loss: 0.6823 - acc: 0.5700
2880/5677 [==============>...............] - ETA: 4:21 - loss: 0.6827 - acc: 0.5691
2944/5677 [==============>...............] - ETA: 4:16 - loss: 0.6826 - acc: 0.5690
3008/5677 [==============>...............] - ETA: 4:11 - loss: 0.6812 - acc: 0.5728
3072/5677 [===============>..............] - ETA: 4:05 - loss: 0.6805 - acc: 0.5739
3136/5677 [===============>..............] - ETA: 3:59 - loss: 0.6799 - acc: 0.5746
3200/5677 [===============>..............] - ETA: 3:53 - loss: 0.6806 - acc: 0.5737
3264/5677 [================>.............] - ETA: 3:47 - loss: 0.6810 - acc: 0.5726
3328/5677 [================>.............] - ETA: 3:41 - loss: 0.6805 - acc: 0.5730
3392/5677 [================>.............] - ETA: 3:35 - loss: 0.6811 - acc: 0.5716
3456/5677 [=================>............] - ETA: 3:29 - loss: 0.6814 - acc: 0.5720
3520/5677 [=================>............] - ETA: 3:23 - loss: 0.6815 - acc: 0.5719
3584/5677 [=================>............] - ETA: 3:17 - loss: 0.6814 - acc: 0.5714
3648/5677 [==================>...........] - ETA: 3:11 - loss: 0.6817 - acc: 0.5713
3712/5677 [==================>...........] - ETA: 3:05 - loss: 0.6823 - acc: 0.5698
3776/5677 [==================>...........] - ETA: 2:58 - loss: 0.6830 - acc: 0.5678
3840/5677 [===================>..........] - ETA: 2:53 - loss: 0.6822 - acc: 0.5682
3904/5677 [===================>..........] - ETA: 2:47 - loss: 0.6823 - acc: 0.5674
3968/5677 [===================>..........] - ETA: 2:41 - loss: 0.6822 - acc: 0.5675
4032/5677 [====================>.........] - ETA: 2:35 - loss: 0.6835 - acc: 0.5650
4096/5677 [====================>.........] - ETA: 2:28 - loss: 0.6838 - acc: 0.5652
4160/5677 [====================>.........] - ETA: 2:22 - loss: 0.6833 - acc: 0.5671
4224/5677 [=====================>........] - ETA: 2:16 - loss: 0.6828 - acc: 0.5687
4288/5677 [=====================>........] - ETA: 2:10 - loss: 0.6828 - acc: 0.5679
4352/5677 [=====================>........] - ETA: 2:04 - loss: 0.6824 - acc: 0.5685
4416/5677 [======================>.......] - ETA: 1:58 - loss: 0.6823 - acc: 0.5684
4480/5677 [======================>.......] - ETA: 1:52 - loss: 0.6829 - acc: 0.5676
4544/5677 [=======================>......] - ETA: 1:46 - loss: 0.6832 - acc: 0.5667
4608/5677 [=======================>......] - ETA: 1:40 - loss: 0.6832 - acc: 0.5666
4672/5677 [=======================>......] - ETA: 1:34 - loss: 0.6833 - acc: 0.5657
4736/5677 [========================>.....] - ETA: 1:28 - loss: 0.6831 - acc: 0.5659
4800/5677 [========================>.....] - ETA: 1:22 - loss: 0.6830 - acc: 0.5654
4864/5677 [========================>.....] - ETA: 1:16 - loss: 0.6832 - acc: 0.5648
4928/5677 [=========================>....] - ETA: 1:10 - loss: 0.6838 - acc: 0.5631
4992/5677 [=========================>....] - ETA: 1:04 - loss: 0.6842 - acc: 0.5613
5056/5677 [=========================>....] - ETA: 58s - loss: 0.6841 - acc: 0.5617 
5120/5677 [==========================>...] - ETA: 52s - loss: 0.6842 - acc: 0.5609
5184/5677 [==========================>...] - ETA: 46s - loss: 0.6841 - acc: 0.5615
5248/5677 [==========================>...] - ETA: 40s - loss: 0.6841 - acc: 0.5612
5312/5677 [===========================>..] - ETA: 34s - loss: 0.6843 - acc: 0.5612
5376/5677 [===========================>..] - ETA: 28s - loss: 0.6842 - acc: 0.5618
5440/5677 [===========================>..] - ETA: 22s - loss: 0.6842 - acc: 0.5619
5504/5677 [============================>.] - ETA: 16s - loss: 0.6841 - acc: 0.5627
5568/5677 [============================>.] - ETA: 10s - loss: 0.6842 - acc: 0.5616
5632/5677 [============================>.] - ETA: 4s - loss: 0.6843 - acc: 0.5602 
5677/5677 [==============================] - 552s 97ms/step - loss: 0.6842 - acc: 0.5595 - val_loss: 0.6827 - val_acc: 0.5658

Epoch 00005: val_acc did not improve from 0.58003
Epoch 6/10

  64/5677 [..............................] - ETA: 8:54 - loss: 0.6733 - acc: 0.7188
 128/5677 [..............................] - ETA: 8:57 - loss: 0.6796 - acc: 0.6094
 192/5677 [>.............................] - ETA: 8:47 - loss: 0.6805 - acc: 0.5885
 256/5677 [>.............................] - ETA: 8:31 - loss: 0.6765 - acc: 0.5938
 320/5677 [>.............................] - ETA: 8:34 - loss: 0.6752 - acc: 0.6000
 384/5677 [=>............................] - ETA: 8:30 - loss: 0.6758 - acc: 0.5990
 448/5677 [=>............................] - ETA: 8:29 - loss: 0.6795 - acc: 0.5960
 512/5677 [=>............................] - ETA: 8:18 - loss: 0.6780 - acc: 0.5957
 576/5677 [==>...........................] - ETA: 8:08 - loss: 0.6764 - acc: 0.5972
 640/5677 [==>...........................] - ETA: 8:00 - loss: 0.6824 - acc: 0.5844
 704/5677 [==>...........................] - ETA: 7:51 - loss: 0.6871 - acc: 0.5767
 768/5677 [===>..........................] - ETA: 7:46 - loss: 0.6823 - acc: 0.5846
 832/5677 [===>..........................] - ETA: 7:38 - loss: 0.6796 - acc: 0.5853
 896/5677 [===>..........................] - ETA: 7:28 - loss: 0.6765 - acc: 0.5904
 960/5677 [====>.........................] - ETA: 7:23 - loss: 0.6733 - acc: 0.5958
1024/5677 [====>.........................] - ETA: 7:17 - loss: 0.6748 - acc: 0.5918
1088/5677 [====>.........................] - ETA: 7:10 - loss: 0.6732 - acc: 0.5947
1152/5677 [=====>........................] - ETA: 7:00 - loss: 0.6728 - acc: 0.5920
1216/5677 [=====>........................] - ETA: 6:54 - loss: 0.6706 - acc: 0.5938
1280/5677 [=====>........................] - ETA: 6:47 - loss: 0.6695 - acc: 0.5953
1344/5677 [======>.......................] - ETA: 6:41 - loss: 0.6715 - acc: 0.5938
1408/5677 [======>.......................] - ETA: 6:44 - loss: 0.6733 - acc: 0.5909
1472/5677 [======>.......................] - ETA: 6:42 - loss: 0.6749 - acc: 0.5870
1536/5677 [=======>......................] - ETA: 6:35 - loss: 0.6750 - acc: 0.5872
1600/5677 [=======>......................] - ETA: 6:26 - loss: 0.6761 - acc: 0.5856
1664/5677 [=======>......................] - ETA: 6:19 - loss: 0.6764 - acc: 0.5823
1728/5677 [========>.....................] - ETA: 6:10 - loss: 0.6778 - acc: 0.5799
1792/5677 [========>.....................] - ETA: 6:03 - loss: 0.6786 - acc: 0.5781
1856/5677 [========>.....................] - ETA: 5:56 - loss: 0.6778 - acc: 0.5814
1920/5677 [=========>....................] - ETA: 5:49 - loss: 0.6778 - acc: 0.5802
1984/5677 [=========>....................] - ETA: 5:42 - loss: 0.6761 - acc: 0.5832
2048/5677 [=========>....................] - ETA: 5:36 - loss: 0.6753 - acc: 0.5835
2112/5677 [==========>...................] - ETA: 5:29 - loss: 0.6770 - acc: 0.5800
2176/5677 [==========>...................] - ETA: 5:22 - loss: 0.6767 - acc: 0.5813
2240/5677 [==========>...................] - ETA: 5:16 - loss: 0.6775 - acc: 0.5804
2304/5677 [===========>..................] - ETA: 5:10 - loss: 0.6784 - acc: 0.5768
2368/5677 [===========>..................] - ETA: 5:04 - loss: 0.6788 - acc: 0.5752
2432/5677 [===========>..................] - ETA: 4:58 - loss: 0.6786 - acc: 0.5761
2496/5677 [============>.................] - ETA: 4:51 - loss: 0.6795 - acc: 0.5733
2560/5677 [============>.................] - ETA: 4:45 - loss: 0.6798 - acc: 0.5730
2624/5677 [============>.................] - ETA: 4:39 - loss: 0.6793 - acc: 0.5739
2688/5677 [=============>................] - ETA: 4:33 - loss: 0.6792 - acc: 0.5748
2752/5677 [=============>................] - ETA: 4:27 - loss: 0.6789 - acc: 0.5767
2816/5677 [=============>................] - ETA: 4:21 - loss: 0.6793 - acc: 0.5756
2880/5677 [==============>...............] - ETA: 4:14 - loss: 0.6798 - acc: 0.5747
2944/5677 [==============>...............] - ETA: 4:09 - loss: 0.6799 - acc: 0.5744
3008/5677 [==============>...............] - ETA: 4:02 - loss: 0.6800 - acc: 0.5755
3072/5677 [===============>..............] - ETA: 3:56 - loss: 0.6799 - acc: 0.5745
3136/5677 [===============>..............] - ETA: 3:50 - loss: 0.6806 - acc: 0.5724
3200/5677 [===============>..............] - ETA: 3:44 - loss: 0.6804 - acc: 0.5725
3264/5677 [================>.............] - ETA: 3:38 - loss: 0.6804 - acc: 0.5714
3328/5677 [================>.............] - ETA: 3:32 - loss: 0.6806 - acc: 0.5706
3392/5677 [================>.............] - ETA: 3:26 - loss: 0.6799 - acc: 0.5719
3456/5677 [=================>............] - ETA: 3:20 - loss: 0.6802 - acc: 0.5712
3520/5677 [=================>............] - ETA: 3:14 - loss: 0.6796 - acc: 0.5710
3584/5677 [=================>............] - ETA: 3:08 - loss: 0.6795 - acc: 0.5711
3648/5677 [==================>...........] - ETA: 3:02 - loss: 0.6792 - acc: 0.5718
3712/5677 [==================>...........] - ETA: 2:56 - loss: 0.6795 - acc: 0.5695
3776/5677 [==================>...........] - ETA: 2:51 - loss: 0.6804 - acc: 0.5670
3840/5677 [===================>..........] - ETA: 2:45 - loss: 0.6803 - acc: 0.5685
3904/5677 [===================>..........] - ETA: 2:39 - loss: 0.6806 - acc: 0.5684
3968/5677 [===================>..........] - ETA: 2:33 - loss: 0.6802 - acc: 0.5696
4032/5677 [====================>.........] - ETA: 2:27 - loss: 0.6803 - acc: 0.5697
4096/5677 [====================>.........] - ETA: 2:22 - loss: 0.6794 - acc: 0.5713
4160/5677 [====================>.........] - ETA: 2:16 - loss: 0.6796 - acc: 0.5709
4224/5677 [=====================>........] - ETA: 2:10 - loss: 0.6797 - acc: 0.5708
4288/5677 [=====================>........] - ETA: 2:04 - loss: 0.6802 - acc: 0.5697
4352/5677 [=====================>........] - ETA: 1:58 - loss: 0.6804 - acc: 0.5689
4416/5677 [======================>.......] - ETA: 1:52 - loss: 0.6810 - acc: 0.5682
4480/5677 [======================>.......] - ETA: 1:47 - loss: 0.6807 - acc: 0.5694
4544/5677 [=======================>......] - ETA: 1:41 - loss: 0.6807 - acc: 0.5698
4608/5677 [=======================>......] - ETA: 1:35 - loss: 0.6810 - acc: 0.5690
4672/5677 [=======================>......] - ETA: 1:29 - loss: 0.6816 - acc: 0.5681
4736/5677 [========================>.....] - ETA: 1:23 - loss: 0.6811 - acc: 0.5697
4800/5677 [========================>.....] - ETA: 1:18 - loss: 0.6812 - acc: 0.5690
4864/5677 [========================>.....] - ETA: 1:12 - loss: 0.6807 - acc: 0.5697
4928/5677 [=========================>....] - ETA: 1:06 - loss: 0.6807 - acc: 0.5694
4992/5677 [=========================>....] - ETA: 1:00 - loss: 0.6805 - acc: 0.5705
5056/5677 [=========================>....] - ETA: 55s - loss: 0.6805 - acc: 0.5704 
5120/5677 [==========================>...] - ETA: 49s - loss: 0.6809 - acc: 0.5697
5184/5677 [==========================>...] - ETA: 43s - loss: 0.6813 - acc: 0.5689
5248/5677 [==========================>...] - ETA: 38s - loss: 0.6816 - acc: 0.5671
5312/5677 [===========================>..] - ETA: 32s - loss: 0.6818 - acc: 0.5665
5376/5677 [===========================>..] - ETA: 26s - loss: 0.6814 - acc: 0.5672
5440/5677 [===========================>..] - ETA: 21s - loss: 0.6818 - acc: 0.5654
5504/5677 [============================>.] - ETA: 15s - loss: 0.6818 - acc: 0.5654
5568/5677 [============================>.] - ETA: 9s - loss: 0.6820 - acc: 0.5652 
5632/5677 [============================>.] - ETA: 4s - loss: 0.6821 - acc: 0.5653
5677/5677 [==============================] - 526s 93ms/step - loss: 0.6819 - acc: 0.5658 - val_loss: 0.6742 - val_acc: 0.5705

Epoch 00006: val_acc did not improve from 0.58003
Epoch 7/10

  64/5677 [..............................] - ETA: 7:54 - loss: 0.6944 - acc: 0.5156
 128/5677 [..............................] - ETA: 8:43 - loss: 0.6809 - acc: 0.5625
 192/5677 [>.............................] - ETA: 8:41 - loss: 0.6851 - acc: 0.5573
 256/5677 [>.............................] - ETA: 8:33 - loss: 0.6782 - acc: 0.5625
 320/5677 [>.............................] - ETA: 8:21 - loss: 0.6776 - acc: 0.5531
 384/5677 [=>............................] - ETA: 8:25 - loss: 0.6782 - acc: 0.5521
 448/5677 [=>............................] - ETA: 8:22 - loss: 0.6833 - acc: 0.5491
 512/5677 [=>............................] - ETA: 8:10 - loss: 0.6866 - acc: 0.5391
 576/5677 [==>...........................] - ETA: 7:59 - loss: 0.6869 - acc: 0.5399
 640/5677 [==>...........................] - ETA: 7:54 - loss: 0.6870 - acc: 0.5484
 704/5677 [==>...........................] - ETA: 7:56 - loss: 0.6862 - acc: 0.5511
 768/5677 [===>..........................] - ETA: 7:50 - loss: 0.6837 - acc: 0.5573
 832/5677 [===>..........................] - ETA: 7:42 - loss: 0.6803 - acc: 0.5661
 896/5677 [===>..........................] - ETA: 7:31 - loss: 0.6797 - acc: 0.5692
 960/5677 [====>.........................] - ETA: 7:24 - loss: 0.6830 - acc: 0.5635
1024/5677 [====>.........................] - ETA: 7:21 - loss: 0.6806 - acc: 0.5713
1088/5677 [====>.........................] - ETA: 7:18 - loss: 0.6809 - acc: 0.5708
1152/5677 [=====>........................] - ETA: 7:13 - loss: 0.6816 - acc: 0.5686
1216/5677 [=====>........................] - ETA: 7:06 - loss: 0.6812 - acc: 0.5715
1280/5677 [=====>........................] - ETA: 6:59 - loss: 0.6816 - acc: 0.5687
1344/5677 [======>.......................] - ETA: 6:51 - loss: 0.6806 - acc: 0.5707
1408/5677 [======>.......................] - ETA: 6:43 - loss: 0.6797 - acc: 0.5717
1472/5677 [======>.......................] - ETA: 6:37 - loss: 0.6788 - acc: 0.5727
1536/5677 [=======>......................] - ETA: 6:32 - loss: 0.6791 - acc: 0.5716
1600/5677 [=======>......................] - ETA: 6:27 - loss: 0.6800 - acc: 0.5700
1664/5677 [=======>......................] - ETA: 6:22 - loss: 0.6788 - acc: 0.5709
1728/5677 [========>.....................] - ETA: 6:17 - loss: 0.6787 - acc: 0.5706
1792/5677 [========>.....................] - ETA: 6:11 - loss: 0.6783 - acc: 0.5731
1856/5677 [========>.....................] - ETA: 6:03 - loss: 0.6786 - acc: 0.5727
1920/5677 [=========>....................] - ETA: 5:56 - loss: 0.6779 - acc: 0.5745
1984/5677 [=========>....................] - ETA: 5:49 - loss: 0.6782 - acc: 0.5741
2048/5677 [=========>....................] - ETA: 5:43 - loss: 0.6782 - acc: 0.5747
2112/5677 [==========>...................] - ETA: 5:40 - loss: 0.6779 - acc: 0.5748
2176/5677 [==========>...................] - ETA: 5:34 - loss: 0.6773 - acc: 0.5758
2240/5677 [==========>...................] - ETA: 5:28 - loss: 0.6779 - acc: 0.5746
2304/5677 [===========>..................] - ETA: 5:22 - loss: 0.6781 - acc: 0.5738
2368/5677 [===========>..................] - ETA: 5:17 - loss: 0.6778 - acc: 0.5752
2432/5677 [===========>..................] - ETA: 5:11 - loss: 0.6787 - acc: 0.5744
2496/5677 [============>.................] - ETA: 5:05 - loss: 0.6797 - acc: 0.5729
2560/5677 [============>.................] - ETA: 4:59 - loss: 0.6786 - acc: 0.5746
2624/5677 [============>.................] - ETA: 4:53 - loss: 0.6788 - acc: 0.5736
2688/5677 [=============>................] - ETA: 4:47 - loss: 0.6781 - acc: 0.5755
2752/5677 [=============>................] - ETA: 4:41 - loss: 0.6782 - acc: 0.5741
2816/5677 [=============>................] - ETA: 4:34 - loss: 0.6778 - acc: 0.5753
2880/5677 [==============>...............] - ETA: 4:28 - loss: 0.6778 - acc: 0.5753
2944/5677 [==============>...............] - ETA: 4:22 - loss: 0.6782 - acc: 0.5744
3008/5677 [==============>...............] - ETA: 4:16 - loss: 0.6787 - acc: 0.5731
3072/5677 [===============>..............] - ETA: 4:10 - loss: 0.6788 - acc: 0.5726
3136/5677 [===============>..............] - ETA: 4:04 - loss: 0.6785 - acc: 0.5730
3200/5677 [===============>..............] - ETA: 3:58 - loss: 0.6793 - acc: 0.5706
3264/5677 [================>.............] - ETA: 3:51 - loss: 0.6800 - acc: 0.5699
3328/5677 [================>.............] - ETA: 3:45 - loss: 0.6805 - acc: 0.5667
3392/5677 [================>.............] - ETA: 3:38 - loss: 0.6803 - acc: 0.5672
3456/5677 [=================>............] - ETA: 3:32 - loss: 0.6813 - acc: 0.5651
3520/5677 [=================>............] - ETA: 3:26 - loss: 0.6820 - acc: 0.5645
3584/5677 [=================>............] - ETA: 3:20 - loss: 0.6817 - acc: 0.5645
3648/5677 [==================>...........] - ETA: 3:14 - loss: 0.6813 - acc: 0.5639
3712/5677 [==================>...........] - ETA: 3:08 - loss: 0.6812 - acc: 0.5638
3776/5677 [==================>...........] - ETA: 3:02 - loss: 0.6813 - acc: 0.5638
3840/5677 [===================>..........] - ETA: 2:56 - loss: 0.6815 - acc: 0.5638
3904/5677 [===================>..........] - ETA: 2:50 - loss: 0.6812 - acc: 0.5638
3968/5677 [===================>..........] - ETA: 2:43 - loss: 0.6817 - acc: 0.5617
4032/5677 [====================>.........] - ETA: 2:38 - loss: 0.6819 - acc: 0.5615
4096/5677 [====================>.........] - ETA: 2:31 - loss: 0.6828 - acc: 0.5596
4160/5677 [====================>.........] - ETA: 2:25 - loss: 0.6827 - acc: 0.5601
4224/5677 [=====================>........] - ETA: 2:19 - loss: 0.6825 - acc: 0.5611
4288/5677 [=====================>........] - ETA: 2:13 - loss: 0.6829 - acc: 0.5599
4352/5677 [=====================>........] - ETA: 2:07 - loss: 0.6833 - acc: 0.5584
4416/5677 [======================>.......] - ETA: 2:01 - loss: 0.6834 - acc: 0.5584
4480/5677 [======================>.......] - ETA: 1:55 - loss: 0.6832 - acc: 0.5583
4544/5677 [=======================>......] - ETA: 1:48 - loss: 0.6830 - acc: 0.5590
4608/5677 [=======================>......] - ETA: 1:42 - loss: 0.6828 - acc: 0.5601
4672/5677 [=======================>......] - ETA: 1:36 - loss: 0.6825 - acc: 0.5610
4736/5677 [========================>.....] - ETA: 1:29 - loss: 0.6823 - acc: 0.5617
4800/5677 [========================>.....] - ETA: 1:23 - loss: 0.6823 - acc: 0.5617
4864/5677 [========================>.....] - ETA: 1:17 - loss: 0.6823 - acc: 0.5613
4928/5677 [=========================>....] - ETA: 1:11 - loss: 0.6824 - acc: 0.5613
4992/5677 [=========================>....] - ETA: 1:05 - loss: 0.6819 - acc: 0.5621
5056/5677 [=========================>....] - ETA: 58s - loss: 0.6817 - acc: 0.5621 
5120/5677 [==========================>...] - ETA: 52s - loss: 0.6823 - acc: 0.5607
5184/5677 [==========================>...] - ETA: 46s - loss: 0.6821 - acc: 0.5615
5248/5677 [==========================>...] - ETA: 40s - loss: 0.6812 - acc: 0.5629
5312/5677 [===========================>..] - ETA: 34s - loss: 0.6812 - acc: 0.5627
5376/5677 [===========================>..] - ETA: 28s - loss: 0.6812 - acc: 0.5631
5440/5677 [===========================>..] - ETA: 22s - loss: 0.6817 - acc: 0.5632
5504/5677 [============================>.] - ETA: 16s - loss: 0.6815 - acc: 0.5632
5568/5677 [============================>.] - ETA: 10s - loss: 0.6822 - acc: 0.5625
5632/5677 [============================>.] - ETA: 4s - loss: 0.6822 - acc: 0.5621 
5677/5677 [==============================] - 554s 98ms/step - loss: 0.6825 - acc: 0.5617 - val_loss: 0.7392 - val_acc: 0.4849

Epoch 00007: val_acc did not improve from 0.58003
Epoch 8/10

  64/5677 [..............................] - ETA: 7:22 - loss: 0.7122 - acc: 0.5156
 128/5677 [..............................] - ETA: 7:42 - loss: 0.6785 - acc: 0.5625
 192/5677 [>.............................] - ETA: 7:31 - loss: 0.6714 - acc: 0.5990
 256/5677 [>.............................] - ETA: 7:42 - loss: 0.6789 - acc: 0.5781
 320/5677 [>.............................] - ETA: 7:33 - loss: 0.6732 - acc: 0.5875
 384/5677 [=>............................] - ETA: 7:33 - loss: 0.6713 - acc: 0.5938
 448/5677 [=>............................] - ETA: 7:35 - loss: 0.6709 - acc: 0.6004
 512/5677 [=>............................] - ETA: 7:34 - loss: 0.6744 - acc: 0.5918
 576/5677 [==>...........................] - ETA: 7:32 - loss: 0.6777 - acc: 0.5851
 640/5677 [==>...........................] - ETA: 7:32 - loss: 0.6786 - acc: 0.5813
 704/5677 [==>...........................] - ETA: 7:23 - loss: 0.6773 - acc: 0.5810
 768/5677 [===>..........................] - ETA: 7:22 - loss: 0.6803 - acc: 0.5781
 832/5677 [===>..........................] - ETA: 7:20 - loss: 0.6806 - acc: 0.5757
 896/5677 [===>..........................] - ETA: 7:17 - loss: 0.6818 - acc: 0.5759
 960/5677 [====>.........................] - ETA: 7:12 - loss: 0.6816 - acc: 0.5740
1024/5677 [====>.........................] - ETA: 7:08 - loss: 0.6821 - acc: 0.5693
1088/5677 [====>.........................] - ETA: 7:04 - loss: 0.6821 - acc: 0.5735
1152/5677 [=====>........................] - ETA: 6:58 - loss: 0.6812 - acc: 0.5747
1216/5677 [=====>........................] - ETA: 6:50 - loss: 0.6817 - acc: 0.5748
1280/5677 [=====>........................] - ETA: 6:44 - loss: 0.6811 - acc: 0.5758
1344/5677 [======>.......................] - ETA: 6:39 - loss: 0.6813 - acc: 0.5744
1408/5677 [======>.......................] - ETA: 6:34 - loss: 0.6807 - acc: 0.5767
1472/5677 [======>.......................] - ETA: 6:28 - loss: 0.6792 - acc: 0.5815
1536/5677 [=======>......................] - ETA: 6:21 - loss: 0.6793 - acc: 0.5814
1600/5677 [=======>......................] - ETA: 6:15 - loss: 0.6796 - acc: 0.5819
1664/5677 [=======>......................] - ETA: 6:08 - loss: 0.6806 - acc: 0.5823
1728/5677 [========>.....................] - ETA: 6:01 - loss: 0.6806 - acc: 0.5839
1792/5677 [========>.....................] - ETA: 5:55 - loss: 0.6804 - acc: 0.5837
1856/5677 [========>.....................] - ETA: 5:49 - loss: 0.6816 - acc: 0.5797
1920/5677 [=========>....................] - ETA: 5:42 - loss: 0.6825 - acc: 0.5766
1984/5677 [=========>....................] - ETA: 5:36 - loss: 0.6826 - acc: 0.5776
2048/5677 [=========>....................] - ETA: 5:29 - loss: 0.6819 - acc: 0.5776
2112/5677 [==========>...................] - ETA: 5:24 - loss: 0.6822 - acc: 0.5786
2176/5677 [==========>...................] - ETA: 5:19 - loss: 0.6825 - acc: 0.5781
2240/5677 [==========>...................] - ETA: 5:13 - loss: 0.6828 - acc: 0.5763
2304/5677 [===========>..................] - ETA: 5:07 - loss: 0.6833 - acc: 0.5742
2368/5677 [===========>..................] - ETA: 5:01 - loss: 0.6835 - acc: 0.5739
2432/5677 [===========>..................] - ETA: 4:55 - loss: 0.6842 - acc: 0.5728
2496/5677 [============>.................] - ETA: 4:49 - loss: 0.6839 - acc: 0.5737
2560/5677 [============>.................] - ETA: 4:43 - loss: 0.6846 - acc: 0.5715
2624/5677 [============>.................] - ETA: 4:37 - loss: 0.6848 - acc: 0.5709
2688/5677 [=============>................] - ETA: 4:32 - loss: 0.6848 - acc: 0.5714
2752/5677 [=============>................] - ETA: 4:25 - loss: 0.6849 - acc: 0.5705
2816/5677 [=============>................] - ETA: 4:19 - loss: 0.6842 - acc: 0.5721
2880/5677 [==============>...............] - ETA: 4:13 - loss: 0.6841 - acc: 0.5722
2944/5677 [==============>...............] - ETA: 4:07 - loss: 0.6839 - acc: 0.5710
3008/5677 [==============>...............] - ETA: 4:01 - loss: 0.6830 - acc: 0.5721
3072/5677 [===============>..............] - ETA: 3:55 - loss: 0.6833 - acc: 0.5710
3136/5677 [===============>..............] - ETA: 3:49 - loss: 0.6830 - acc: 0.5727
3200/5677 [===============>..............] - ETA: 3:44 - loss: 0.6829 - acc: 0.5731
3264/5677 [================>.............] - ETA: 3:37 - loss: 0.6820 - acc: 0.5738
3328/5677 [================>.............] - ETA: 3:32 - loss: 0.6820 - acc: 0.5736
3392/5677 [================>.............] - ETA: 3:26 - loss: 0.6813 - acc: 0.5755
3456/5677 [=================>............] - ETA: 3:21 - loss: 0.6810 - acc: 0.5761
3520/5677 [=================>............] - ETA: 3:15 - loss: 0.6811 - acc: 0.5753
3584/5677 [=================>............] - ETA: 3:09 - loss: 0.6815 - acc: 0.5756
3648/5677 [==================>...........] - ETA: 3:03 - loss: 0.6809 - acc: 0.5762
3712/5677 [==================>...........] - ETA: 2:58 - loss: 0.6807 - acc: 0.5762
3776/5677 [==================>...........] - ETA: 2:52 - loss: 0.6799 - acc: 0.5781
3840/5677 [===================>..........] - ETA: 2:46 - loss: 0.6796 - acc: 0.5781
3904/5677 [===================>..........] - ETA: 2:41 - loss: 0.6799 - acc: 0.5776
3968/5677 [===================>..........] - ETA: 2:35 - loss: 0.6798 - acc: 0.5769
4032/5677 [====================>.........] - ETA: 2:29 - loss: 0.6799 - acc: 0.5769
4096/5677 [====================>.........] - ETA: 2:23 - loss: 0.6796 - acc: 0.5769
4160/5677 [====================>.........] - ETA: 2:17 - loss: 0.6794 - acc: 0.5779
4224/5677 [=====================>........] - ETA: 2:11 - loss: 0.6793 - acc: 0.5784
4288/5677 [=====================>........] - ETA: 2:06 - loss: 0.6789 - acc: 0.5788
4352/5677 [=====================>........] - ETA: 2:00 - loss: 0.6787 - acc: 0.5779
4416/5677 [======================>.......] - ETA: 1:54 - loss: 0.6785 - acc: 0.5777
4480/5677 [======================>.......] - ETA: 1:48 - loss: 0.6790 - acc: 0.5761
4544/5677 [=======================>......] - ETA: 1:42 - loss: 0.6787 - acc: 0.5766
4608/5677 [=======================>......] - ETA: 1:36 - loss: 0.6787 - acc: 0.5760
4672/5677 [=======================>......] - ETA: 1:31 - loss: 0.6782 - acc: 0.5771
4736/5677 [========================>.....] - ETA: 1:25 - loss: 0.6778 - acc: 0.5777
4800/5677 [========================>.....] - ETA: 1:19 - loss: 0.6783 - acc: 0.5758
4864/5677 [========================>.....] - ETA: 1:13 - loss: 0.6792 - acc: 0.5744
4928/5677 [=========================>....] - ETA: 1:08 - loss: 0.6791 - acc: 0.5741
4992/5677 [=========================>....] - ETA: 1:02 - loss: 0.6789 - acc: 0.5749
5056/5677 [=========================>....] - ETA: 56s - loss: 0.6785 - acc: 0.5756 
5120/5677 [==========================>...] - ETA: 50s - loss: 0.6787 - acc: 0.5750
5184/5677 [==========================>...] - ETA: 44s - loss: 0.6785 - acc: 0.5756
5248/5677 [==========================>...] - ETA: 39s - loss: 0.6776 - acc: 0.5776
5312/5677 [===========================>..] - ETA: 33s - loss: 0.6777 - acc: 0.5772
5376/5677 [===========================>..] - ETA: 27s - loss: 0.6781 - acc: 0.5766
5440/5677 [===========================>..] - ETA: 21s - loss: 0.6785 - acc: 0.5754
5504/5677 [============================>.] - ETA: 15s - loss: 0.6782 - acc: 0.5759
5568/5677 [============================>.] - ETA: 9s - loss: 0.6778 - acc: 0.5772 
5632/5677 [============================>.] - ETA: 4s - loss: 0.6774 - acc: 0.5776
5677/5677 [==============================] - 537s 95ms/step - loss: 0.6770 - acc: 0.5785 - val_loss: 0.6710 - val_acc: 0.5848

Epoch 00008: val_acc improved from 0.58003 to 0.58479, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window01/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 9/10

  64/5677 [..............................] - ETA: 8:33 - loss: 0.6627 - acc: 0.6250
 128/5677 [..............................] - ETA: 8:16 - loss: 0.6705 - acc: 0.5703
 192/5677 [>.............................] - ETA: 7:47 - loss: 0.6733 - acc: 0.5625
 256/5677 [>.............................] - ETA: 7:47 - loss: 0.6685 - acc: 0.5742
 320/5677 [>.............................] - ETA: 7:56 - loss: 0.6715 - acc: 0.5750
 384/5677 [=>............................] - ETA: 8:00 - loss: 0.6688 - acc: 0.5859
 448/5677 [=>............................] - ETA: 7:57 - loss: 0.6717 - acc: 0.5893
 512/5677 [=>............................] - ETA: 7:54 - loss: 0.6675 - acc: 0.5938
 576/5677 [==>...........................] - ETA: 7:42 - loss: 0.6691 - acc: 0.5955
 640/5677 [==>...........................] - ETA: 7:33 - loss: 0.6700 - acc: 0.5922
 704/5677 [==>...........................] - ETA: 7:25 - loss: 0.6681 - acc: 0.5895
 768/5677 [===>..........................] - ETA: 7:17 - loss: 0.6717 - acc: 0.5885
 832/5677 [===>..........................] - ETA: 7:13 - loss: 0.6687 - acc: 0.5950
 896/5677 [===>..........................] - ETA: 7:11 - loss: 0.6703 - acc: 0.5904
 960/5677 [====>.........................] - ETA: 7:10 - loss: 0.6737 - acc: 0.5833
1024/5677 [====>.........................] - ETA: 7:06 - loss: 0.6736 - acc: 0.5801
1088/5677 [====>.........................] - ETA: 7:00 - loss: 0.6743 - acc: 0.5836
1152/5677 [=====>........................] - ETA: 6:57 - loss: 0.6739 - acc: 0.5851
1216/5677 [=====>........................] - ETA: 6:53 - loss: 0.6754 - acc: 0.5831
1280/5677 [=====>........................] - ETA: 6:48 - loss: 0.6764 - acc: 0.5805
1344/5677 [======>.......................] - ETA: 6:43 - loss: 0.6762 - acc: 0.5841
1408/5677 [======>.......................] - ETA: 6:38 - loss: 0.6761 - acc: 0.5852
1472/5677 [======>.......................] - ETA: 6:34 - loss: 0.6773 - acc: 0.5802
1536/5677 [=======>......................] - ETA: 6:29 - loss: 0.6767 - acc: 0.5794
1600/5677 [=======>......................] - ETA: 6:25 - loss: 0.6771 - acc: 0.5806
1664/5677 [=======>......................] - ETA: 6:19 - loss: 0.6778 - acc: 0.5799
1728/5677 [========>.....................] - ETA: 6:13 - loss: 0.6785 - acc: 0.5787
1792/5677 [========>.....................] - ETA: 6:07 - loss: 0.6799 - acc: 0.5753
1856/5677 [========>.....................] - ETA: 6:02 - loss: 0.6793 - acc: 0.5760
1920/5677 [=========>....................] - ETA: 5:56 - loss: 0.6796 - acc: 0.5760
1984/5677 [=========>....................] - ETA: 5:49 - loss: 0.6783 - acc: 0.5776
2048/5677 [=========>....................] - ETA: 5:43 - loss: 0.6786 - acc: 0.5757
2112/5677 [==========>...................] - ETA: 5:35 - loss: 0.6785 - acc: 0.5753
2176/5677 [==========>...................] - ETA: 5:28 - loss: 0.6786 - acc: 0.5749
2240/5677 [==========>...................] - ETA: 5:22 - loss: 0.6782 - acc: 0.5763
2304/5677 [===========>..................] - ETA: 5:17 - loss: 0.6780 - acc: 0.5773
2368/5677 [===========>..................] - ETA: 5:11 - loss: 0.6777 - acc: 0.5781
2432/5677 [===========>..................] - ETA: 5:05 - loss: 0.6773 - acc: 0.5789
2496/5677 [============>.................] - ETA: 4:59 - loss: 0.6767 - acc: 0.5801
2560/5677 [============>.................] - ETA: 4:54 - loss: 0.6765 - acc: 0.5801
2624/5677 [============>.................] - ETA: 4:48 - loss: 0.6773 - acc: 0.5781
2688/5677 [=============>................] - ETA: 4:42 - loss: 0.6767 - acc: 0.5789
2752/5677 [=============>................] - ETA: 4:36 - loss: 0.6775 - acc: 0.5770
2816/5677 [=============>................] - ETA: 4:31 - loss: 0.6778 - acc: 0.5756
2880/5677 [==============>...............] - ETA: 4:25 - loss: 0.6772 - acc: 0.5757
2944/5677 [==============>...............] - ETA: 4:19 - loss: 0.6767 - acc: 0.5778
3008/5677 [==============>...............] - ETA: 4:13 - loss: 0.6761 - acc: 0.5791
3072/5677 [===============>..............] - ETA: 4:07 - loss: 0.6757 - acc: 0.5791
3136/5677 [===============>..............] - ETA: 4:01 - loss: 0.6764 - acc: 0.5791
3200/5677 [===============>..............] - ETA: 3:55 - loss: 0.6765 - acc: 0.5781
3264/5677 [================>.............] - ETA: 3:48 - loss: 0.6773 - acc: 0.5769
3328/5677 [================>.............] - ETA: 3:42 - loss: 0.6765 - acc: 0.5796
3392/5677 [================>.............] - ETA: 3:35 - loss: 0.6772 - acc: 0.5775
3456/5677 [=================>............] - ETA: 3:29 - loss: 0.6770 - acc: 0.5775
3520/5677 [=================>............] - ETA: 3:23 - loss: 0.6771 - acc: 0.5773
3584/5677 [=================>............] - ETA: 3:17 - loss: 0.6764 - acc: 0.5784
3648/5677 [==================>...........] - ETA: 3:11 - loss: 0.6763 - acc: 0.5781
3712/5677 [==================>...........] - ETA: 3:05 - loss: 0.6769 - acc: 0.5770
3776/5677 [==================>...........] - ETA: 2:59 - loss: 0.6773 - acc: 0.5757
3840/5677 [===================>..........] - ETA: 2:53 - loss: 0.6773 - acc: 0.5753
3904/5677 [===================>..........] - ETA: 2:47 - loss: 0.6769 - acc: 0.5771
3968/5677 [===================>..........] - ETA: 2:41 - loss: 0.6761 - acc: 0.5781
4032/5677 [====================>.........] - ETA: 2:35 - loss: 0.6753 - acc: 0.5806
4096/5677 [====================>.........] - ETA: 2:29 - loss: 0.6752 - acc: 0.5811
4160/5677 [====================>.........] - ETA: 2:23 - loss: 0.6757 - acc: 0.5791
4224/5677 [=====================>........] - ETA: 2:16 - loss: 0.6760 - acc: 0.5781
4288/5677 [=====================>........] - ETA: 2:10 - loss: 0.6759 - acc: 0.5786
4352/5677 [=====================>........] - ETA: 2:04 - loss: 0.6758 - acc: 0.5800
4416/5677 [======================>.......] - ETA: 1:58 - loss: 0.6768 - acc: 0.5788
4480/5677 [======================>.......] - ETA: 1:52 - loss: 0.6768 - acc: 0.5781
4544/5677 [=======================>......] - ETA: 1:46 - loss: 0.6773 - acc: 0.5779
4608/5677 [=======================>......] - ETA: 1:40 - loss: 0.6774 - acc: 0.5779
4672/5677 [=======================>......] - ETA: 1:34 - loss: 0.6778 - acc: 0.5773
4736/5677 [========================>.....] - ETA: 1:28 - loss: 0.6774 - acc: 0.5783
4800/5677 [========================>.....] - ETA: 1:22 - loss: 0.6774 - acc: 0.5783
4864/5677 [========================>.....] - ETA: 1:16 - loss: 0.6774 - acc: 0.5785
4928/5677 [=========================>....] - ETA: 1:10 - loss: 0.6775 - acc: 0.5783
4992/5677 [=========================>....] - ETA: 1:03 - loss: 0.6775 - acc: 0.5781
5056/5677 [=========================>....] - ETA: 57s - loss: 0.6779 - acc: 0.5773 
5120/5677 [==========================>...] - ETA: 51s - loss: 0.6777 - acc: 0.5771
5184/5677 [==========================>...] - ETA: 45s - loss: 0.6777 - acc: 0.5777
5248/5677 [==========================>...] - ETA: 39s - loss: 0.6774 - acc: 0.5774
5312/5677 [===========================>..] - ETA: 33s - loss: 0.6776 - acc: 0.5764
5376/5677 [===========================>..] - ETA: 27s - loss: 0.6774 - acc: 0.5768
5440/5677 [===========================>..] - ETA: 21s - loss: 0.6778 - acc: 0.5757
5504/5677 [============================>.] - ETA: 15s - loss: 0.6778 - acc: 0.5750
5568/5677 [============================>.] - ETA: 10s - loss: 0.6778 - acc: 0.5760
5632/5677 [============================>.] - ETA: 4s - loss: 0.6777 - acc: 0.5765 
5677/5677 [==============================] - 543s 96ms/step - loss: 0.6778 - acc: 0.5764 - val_loss: 0.6817 - val_acc: 0.5753

Epoch 00009: val_acc did not improve from 0.58479
Epoch 10/10

  64/5677 [..............................] - ETA: 8:05 - loss: 0.6529 - acc: 0.5938
 128/5677 [..............................] - ETA: 7:57 - loss: 0.6662 - acc: 0.5469
 192/5677 [>.............................] - ETA: 7:44 - loss: 0.6732 - acc: 0.5781
 256/5677 [>.............................] - ETA: 7:33 - loss: 0.6836 - acc: 0.5820
 320/5677 [>.............................] - ETA: 7:19 - loss: 0.6862 - acc: 0.5813
 384/5677 [=>............................] - ETA: 7:15 - loss: 0.6797 - acc: 0.5859
 448/5677 [=>............................] - ETA: 7:05 - loss: 0.6744 - acc: 0.5915
 512/5677 [=>............................] - ETA: 7:00 - loss: 0.6706 - acc: 0.6016
 576/5677 [==>...........................] - ETA: 6:54 - loss: 0.6732 - acc: 0.5938
 640/5677 [==>...........................] - ETA: 6:48 - loss: 0.6680 - acc: 0.6078
 704/5677 [==>...........................] - ETA: 6:43 - loss: 0.6678 - acc: 0.6080
 768/5677 [===>..........................] - ETA: 6:37 - loss: 0.6668 - acc: 0.6081
 832/5677 [===>..........................] - ETA: 6:33 - loss: 0.6642 - acc: 0.6106
 896/5677 [===>..........................] - ETA: 6:26 - loss: 0.6663 - acc: 0.6038
 960/5677 [====>.........................] - ETA: 6:21 - loss: 0.6618 - acc: 0.6115
1024/5677 [====>.........................] - ETA: 6:13 - loss: 0.6677 - acc: 0.6025
1088/5677 [====>.........................] - ETA: 6:09 - loss: 0.6704 - acc: 0.5974
1152/5677 [=====>........................] - ETA: 6:03 - loss: 0.6718 - acc: 0.5929
1216/5677 [=====>........................] - ETA: 6:00 - loss: 0.6723 - acc: 0.5913
1280/5677 [=====>........................] - ETA: 5:55 - loss: 0.6722 - acc: 0.5930
1344/5677 [======>.......................] - ETA: 5:50 - loss: 0.6717 - acc: 0.5923
1408/5677 [======>.......................] - ETA: 5:46 - loss: 0.6724 - acc: 0.5930
1472/5677 [======>.......................] - ETA: 5:41 - loss: 0.6708 - acc: 0.5958
1536/5677 [=======>......................] - ETA: 5:36 - loss: 0.6708 - acc: 0.5944
1600/5677 [=======>......................] - ETA: 5:31 - loss: 0.6714 - acc: 0.5925
1664/5677 [=======>......................] - ETA: 5:25 - loss: 0.6722 - acc: 0.5913
1728/5677 [========>.....................] - ETA: 5:20 - loss: 0.6731 - acc: 0.5909
1792/5677 [========>.....................] - ETA: 5:16 - loss: 0.6729 - acc: 0.5926
1856/5677 [========>.....................] - ETA: 5:11 - loss: 0.6731 - acc: 0.5943
1920/5677 [=========>....................] - ETA: 5:06 - loss: 0.6723 - acc: 0.5964
1984/5677 [=========>....................] - ETA: 5:00 - loss: 0.6723 - acc: 0.5943
2048/5677 [=========>....................] - ETA: 4:56 - loss: 0.6715 - acc: 0.5947
2112/5677 [==========>...................] - ETA: 4:52 - loss: 0.6704 - acc: 0.5956
2176/5677 [==========>...................] - ETA: 4:48 - loss: 0.6710 - acc: 0.5942
2240/5677 [==========>...................] - ETA: 4:43 - loss: 0.6697 - acc: 0.5964
2304/5677 [===========>..................] - ETA: 4:40 - loss: 0.6707 - acc: 0.5933
2368/5677 [===========>..................] - ETA: 4:35 - loss: 0.6709 - acc: 0.5916
2432/5677 [===========>..................] - ETA: 4:31 - loss: 0.6719 - acc: 0.5909
2496/5677 [============>.................] - ETA: 4:27 - loss: 0.6726 - acc: 0.5885
2560/5677 [============>.................] - ETA: 4:24 - loss: 0.6726 - acc: 0.5875
2624/5677 [============>.................] - ETA: 4:20 - loss: 0.6730 - acc: 0.5877
2688/5677 [=============>................] - ETA: 4:15 - loss: 0.6737 - acc: 0.5852
2752/5677 [=============>................] - ETA: 4:10 - loss: 0.6744 - acc: 0.5847
2816/5677 [=============>................] - ETA: 4:04 - loss: 0.6750 - acc: 0.5835
2880/5677 [==============>...............] - ETA: 3:59 - loss: 0.6758 - acc: 0.5840
2944/5677 [==============>...............] - ETA: 3:55 - loss: 0.6756 - acc: 0.5839
3008/5677 [==============>...............] - ETA: 3:49 - loss: 0.6754 - acc: 0.5834
3072/5677 [===============>..............] - ETA: 3:44 - loss: 0.6755 - acc: 0.5833
3136/5677 [===============>..............] - ETA: 3:39 - loss: 0.6759 - acc: 0.5835
3200/5677 [===============>..............] - ETA: 3:34 - loss: 0.6763 - acc: 0.5809
3264/5677 [================>.............] - ETA: 3:29 - loss: 0.6761 - acc: 0.5815
3328/5677 [================>.............] - ETA: 3:23 - loss: 0.6762 - acc: 0.5820
3392/5677 [================>.............] - ETA: 3:18 - loss: 0.6766 - acc: 0.5817
3456/5677 [=================>............] - ETA: 3:13 - loss: 0.6762 - acc: 0.5825
3520/5677 [=================>............] - ETA: 3:07 - loss: 0.6754 - acc: 0.5844
3584/5677 [=================>............] - ETA: 3:02 - loss: 0.6747 - acc: 0.5859
3648/5677 [==================>...........] - ETA: 2:56 - loss: 0.6747 - acc: 0.5866
3712/5677 [==================>...........] - ETA: 2:51 - loss: 0.6749 - acc: 0.5867
3776/5677 [==================>...........] - ETA: 2:45 - loss: 0.6752 - acc: 0.5863
3840/5677 [===================>..........] - ETA: 2:40 - loss: 0.6753 - acc: 0.5862
3904/5677 [===================>..........] - ETA: 2:34 - loss: 0.6752 - acc: 0.5858
3968/5677 [===================>..........] - ETA: 2:28 - loss: 0.6751 - acc: 0.5869
4032/5677 [====================>.........] - ETA: 2:23 - loss: 0.6744 - acc: 0.5885
4096/5677 [====================>.........] - ETA: 2:17 - loss: 0.6750 - acc: 0.5850
4160/5677 [====================>.........] - ETA: 2:12 - loss: 0.6746 - acc: 0.5858
4224/5677 [=====================>........] - ETA: 2:06 - loss: 0.6748 - acc: 0.5859
4288/5677 [=====================>........] - ETA: 2:01 - loss: 0.6750 - acc: 0.5856
4352/5677 [=====================>........] - ETA: 1:55 - loss: 0.6747 - acc: 0.5859
4416/5677 [======================>.......] - ETA: 1:49 - loss: 0.6746 - acc: 0.5861
4480/5677 [======================>.......] - ETA: 1:44 - loss: 0.6746 - acc: 0.5862
4544/5677 [=======================>......] - ETA: 1:38 - loss: 0.6746 - acc: 0.5860
4608/5677 [=======================>......] - ETA: 1:33 - loss: 0.6747 - acc: 0.5862
4672/5677 [=======================>......] - ETA: 1:27 - loss: 0.6744 - acc: 0.5865
4736/5677 [========================>.....] - ETA: 1:22 - loss: 0.6748 - acc: 0.5861
4800/5677 [========================>.....] - ETA: 1:16 - loss: 0.6750 - acc: 0.5858
4864/5677 [========================>.....] - ETA: 1:10 - loss: 0.6748 - acc: 0.5859
4928/5677 [=========================>....] - ETA: 1:05 - loss: 0.6744 - acc: 0.5858
4992/5677 [=========================>....] - ETA: 59s - loss: 0.6744 - acc: 0.5855 
5056/5677 [=========================>....] - ETA: 54s - loss: 0.6742 - acc: 0.5860
5120/5677 [==========================>...] - ETA: 48s - loss: 0.6738 - acc: 0.5867
5184/5677 [==========================>...] - ETA: 43s - loss: 0.6738 - acc: 0.5872
5248/5677 [==========================>...] - ETA: 37s - loss: 0.6732 - acc: 0.5882
5312/5677 [===========================>..] - ETA: 32s - loss: 0.6732 - acc: 0.5883
5376/5677 [===========================>..] - ETA: 26s - loss: 0.6730 - acc: 0.5884
5440/5677 [===========================>..] - ETA: 20s - loss: 0.6731 - acc: 0.5879
5504/5677 [============================>.] - ETA: 15s - loss: 0.6740 - acc: 0.5865
5568/5677 [============================>.] - ETA: 9s - loss: 0.6737 - acc: 0.5867 
5632/5677 [============================>.] - ETA: 3s - loss: 0.6736 - acc: 0.5872
5677/5677 [==============================] - 526s 93ms/step - loss: 0.6739 - acc: 0.5866 - val_loss: 0.6678 - val_acc: 0.5975

Epoch 00010: val_acc improved from 0.58479 to 0.59746, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window01/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f2502bd6550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f2502bd6550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f2502b62990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f2502b62990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24fa73db10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24fa73db10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24fa62d550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24fa62d550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24fa623d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24fa623d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24fa5523d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24fa5523d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24fa62dd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24fa62dd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b0c28390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b0c28390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24fa5ae050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24fa5ae050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24fa33de50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24fa33de50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24fa3937d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24fa3937d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24fa324f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24fa324f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24fa448310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24fa448310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24f2113190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24f2113190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24f202bfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24f202bfd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b97ebe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b97ebe10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24f2113550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24f2113550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f9814b150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f9814b150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24f1e65e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24f1e65e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f98166d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f98166d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b9583d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b9583d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24f1e1ed50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24f1e1ed50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f1d59210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f1d59210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24f1a88250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24f1a88250>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24f1999ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24f1999ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b929f290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24b929f290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24f1a88910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24f1a88910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f1996990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f1996990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24f19cfa10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24f19cfa10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24f1715310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24f1715310>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f19db490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f19db490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24f17b7910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24f17b7910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f163a250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f163a250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24f148ab10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24f148ab10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24f142fc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24f142fc90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f13bced0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f13bced0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24f15a67d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24f15a67d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f1397d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f1397d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24f13355d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24f13355d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24f10b9f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24f10b9f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f14d3c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f14d3c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24f1335810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24f1335810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f1051b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f1051b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24f14e2fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24f14e2fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24f10c3ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24f10c3ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f1044750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f1044750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24f0e14dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24f0e14dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f0d87790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f0d87790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24f0ab7090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24f0ab7090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24f0b3f190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24f0b3f190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f0d62190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f0d62190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24f0ab7d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24f0ab7d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24e09c7050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24e09c7050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24e085e050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24e085e050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24e0747790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24e0747790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24e07cc910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24e07cc910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24e08c0190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24e08c0190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24e072f1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24e072f1d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24e0565f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24e0565f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24e03a0b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24e03a0b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24e04cd450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24e04cd450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24e050cd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24e050cd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24e03dc550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24e03dc550>>: AttributeError: module 'gast' has no attribute 'Str'
01window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 2:42
 128/1578 [=>............................] - ETA: 1:43
 192/1578 [==>...........................] - ETA: 1:21
 256/1578 [===>..........................] - ETA: 1:10
 320/1578 [=====>........................] - ETA: 1:03
 384/1578 [======>.......................] - ETA: 59s 
 448/1578 [=======>......................] - ETA: 54s
 512/1578 [========>.....................] - ETA: 50s
 576/1578 [=========>....................] - ETA: 45s
 640/1578 [===========>..................] - ETA: 41s
 704/1578 [============>.................] - ETA: 38s
 768/1578 [=============>................] - ETA: 35s
 832/1578 [==============>...............] - ETA: 32s
 896/1578 [================>.............] - ETA: 29s
 960/1578 [=================>............] - ETA: 26s
1024/1578 [==================>...........] - ETA: 23s
1088/1578 [===================>..........] - ETA: 20s
1152/1578 [====================>.........] - ETA: 17s
1216/1578 [======================>.......] - ETA: 14s
1280/1578 [=======================>......] - ETA: 12s
1344/1578 [========================>.....] - ETA: 9s 
1408/1578 [=========================>....] - ETA: 6s
1472/1578 [==========================>...] - ETA: 4s
1536/1578 [============================>.] - ETA: 1s
1578/1578 [==============================] - 64s 41ms/step
loss: 0.6734124144069444
acc: 0.5804816223067174
样本个数 3154
样本个数 6308
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f1f98130d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f1f98130d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f2502b62c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f2502b62c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24cb109b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24cb109b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24d51dc990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24d51dc990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24fa876c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24fa876c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24e03cd450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24e03cd450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24d51dc610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24d51dc610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d51c0450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d51c0450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2502b53210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f2502b53210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2502a4db90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f2502a4db90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2502a751d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f2502a751d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24faa1afd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24faa1afd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24e02e6510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24e02e6510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1f886a11d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1f886a11d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f88540d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f88540d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f0d7ed10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24f0d7ed10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1f886a1f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1f886a1f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f88630e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f88630e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1f8834ee10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1f8834ee10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f88260ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f88260ad0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f88149890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f88149890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24b929f0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24b929f0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f8829d450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f8829d450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1f882aecd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1f882aecd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f88298190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f88298190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24e07c8610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24e07c8610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1f88061290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1f88061290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f5460df10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f5460df10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1f547e9990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1f547e9990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f543e2750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f543e2750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f5441e250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f5441e250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1f545d5590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1f545d5590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f54429a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f54429a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1f543db750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1f543db750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f5407be50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f5407be50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f544308d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f544308d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1f544bca90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1f544bca90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f54226850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f54226850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1f387b2050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1f387b2050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f540ea650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f540ea650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f38457c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f38457c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1f387b27d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1f387b27d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f384579d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f384579d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1f3869b550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1f3869b550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f3823d850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f3823d850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f383a7590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f383a7590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1f384a0690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1f384a0690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f383a3e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f383a3e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1f107c5f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1f107c5f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f38040110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f38040110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f38243cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f38243cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1f3809a050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1f3809a050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f1059fdd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f1059fdd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1db440df90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1db440df90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f1073c090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f1073c090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f38060bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f38060bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1db440d6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1db440d6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1db436f950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1db436f950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1db4130490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1db4130490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1db4145d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1db4145d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1db4152b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1db4152b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1db4130890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1db4130890>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1d907e2790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1d907e2790>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 50:19 - loss: 0.6482 - acc: 0.6562
 128/5677 [..............................] - ETA: 29:26 - loss: 0.7376 - acc: 0.5156
 192/5677 [>.............................] - ETA: 22:11 - loss: 0.7403 - acc: 0.4948
 256/5677 [>.............................] - ETA: 18:27 - loss: 0.7302 - acc: 0.5078
 320/5677 [>.............................] - ETA: 16:13 - loss: 0.7519 - acc: 0.4875
 384/5677 [=>............................] - ETA: 14:49 - loss: 0.7608 - acc: 0.4688
 448/5677 [=>............................] - ETA: 13:55 - loss: 0.7749 - acc: 0.4554
 512/5677 [=>............................] - ETA: 13:24 - loss: 0.7676 - acc: 0.4512
 576/5677 [==>...........................] - ETA: 12:48 - loss: 0.7610 - acc: 0.4549
 640/5677 [==>...........................] - ETA: 12:18 - loss: 0.7552 - acc: 0.4688
 704/5677 [==>...........................] - ETA: 11:52 - loss: 0.7563 - acc: 0.4645
 768/5677 [===>..........................] - ETA: 11:28 - loss: 0.7534 - acc: 0.4674
 832/5677 [===>..........................] - ETA: 11:11 - loss: 0.7562 - acc: 0.4615
 896/5677 [===>..........................] - ETA: 10:58 - loss: 0.7572 - acc: 0.4621
 960/5677 [====>.........................] - ETA: 10:44 - loss: 0.7528 - acc: 0.4667
1024/5677 [====>.........................] - ETA: 10:28 - loss: 0.7510 - acc: 0.4658
1088/5677 [====>.........................] - ETA: 10:13 - loss: 0.7483 - acc: 0.4678
1152/5677 [=====>........................] - ETA: 10:04 - loss: 0.7445 - acc: 0.4757
1216/5677 [=====>........................] - ETA: 9:50 - loss: 0.7414 - acc: 0.4762 
1280/5677 [=====>........................] - ETA: 9:39 - loss: 0.7397 - acc: 0.4773
1344/5677 [======>.......................] - ETA: 9:28 - loss: 0.7402 - acc: 0.4784
1408/5677 [======>.......................] - ETA: 9:16 - loss: 0.7419 - acc: 0.4737
1472/5677 [======>.......................] - ETA: 9:07 - loss: 0.7403 - acc: 0.4776
1536/5677 [=======>......................] - ETA: 8:55 - loss: 0.7392 - acc: 0.4798
1600/5677 [=======>......................] - ETA: 8:44 - loss: 0.7385 - acc: 0.4825
1664/5677 [=======>......................] - ETA: 8:34 - loss: 0.7391 - acc: 0.4802
1728/5677 [========>.....................] - ETA: 8:23 - loss: 0.7366 - acc: 0.4832
1792/5677 [========>.....................] - ETA: 8:12 - loss: 0.7374 - acc: 0.4827
1856/5677 [========>.....................] - ETA: 8:02 - loss: 0.7390 - acc: 0.4806
1920/5677 [=========>....................] - ETA: 7:51 - loss: 0.7379 - acc: 0.4828
1984/5677 [=========>....................] - ETA: 7:39 - loss: 0.7369 - acc: 0.4874
2048/5677 [=========>....................] - ETA: 7:28 - loss: 0.7359 - acc: 0.4893
2112/5677 [==========>...................] - ETA: 7:19 - loss: 0.7355 - acc: 0.4891
2176/5677 [==========>...................] - ETA: 7:09 - loss: 0.7335 - acc: 0.4908
2240/5677 [==========>...................] - ETA: 7:02 - loss: 0.7327 - acc: 0.4929
2304/5677 [===========>..................] - ETA: 6:54 - loss: 0.7326 - acc: 0.4931
2368/5677 [===========>..................] - ETA: 6:45 - loss: 0.7315 - acc: 0.4949
2432/5677 [===========>..................] - ETA: 6:36 - loss: 0.7307 - acc: 0.4963
2496/5677 [============>.................] - ETA: 6:28 - loss: 0.7311 - acc: 0.4936
2560/5677 [============>.................] - ETA: 6:20 - loss: 0.7302 - acc: 0.4941
2624/5677 [============>.................] - ETA: 6:12 - loss: 0.7297 - acc: 0.4950
2688/5677 [=============>................] - ETA: 6:04 - loss: 0.7295 - acc: 0.4933
2752/5677 [=============>................] - ETA: 5:55 - loss: 0.7280 - acc: 0.4956
2816/5677 [=============>................] - ETA: 5:47 - loss: 0.7283 - acc: 0.4947
2880/5677 [==============>...............] - ETA: 5:39 - loss: 0.7279 - acc: 0.4951
2944/5677 [==============>...............] - ETA: 5:30 - loss: 0.7284 - acc: 0.4939
3008/5677 [==============>...............] - ETA: 5:21 - loss: 0.7276 - acc: 0.4953
3072/5677 [===============>..............] - ETA: 5:13 - loss: 0.7276 - acc: 0.4951
3136/5677 [===============>..............] - ETA: 5:05 - loss: 0.7272 - acc: 0.4952
3200/5677 [===============>..............] - ETA: 4:56 - loss: 0.7267 - acc: 0.4947
3264/5677 [================>.............] - ETA: 4:48 - loss: 0.7268 - acc: 0.4948
3328/5677 [================>.............] - ETA: 4:39 - loss: 0.7263 - acc: 0.4946
3392/5677 [================>.............] - ETA: 4:31 - loss: 0.7260 - acc: 0.4962
3456/5677 [=================>............] - ETA: 4:23 - loss: 0.7255 - acc: 0.4965
3520/5677 [=================>............] - ETA: 4:14 - loss: 0.7255 - acc: 0.4977
3584/5677 [=================>............] - ETA: 4:06 - loss: 0.7252 - acc: 0.4986
3648/5677 [==================>...........] - ETA: 3:59 - loss: 0.7252 - acc: 0.4992
3712/5677 [==================>...........] - ETA: 3:52 - loss: 0.7252 - acc: 0.4987
3776/5677 [==================>...........] - ETA: 3:44 - loss: 0.7258 - acc: 0.4979
3840/5677 [===================>..........] - ETA: 3:36 - loss: 0.7249 - acc: 0.4997
3904/5677 [===================>..........] - ETA: 3:28 - loss: 0.7251 - acc: 0.5000
3968/5677 [===================>..........] - ETA: 3:20 - loss: 0.7248 - acc: 0.5003
4032/5677 [====================>.........] - ETA: 3:13 - loss: 0.7239 - acc: 0.5007
4096/5677 [====================>.........] - ETA: 3:05 - loss: 0.7240 - acc: 0.5007
4160/5677 [====================>.........] - ETA: 2:57 - loss: 0.7243 - acc: 0.5000
4224/5677 [=====================>........] - ETA: 2:50 - loss: 0.7230 - acc: 0.5028
4288/5677 [=====================>........] - ETA: 2:42 - loss: 0.7228 - acc: 0.5037
4352/5677 [=====================>........] - ETA: 2:34 - loss: 0.7224 - acc: 0.5039
4416/5677 [======================>.......] - ETA: 2:26 - loss: 0.7222 - acc: 0.5041
4480/5677 [======================>.......] - ETA: 2:19 - loss: 0.7217 - acc: 0.5062
4544/5677 [=======================>......] - ETA: 2:11 - loss: 0.7214 - acc: 0.5070
4608/5677 [=======================>......] - ETA: 2:04 - loss: 0.7210 - acc: 0.5067
4672/5677 [=======================>......] - ETA: 1:56 - loss: 0.7204 - acc: 0.5083
4736/5677 [========================>.....] - ETA: 1:48 - loss: 0.7205 - acc: 0.5076
4800/5677 [========================>.....] - ETA: 1:41 - loss: 0.7208 - acc: 0.5071
4864/5677 [========================>.....] - ETA: 1:33 - loss: 0.7205 - acc: 0.5074
4928/5677 [=========================>....] - ETA: 1:26 - loss: 0.7197 - acc: 0.5091
4992/5677 [=========================>....] - ETA: 1:18 - loss: 0.7196 - acc: 0.5090
5056/5677 [=========================>....] - ETA: 1:11 - loss: 0.7195 - acc: 0.5087
5120/5677 [==========================>...] - ETA: 1:03 - loss: 0.7190 - acc: 0.5102
5184/5677 [==========================>...] - ETA: 56s - loss: 0.7191 - acc: 0.5096 
5248/5677 [==========================>...] - ETA: 48s - loss: 0.7186 - acc: 0.5097
5312/5677 [===========================>..] - ETA: 41s - loss: 0.7189 - acc: 0.5087
5376/5677 [===========================>..] - ETA: 34s - loss: 0.7180 - acc: 0.5104
5440/5677 [===========================>..] - ETA: 26s - loss: 0.7175 - acc: 0.5119
5504/5677 [============================>.] - ETA: 19s - loss: 0.7174 - acc: 0.5118
5568/5677 [============================>.] - ETA: 12s - loss: 0.7176 - acc: 0.5115
5632/5677 [============================>.] - ETA: 5s - loss: 0.7178 - acc: 0.5108 
5677/5677 [==============================] - 665s 117ms/step - loss: 0.7173 - acc: 0.5114 - val_loss: 0.6874 - val_acc: 0.5531

Epoch 00001: val_acc improved from -inf to 0.55309, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window02/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 8:53 - loss: 0.7054 - acc: 0.5938
 128/5677 [..............................] - ETA: 8:56 - loss: 0.7232 - acc: 0.5391
 192/5677 [>.............................] - ETA: 8:42 - loss: 0.7279 - acc: 0.5208
 256/5677 [>.............................] - ETA: 8:38 - loss: 0.7210 - acc: 0.5312
 320/5677 [>.............................] - ETA: 8:27 - loss: 0.7184 - acc: 0.5250
 384/5677 [=>............................] - ETA: 8:29 - loss: 0.7170 - acc: 0.5104
 448/5677 [=>............................] - ETA: 8:21 - loss: 0.7204 - acc: 0.4978
 512/5677 [=>............................] - ETA: 8:14 - loss: 0.7214 - acc: 0.4902
 576/5677 [==>...........................] - ETA: 8:14 - loss: 0.7160 - acc: 0.4948
 640/5677 [==>...........................] - ETA: 8:07 - loss: 0.7093 - acc: 0.5094
 704/5677 [==>...........................] - ETA: 8:00 - loss: 0.7108 - acc: 0.5071
 768/5677 [===>..........................] - ETA: 7:53 - loss: 0.7139 - acc: 0.5013
 832/5677 [===>..........................] - ETA: 7:45 - loss: 0.7142 - acc: 0.5012
 896/5677 [===>..........................] - ETA: 7:42 - loss: 0.7120 - acc: 0.5067
 960/5677 [====>.........................] - ETA: 7:35 - loss: 0.7103 - acc: 0.5125
1024/5677 [====>.........................] - ETA: 7:29 - loss: 0.7104 - acc: 0.5137
1088/5677 [====>.........................] - ETA: 7:23 - loss: 0.7110 - acc: 0.5165
1152/5677 [=====>........................] - ETA: 7:16 - loss: 0.7132 - acc: 0.5122
1216/5677 [=====>........................] - ETA: 7:11 - loss: 0.7132 - acc: 0.5123
1280/5677 [=====>........................] - ETA: 7:05 - loss: 0.7134 - acc: 0.5109
1344/5677 [======>.......................] - ETA: 6:59 - loss: 0.7151 - acc: 0.5060
1408/5677 [======>.......................] - ETA: 6:54 - loss: 0.7177 - acc: 0.5021
1472/5677 [======>.......................] - ETA: 6:46 - loss: 0.7188 - acc: 0.5014
1536/5677 [=======>......................] - ETA: 6:40 - loss: 0.7155 - acc: 0.5059
1600/5677 [=======>......................] - ETA: 6:37 - loss: 0.7144 - acc: 0.5075
1664/5677 [=======>......................] - ETA: 6:31 - loss: 0.7132 - acc: 0.5078
1728/5677 [========>.....................] - ETA: 6:25 - loss: 0.7123 - acc: 0.5081
1792/5677 [========>.....................] - ETA: 6:20 - loss: 0.7113 - acc: 0.5123
1856/5677 [========>.....................] - ETA: 6:15 - loss: 0.7119 - acc: 0.5108
1920/5677 [=========>....................] - ETA: 6:10 - loss: 0.7122 - acc: 0.5109
1984/5677 [=========>....................] - ETA: 6:04 - loss: 0.7128 - acc: 0.5101
2048/5677 [=========>....................] - ETA: 5:59 - loss: 0.7123 - acc: 0.5112
2112/5677 [==========>...................] - ETA: 5:52 - loss: 0.7118 - acc: 0.5137
2176/5677 [==========>...................] - ETA: 5:45 - loss: 0.7116 - acc: 0.5142
2240/5677 [==========>...................] - ETA: 5:41 - loss: 0.7101 - acc: 0.5201
2304/5677 [===========>..................] - ETA: 5:35 - loss: 0.7099 - acc: 0.5191
2368/5677 [===========>..................] - ETA: 5:28 - loss: 0.7092 - acc: 0.5211
2432/5677 [===========>..................] - ETA: 5:23 - loss: 0.7078 - acc: 0.5247
2496/5677 [============>.................] - ETA: 5:18 - loss: 0.7089 - acc: 0.5200
2560/5677 [============>.................] - ETA: 5:11 - loss: 0.7086 - acc: 0.5207
2624/5677 [============>.................] - ETA: 5:04 - loss: 0.7077 - acc: 0.5221
2688/5677 [=============>................] - ETA: 4:58 - loss: 0.7070 - acc: 0.5212
2752/5677 [=============>................] - ETA: 4:52 - loss: 0.7071 - acc: 0.5207
2816/5677 [=============>................] - ETA: 4:46 - loss: 0.7068 - acc: 0.5224
2880/5677 [==============>...............] - ETA: 4:40 - loss: 0.7059 - acc: 0.5236
2944/5677 [==============>...............] - ETA: 4:34 - loss: 0.7058 - acc: 0.5248
3008/5677 [==============>...............] - ETA: 4:28 - loss: 0.7065 - acc: 0.5246
3072/5677 [===============>..............] - ETA: 4:21 - loss: 0.7061 - acc: 0.5257
3136/5677 [===============>..............] - ETA: 4:15 - loss: 0.7056 - acc: 0.5252
3200/5677 [===============>..............] - ETA: 4:09 - loss: 0.7059 - acc: 0.5234
3264/5677 [================>.............] - ETA: 4:03 - loss: 0.7061 - acc: 0.5230
3328/5677 [================>.............] - ETA: 3:57 - loss: 0.7055 - acc: 0.5243
3392/5677 [================>.............] - ETA: 3:50 - loss: 0.7057 - acc: 0.5251
3456/5677 [=================>............] - ETA: 3:43 - loss: 0.7052 - acc: 0.5263
3520/5677 [=================>............] - ETA: 3:36 - loss: 0.7048 - acc: 0.5278
3584/5677 [=================>............] - ETA: 3:30 - loss: 0.7049 - acc: 0.5282
3648/5677 [==================>...........] - ETA: 3:24 - loss: 0.7045 - acc: 0.5302
3712/5677 [==================>...........] - ETA: 3:18 - loss: 0.7047 - acc: 0.5291
3776/5677 [==================>...........] - ETA: 3:11 - loss: 0.7050 - acc: 0.5278
3840/5677 [===================>..........] - ETA: 3:05 - loss: 0.7050 - acc: 0.5266
3904/5677 [===================>..........] - ETA: 2:58 - loss: 0.7048 - acc: 0.5274
3968/5677 [===================>..........] - ETA: 2:52 - loss: 0.7047 - acc: 0.5270
4032/5677 [====================>.........] - ETA: 2:45 - loss: 0.7045 - acc: 0.5265
4096/5677 [====================>.........] - ETA: 2:38 - loss: 0.7042 - acc: 0.5269
4160/5677 [====================>.........] - ETA: 2:32 - loss: 0.7041 - acc: 0.5272
4224/5677 [=====================>........] - ETA: 2:26 - loss: 0.7034 - acc: 0.5286
4288/5677 [=====================>........] - ETA: 2:19 - loss: 0.7035 - acc: 0.5278
4352/5677 [=====================>........] - ETA: 2:13 - loss: 0.7035 - acc: 0.5287
4416/5677 [======================>.......] - ETA: 2:06 - loss: 0.7033 - acc: 0.5288
4480/5677 [======================>.......] - ETA: 2:00 - loss: 0.7036 - acc: 0.5281
4544/5677 [=======================>......] - ETA: 1:54 - loss: 0.7036 - acc: 0.5288
4608/5677 [=======================>......] - ETA: 1:47 - loss: 0.7033 - acc: 0.5302
4672/5677 [=======================>......] - ETA: 1:41 - loss: 0.7031 - acc: 0.5304
4736/5677 [========================>.....] - ETA: 1:34 - loss: 0.7027 - acc: 0.5308
4800/5677 [========================>.....] - ETA: 1:28 - loss: 0.7025 - acc: 0.5298
4864/5677 [========================>.....] - ETA: 1:22 - loss: 0.7025 - acc: 0.5302
4928/5677 [=========================>....] - ETA: 1:15 - loss: 0.7022 - acc: 0.5310
4992/5677 [=========================>....] - ETA: 1:09 - loss: 0.7022 - acc: 0.5306
5056/5677 [=========================>....] - ETA: 1:02 - loss: 0.7022 - acc: 0.5309
5120/5677 [==========================>...] - ETA: 56s - loss: 0.7025 - acc: 0.5299 
5184/5677 [==========================>...] - ETA: 49s - loss: 0.7024 - acc: 0.5297
5248/5677 [==========================>...] - ETA: 43s - loss: 0.7026 - acc: 0.5297
5312/5677 [===========================>..] - ETA: 36s - loss: 0.7020 - acc: 0.5311
5376/5677 [===========================>..] - ETA: 30s - loss: 0.7024 - acc: 0.5298
5440/5677 [===========================>..] - ETA: 23s - loss: 0.7024 - acc: 0.5294
5504/5677 [============================>.] - ETA: 17s - loss: 0.7022 - acc: 0.5302
5568/5677 [============================>.] - ETA: 11s - loss: 0.7019 - acc: 0.5302
5632/5677 [============================>.] - ETA: 4s - loss: 0.7022 - acc: 0.5298 
5677/5677 [==============================] - 599s 106ms/step - loss: 0.7023 - acc: 0.5292 - val_loss: 0.6873 - val_acc: 0.5547

Epoch 00002: val_acc improved from 0.55309 to 0.55468, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window02/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 3/10

  64/5677 [..............................] - ETA: 10:25 - loss: 0.6675 - acc: 0.6562
 128/5677 [..............................] - ETA: 10:18 - loss: 0.6875 - acc: 0.5625
 192/5677 [>.............................] - ETA: 10:08 - loss: 0.6920 - acc: 0.5573
 256/5677 [>.............................] - ETA: 9:55 - loss: 0.6947 - acc: 0.5352 
 320/5677 [>.............................] - ETA: 9:42 - loss: 0.6902 - acc: 0.5500
 384/5677 [=>............................] - ETA: 9:30 - loss: 0.6933 - acc: 0.5417
 448/5677 [=>............................] - ETA: 9:25 - loss: 0.6901 - acc: 0.5446
 512/5677 [=>............................] - ETA: 9:12 - loss: 0.6912 - acc: 0.5449
 576/5677 [==>...........................] - ETA: 9:05 - loss: 0.6938 - acc: 0.5399
 640/5677 [==>...........................] - ETA: 8:50 - loss: 0.6919 - acc: 0.5500
 704/5677 [==>...........................] - ETA: 8:37 - loss: 0.6933 - acc: 0.5426
 768/5677 [===>..........................] - ETA: 8:26 - loss: 0.6922 - acc: 0.5417
 832/5677 [===>..........................] - ETA: 8:18 - loss: 0.6943 - acc: 0.5361
 896/5677 [===>..........................] - ETA: 8:07 - loss: 0.6940 - acc: 0.5413
 960/5677 [====>.........................] - ETA: 7:58 - loss: 0.6929 - acc: 0.5427
1024/5677 [====>.........................] - ETA: 7:51 - loss: 0.6930 - acc: 0.5420
1088/5677 [====>.........................] - ETA: 7:42 - loss: 0.6945 - acc: 0.5404
1152/5677 [=====>........................] - ETA: 7:37 - loss: 0.6932 - acc: 0.5417
1216/5677 [=====>........................] - ETA: 7:29 - loss: 0.6928 - acc: 0.5362
1280/5677 [=====>........................] - ETA: 7:22 - loss: 0.6935 - acc: 0.5305
1344/5677 [======>.......................] - ETA: 7:16 - loss: 0.6938 - acc: 0.5312
1408/5677 [======>.......................] - ETA: 7:08 - loss: 0.6929 - acc: 0.5327
1472/5677 [======>.......................] - ETA: 7:00 - loss: 0.6916 - acc: 0.5353
1536/5677 [=======>......................] - ETA: 6:53 - loss: 0.6918 - acc: 0.5339
1600/5677 [=======>......................] - ETA: 6:46 - loss: 0.6912 - acc: 0.5356
1664/5677 [=======>......................] - ETA: 6:40 - loss: 0.6902 - acc: 0.5373
1728/5677 [========>.....................] - ETA: 6:34 - loss: 0.6908 - acc: 0.5353
1792/5677 [========>.....................] - ETA: 6:27 - loss: 0.6898 - acc: 0.5379
1856/5677 [========>.....................] - ETA: 6:21 - loss: 0.6900 - acc: 0.5377
1920/5677 [=========>....................] - ETA: 6:13 - loss: 0.6897 - acc: 0.5396
1984/5677 [=========>....................] - ETA: 6:05 - loss: 0.6906 - acc: 0.5378
2048/5677 [=========>....................] - ETA: 5:59 - loss: 0.6901 - acc: 0.5420
2112/5677 [==========>...................] - ETA: 5:53 - loss: 0.6915 - acc: 0.5374
2176/5677 [==========>...................] - ETA: 5:47 - loss: 0.6919 - acc: 0.5368
2240/5677 [==========>...................] - ETA: 5:40 - loss: 0.6919 - acc: 0.5353
2304/5677 [===========>..................] - ETA: 5:33 - loss: 0.6915 - acc: 0.5365
2368/5677 [===========>..................] - ETA: 5:26 - loss: 0.6911 - acc: 0.5363
2432/5677 [===========>..................] - ETA: 5:18 - loss: 0.6913 - acc: 0.5362
2496/5677 [============>.................] - ETA: 5:11 - loss: 0.6917 - acc: 0.5357
2560/5677 [============>.................] - ETA: 5:03 - loss: 0.6919 - acc: 0.5348
2624/5677 [============>.................] - ETA: 4:55 - loss: 0.6926 - acc: 0.5320
2688/5677 [=============>................] - ETA: 4:48 - loss: 0.6930 - acc: 0.5327
2752/5677 [=============>................] - ETA: 4:41 - loss: 0.6934 - acc: 0.5302
2816/5677 [=============>................] - ETA: 4:33 - loss: 0.6932 - acc: 0.5316
2880/5677 [==============>...............] - ETA: 4:27 - loss: 0.6927 - acc: 0.5316
2944/5677 [==============>...............] - ETA: 4:22 - loss: 0.6927 - acc: 0.5312
3008/5677 [==============>...............] - ETA: 4:15 - loss: 0.6922 - acc: 0.5316
3072/5677 [===============>..............] - ETA: 4:08 - loss: 0.6918 - acc: 0.5326
3136/5677 [===============>..............] - ETA: 4:01 - loss: 0.6920 - acc: 0.5312
3200/5677 [===============>..............] - ETA: 3:55 - loss: 0.6915 - acc: 0.5334
3264/5677 [================>.............] - ETA: 3:48 - loss: 0.6909 - acc: 0.5349
3328/5677 [================>.............] - ETA: 3:42 - loss: 0.6909 - acc: 0.5352
3392/5677 [================>.............] - ETA: 3:35 - loss: 0.6910 - acc: 0.5351
3456/5677 [=================>............] - ETA: 3:29 - loss: 0.6914 - acc: 0.5350
3520/5677 [=================>............] - ETA: 3:22 - loss: 0.6914 - acc: 0.5352
3584/5677 [=================>............] - ETA: 3:16 - loss: 0.6910 - acc: 0.5357
3648/5677 [==================>...........] - ETA: 3:10 - loss: 0.6917 - acc: 0.5329
3712/5677 [==================>...........] - ETA: 3:03 - loss: 0.6915 - acc: 0.5337
3776/5677 [==================>...........] - ETA: 2:57 - loss: 0.6916 - acc: 0.5328
3840/5677 [===================>..........] - ETA: 2:50 - loss: 0.6920 - acc: 0.5320
3904/5677 [===================>..........] - ETA: 2:44 - loss: 0.6915 - acc: 0.5341
3968/5677 [===================>..........] - ETA: 2:38 - loss: 0.6922 - acc: 0.5333
4032/5677 [====================>.........] - ETA: 2:32 - loss: 0.6921 - acc: 0.5340
4096/5677 [====================>.........] - ETA: 2:26 - loss: 0.6922 - acc: 0.5330
4160/5677 [====================>.........] - ETA: 2:20 - loss: 0.6918 - acc: 0.5337
4224/5677 [=====================>........] - ETA: 2:13 - loss: 0.6919 - acc: 0.5336
4288/5677 [=====================>........] - ETA: 2:07 - loss: 0.6917 - acc: 0.5343
4352/5677 [=====================>........] - ETA: 2:01 - loss: 0.6920 - acc: 0.5340
4416/5677 [======================>.......] - ETA: 1:55 - loss: 0.6923 - acc: 0.5333
4480/5677 [======================>.......] - ETA: 1:49 - loss: 0.6919 - acc: 0.5344
4544/5677 [=======================>......] - ETA: 1:43 - loss: 0.6919 - acc: 0.5341
4608/5677 [=======================>......] - ETA: 1:37 - loss: 0.6919 - acc: 0.5341
4672/5677 [=======================>......] - ETA: 1:31 - loss: 0.6921 - acc: 0.5342
4736/5677 [========================>.....] - ETA: 1:25 - loss: 0.6923 - acc: 0.5327
4800/5677 [========================>.....] - ETA: 1:19 - loss: 0.6925 - acc: 0.5325
4864/5677 [========================>.....] - ETA: 1:13 - loss: 0.6923 - acc: 0.5329
4928/5677 [=========================>....] - ETA: 1:08 - loss: 0.6929 - acc: 0.5325
4992/5677 [=========================>....] - ETA: 1:02 - loss: 0.6925 - acc: 0.5337
5056/5677 [=========================>....] - ETA: 56s - loss: 0.6921 - acc: 0.5348 
5120/5677 [==========================>...] - ETA: 50s - loss: 0.6923 - acc: 0.5346
5184/5677 [==========================>...] - ETA: 44s - loss: 0.6925 - acc: 0.5341
5248/5677 [==========================>...] - ETA: 38s - loss: 0.6922 - acc: 0.5356
5312/5677 [===========================>..] - ETA: 32s - loss: 0.6920 - acc: 0.5358
5376/5677 [===========================>..] - ETA: 27s - loss: 0.6916 - acc: 0.5368
5440/5677 [===========================>..] - ETA: 21s - loss: 0.6917 - acc: 0.5368
5504/5677 [============================>.] - ETA: 15s - loss: 0.6916 - acc: 0.5378
5568/5677 [============================>.] - ETA: 9s - loss: 0.6908 - acc: 0.5397 
5632/5677 [============================>.] - ETA: 4s - loss: 0.6911 - acc: 0.5392
5677/5677 [==============================] - 530s 93ms/step - loss: 0.6910 - acc: 0.5392 - val_loss: 0.6930 - val_acc: 0.5293

Epoch 00003: val_acc did not improve from 0.55468
Epoch 4/10

  64/5677 [..............................] - ETA: 7:29 - loss: 0.6670 - acc: 0.5781
 128/5677 [..............................] - ETA: 7:39 - loss: 0.6757 - acc: 0.5312
 192/5677 [>.............................] - ETA: 7:28 - loss: 0.6871 - acc: 0.5208
 256/5677 [>.............................] - ETA: 7:23 - loss: 0.6831 - acc: 0.5273
 320/5677 [>.............................] - ETA: 7:33 - loss: 0.6879 - acc: 0.5250
 384/5677 [=>............................] - ETA: 7:28 - loss: 0.6862 - acc: 0.5312
 448/5677 [=>............................] - ETA: 7:19 - loss: 0.6906 - acc: 0.5357
 512/5677 [=>............................] - ETA: 7:14 - loss: 0.6925 - acc: 0.5371
 576/5677 [==>...........................] - ETA: 7:06 - loss: 0.6926 - acc: 0.5399
 640/5677 [==>...........................] - ETA: 6:58 - loss: 0.6925 - acc: 0.5391
 704/5677 [==>...........................] - ETA: 6:54 - loss: 0.6957 - acc: 0.5327
 768/5677 [===>..........................] - ETA: 6:51 - loss: 0.6939 - acc: 0.5352
 832/5677 [===>..........................] - ETA: 6:44 - loss: 0.6931 - acc: 0.5337
 896/5677 [===>..........................] - ETA: 6:39 - loss: 0.6929 - acc: 0.5391
 960/5677 [====>.........................] - ETA: 6:33 - loss: 0.6918 - acc: 0.5437
1024/5677 [====>.........................] - ETA: 6:30 - loss: 0.6904 - acc: 0.5439
1088/5677 [====>.........................] - ETA: 6:24 - loss: 0.6904 - acc: 0.5432
1152/5677 [=====>........................] - ETA: 6:21 - loss: 0.6906 - acc: 0.5417
1216/5677 [=====>........................] - ETA: 6:13 - loss: 0.6892 - acc: 0.5444
1280/5677 [=====>........................] - ETA: 6:07 - loss: 0.6895 - acc: 0.5461
1344/5677 [======>.......................] - ETA: 6:02 - loss: 0.6885 - acc: 0.5536
1408/5677 [======>.......................] - ETA: 5:55 - loss: 0.6898 - acc: 0.5533
1472/5677 [======>.......................] - ETA: 5:51 - loss: 0.6902 - acc: 0.5543
1536/5677 [=======>......................] - ETA: 5:46 - loss: 0.6907 - acc: 0.5527
1600/5677 [=======>......................] - ETA: 5:40 - loss: 0.6909 - acc: 0.5537
1664/5677 [=======>......................] - ETA: 5:36 - loss: 0.6913 - acc: 0.5511
1728/5677 [========>.....................] - ETA: 5:30 - loss: 0.6914 - acc: 0.5475
1792/5677 [========>.....................] - ETA: 5:24 - loss: 0.6927 - acc: 0.5441
1856/5677 [========>.....................] - ETA: 5:19 - loss: 0.6918 - acc: 0.5453
1920/5677 [=========>....................] - ETA: 5:15 - loss: 0.6919 - acc: 0.5469
1984/5677 [=========>....................] - ETA: 5:10 - loss: 0.6906 - acc: 0.5509
2048/5677 [=========>....................] - ETA: 5:05 - loss: 0.6899 - acc: 0.5532
2112/5677 [==========>...................] - ETA: 5:01 - loss: 0.6899 - acc: 0.5530
2176/5677 [==========>...................] - ETA: 4:57 - loss: 0.6892 - acc: 0.5528
2240/5677 [==========>...................] - ETA: 4:53 - loss: 0.6883 - acc: 0.5540
2304/5677 [===========>..................] - ETA: 4:48 - loss: 0.6886 - acc: 0.5547
2368/5677 [===========>..................] - ETA: 4:43 - loss: 0.6881 - acc: 0.5570
2432/5677 [===========>..................] - ETA: 4:38 - loss: 0.6894 - acc: 0.5547
2496/5677 [============>.................] - ETA: 4:34 - loss: 0.6896 - acc: 0.5545
2560/5677 [============>.................] - ETA: 4:28 - loss: 0.6895 - acc: 0.5539
2624/5677 [============>.................] - ETA: 4:24 - loss: 0.6895 - acc: 0.5518
2688/5677 [=============>................] - ETA: 4:18 - loss: 0.6904 - acc: 0.5502
2752/5677 [=============>................] - ETA: 4:12 - loss: 0.6899 - acc: 0.5494
2816/5677 [=============>................] - ETA: 4:07 - loss: 0.6897 - acc: 0.5515
2880/5677 [==============>...............] - ETA: 4:03 - loss: 0.6891 - acc: 0.5528
2944/5677 [==============>...............] - ETA: 3:58 - loss: 0.6896 - acc: 0.5516
3008/5677 [==============>...............] - ETA: 3:52 - loss: 0.6898 - acc: 0.5499
3072/5677 [===============>..............] - ETA: 3:46 - loss: 0.6888 - acc: 0.5531
3136/5677 [===============>..............] - ETA: 3:41 - loss: 0.6897 - acc: 0.5510
3200/5677 [===============>..............] - ETA: 3:35 - loss: 0.6896 - acc: 0.5516
3264/5677 [================>.............] - ETA: 3:30 - loss: 0.6892 - acc: 0.5527
3328/5677 [================>.............] - ETA: 3:24 - loss: 0.6893 - acc: 0.5526
3392/5677 [================>.............] - ETA: 3:19 - loss: 0.6894 - acc: 0.5510
3456/5677 [=================>............] - ETA: 3:14 - loss: 0.6891 - acc: 0.5503
3520/5677 [=================>............] - ETA: 3:08 - loss: 0.6895 - acc: 0.5489
3584/5677 [=================>............] - ETA: 3:03 - loss: 0.6896 - acc: 0.5488
3648/5677 [==================>...........] - ETA: 2:57 - loss: 0.6897 - acc: 0.5480
3712/5677 [==================>...........] - ETA: 2:51 - loss: 0.6895 - acc: 0.5488
3776/5677 [==================>...........] - ETA: 2:45 - loss: 0.6894 - acc: 0.5498
3840/5677 [===================>..........] - ETA: 2:40 - loss: 0.6888 - acc: 0.5505
3904/5677 [===================>..........] - ETA: 2:34 - loss: 0.6896 - acc: 0.5482
3968/5677 [===================>..........] - ETA: 2:29 - loss: 0.6894 - acc: 0.5484
4032/5677 [====================>.........] - ETA: 2:24 - loss: 0.6895 - acc: 0.5484
4096/5677 [====================>.........] - ETA: 2:18 - loss: 0.6901 - acc: 0.5466
4160/5677 [====================>.........] - ETA: 2:12 - loss: 0.6903 - acc: 0.5457
4224/5677 [=====================>........] - ETA: 2:07 - loss: 0.6905 - acc: 0.5464
4288/5677 [=====================>........] - ETA: 2:01 - loss: 0.6908 - acc: 0.5452
4352/5677 [=====================>........] - ETA: 1:55 - loss: 0.6906 - acc: 0.5457
4416/5677 [======================>.......] - ETA: 1:50 - loss: 0.6908 - acc: 0.5444
4480/5677 [======================>.......] - ETA: 1:45 - loss: 0.6912 - acc: 0.5435
4544/5677 [=======================>......] - ETA: 1:39 - loss: 0.6910 - acc: 0.5447
4608/5677 [=======================>......] - ETA: 1:34 - loss: 0.6908 - acc: 0.5449
4672/5677 [=======================>......] - ETA: 1:28 - loss: 0.6906 - acc: 0.5460
4736/5677 [========================>.....] - ETA: 1:22 - loss: 0.6909 - acc: 0.5450
4800/5677 [========================>.....] - ETA: 1:17 - loss: 0.6907 - acc: 0.5458
4864/5677 [========================>.....] - ETA: 1:11 - loss: 0.6904 - acc: 0.5461
4928/5677 [=========================>....] - ETA: 1:05 - loss: 0.6904 - acc: 0.5471
4992/5677 [=========================>....] - ETA: 1:00 - loss: 0.6905 - acc: 0.5467
5056/5677 [=========================>....] - ETA: 54s - loss: 0.6909 - acc: 0.5457 
5120/5677 [==========================>...] - ETA: 49s - loss: 0.6912 - acc: 0.5443
5184/5677 [==========================>...] - ETA: 43s - loss: 0.6916 - acc: 0.5428
5248/5677 [==========================>...] - ETA: 37s - loss: 0.6919 - acc: 0.5415
5312/5677 [===========================>..] - ETA: 32s - loss: 0.6917 - acc: 0.5416
5376/5677 [===========================>..] - ETA: 26s - loss: 0.6915 - acc: 0.5428
5440/5677 [===========================>..] - ETA: 20s - loss: 0.6911 - acc: 0.5437
5504/5677 [============================>.] - ETA: 15s - loss: 0.6911 - acc: 0.5438
5568/5677 [============================>.] - ETA: 9s - loss: 0.6912 - acc: 0.5431 
5632/5677 [============================>.] - ETA: 3s - loss: 0.6911 - acc: 0.5433
5677/5677 [==============================] - 524s 92ms/step - loss: 0.6911 - acc: 0.5432 - val_loss: 0.6876 - val_acc: 0.5626

Epoch 00004: val_acc improved from 0.55468 to 0.56260, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window02/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 5/10

  64/5677 [..............................] - ETA: 8:43 - loss: 0.6823 - acc: 0.6094
 128/5677 [..............................] - ETA: 8:32 - loss: 0.6882 - acc: 0.5469
 192/5677 [>.............................] - ETA: 8:02 - loss: 0.6844 - acc: 0.5573
 256/5677 [>.............................] - ETA: 7:43 - loss: 0.6837 - acc: 0.5703
 320/5677 [>.............................] - ETA: 7:31 - loss: 0.6833 - acc: 0.5687
 384/5677 [=>............................] - ETA: 7:23 - loss: 0.6870 - acc: 0.5625
 448/5677 [=>............................] - ETA: 7:12 - loss: 0.6835 - acc: 0.5625
 512/5677 [=>............................] - ETA: 7:08 - loss: 0.6825 - acc: 0.5645
 576/5677 [==>...........................] - ETA: 7:13 - loss: 0.6865 - acc: 0.5486
 640/5677 [==>...........................] - ETA: 7:16 - loss: 0.6860 - acc: 0.5437
 704/5677 [==>...........................] - ETA: 7:15 - loss: 0.6882 - acc: 0.5369
 768/5677 [===>..........................] - ETA: 7:12 - loss: 0.6871 - acc: 0.5404
 832/5677 [===>..........................] - ETA: 7:07 - loss: 0.6859 - acc: 0.5445
 896/5677 [===>..........................] - ETA: 7:01 - loss: 0.6840 - acc: 0.5569
 960/5677 [====>.........................] - ETA: 6:53 - loss: 0.6840 - acc: 0.5573
1024/5677 [====>.........................] - ETA: 6:48 - loss: 0.6839 - acc: 0.5586
1088/5677 [====>.........................] - ETA: 6:42 - loss: 0.6847 - acc: 0.5542
1152/5677 [=====>........................] - ETA: 6:37 - loss: 0.6852 - acc: 0.5495
1216/5677 [=====>........................] - ETA: 6:29 - loss: 0.6837 - acc: 0.5535
1280/5677 [=====>........................] - ETA: 6:22 - loss: 0.6845 - acc: 0.5523
1344/5677 [======>.......................] - ETA: 6:23 - loss: 0.6857 - acc: 0.5476
1408/5677 [======>.......................] - ETA: 6:18 - loss: 0.6849 - acc: 0.5483
1472/5677 [======>.......................] - ETA: 6:15 - loss: 0.6846 - acc: 0.5482
1536/5677 [=======>......................] - ETA: 6:11 - loss: 0.6833 - acc: 0.5508
1600/5677 [=======>......................] - ETA: 6:07 - loss: 0.6840 - acc: 0.5500
1664/5677 [=======>......................] - ETA: 6:03 - loss: 0.6863 - acc: 0.5481
1728/5677 [========>.....................] - ETA: 5:56 - loss: 0.6881 - acc: 0.5446
1792/5677 [========>.....................] - ETA: 5:52 - loss: 0.6885 - acc: 0.5435
1856/5677 [========>.....................] - ETA: 5:47 - loss: 0.6879 - acc: 0.5463
1920/5677 [=========>....................] - ETA: 5:44 - loss: 0.6880 - acc: 0.5453
1984/5677 [=========>....................] - ETA: 5:38 - loss: 0.6886 - acc: 0.5439
2048/5677 [=========>....................] - ETA: 5:34 - loss: 0.6884 - acc: 0.5425
2112/5677 [==========>...................] - ETA: 5:29 - loss: 0.6883 - acc: 0.5421
2176/5677 [==========>...................] - ETA: 5:24 - loss: 0.6887 - acc: 0.5409
2240/5677 [==========>...................] - ETA: 5:20 - loss: 0.6884 - acc: 0.5402
2304/5677 [===========>..................] - ETA: 5:15 - loss: 0.6884 - acc: 0.5395
2368/5677 [===========>..................] - ETA: 5:10 - loss: 0.6881 - acc: 0.5393
2432/5677 [===========>..................] - ETA: 5:04 - loss: 0.6870 - acc: 0.5432
2496/5677 [============>.................] - ETA: 4:58 - loss: 0.6861 - acc: 0.5465
2560/5677 [============>.................] - ETA: 4:53 - loss: 0.6850 - acc: 0.5496
2624/5677 [============>.................] - ETA: 4:47 - loss: 0.6857 - acc: 0.5473
2688/5677 [=============>................] - ETA: 4:42 - loss: 0.6865 - acc: 0.5465
2752/5677 [=============>................] - ETA: 4:36 - loss: 0.6866 - acc: 0.5461
2816/5677 [=============>................] - ETA: 4:31 - loss: 0.6868 - acc: 0.5458
2880/5677 [==============>...............] - ETA: 4:26 - loss: 0.6869 - acc: 0.5451
2944/5677 [==============>...............] - ETA: 4:19 - loss: 0.6863 - acc: 0.5472
3008/5677 [==============>...............] - ETA: 4:13 - loss: 0.6868 - acc: 0.5455
3072/5677 [===============>..............] - ETA: 4:06 - loss: 0.6871 - acc: 0.5449
3136/5677 [===============>..............] - ETA: 3:59 - loss: 0.6869 - acc: 0.5453
3200/5677 [===============>..............] - ETA: 3:53 - loss: 0.6871 - acc: 0.5444
3264/5677 [================>.............] - ETA: 3:46 - loss: 0.6863 - acc: 0.5466
3328/5677 [================>.............] - ETA: 3:40 - loss: 0.6865 - acc: 0.5463
3392/5677 [================>.............] - ETA: 3:34 - loss: 0.6864 - acc: 0.5460
3456/5677 [=================>............] - ETA: 3:28 - loss: 0.6861 - acc: 0.5472
3520/5677 [=================>............] - ETA: 3:22 - loss: 0.6852 - acc: 0.5503
3584/5677 [=================>............] - ETA: 3:17 - loss: 0.6853 - acc: 0.5499
3648/5677 [==================>...........] - ETA: 3:10 - loss: 0.6851 - acc: 0.5513
3712/5677 [==================>...........] - ETA: 3:04 - loss: 0.6853 - acc: 0.5509
3776/5677 [==================>...........] - ETA: 2:57 - loss: 0.6853 - acc: 0.5511
3840/5677 [===================>..........] - ETA: 2:51 - loss: 0.6843 - acc: 0.5542
3904/5677 [===================>..........] - ETA: 2:45 - loss: 0.6837 - acc: 0.5558
3968/5677 [===================>..........] - ETA: 2:38 - loss: 0.6836 - acc: 0.5559
4032/5677 [====================>.........] - ETA: 2:33 - loss: 0.6829 - acc: 0.5573
4096/5677 [====================>.........] - ETA: 2:27 - loss: 0.6833 - acc: 0.5569
4160/5677 [====================>.........] - ETA: 2:21 - loss: 0.6837 - acc: 0.5570
4224/5677 [=====================>........] - ETA: 2:15 - loss: 0.6838 - acc: 0.5578
4288/5677 [=====================>........] - ETA: 2:09 - loss: 0.6845 - acc: 0.5569
4352/5677 [=====================>........] - ETA: 2:03 - loss: 0.6845 - acc: 0.5565
4416/5677 [======================>.......] - ETA: 1:57 - loss: 0.6844 - acc: 0.5566
4480/5677 [======================>.......] - ETA: 1:51 - loss: 0.6841 - acc: 0.5567
4544/5677 [=======================>......] - ETA: 1:45 - loss: 0.6842 - acc: 0.5566
4608/5677 [=======================>......] - ETA: 1:39 - loss: 0.6839 - acc: 0.5573
4672/5677 [=======================>......] - ETA: 1:33 - loss: 0.6836 - acc: 0.5578
4736/5677 [========================>.....] - ETA: 1:27 - loss: 0.6835 - acc: 0.5576
4800/5677 [========================>.....] - ETA: 1:21 - loss: 0.6832 - acc: 0.5585
4864/5677 [========================>.....] - ETA: 1:15 - loss: 0.6829 - acc: 0.5588
4928/5677 [=========================>....] - ETA: 1:09 - loss: 0.6834 - acc: 0.5578
4992/5677 [=========================>....] - ETA: 1:03 - loss: 0.6835 - acc: 0.5587
5056/5677 [=========================>....] - ETA: 57s - loss: 0.6832 - acc: 0.5589 
5120/5677 [==========================>...] - ETA: 51s - loss: 0.6834 - acc: 0.5584
5184/5677 [==========================>...] - ETA: 45s - loss: 0.6832 - acc: 0.5588
5248/5677 [==========================>...] - ETA: 39s - loss: 0.6832 - acc: 0.5581
5312/5677 [===========================>..] - ETA: 33s - loss: 0.6837 - acc: 0.5565
5376/5677 [===========================>..] - ETA: 27s - loss: 0.6835 - acc: 0.5575
5440/5677 [===========================>..] - ETA: 21s - loss: 0.6836 - acc: 0.5572
5504/5677 [============================>.] - ETA: 16s - loss: 0.6833 - acc: 0.5583
5568/5677 [============================>.] - ETA: 10s - loss: 0.6830 - acc: 0.5594
5632/5677 [============================>.] - ETA: 4s - loss: 0.6828 - acc: 0.5595 
5677/5677 [==============================] - 549s 97ms/step - loss: 0.6829 - acc: 0.5595 - val_loss: 0.6791 - val_acc: 0.5563

Epoch 00005: val_acc did not improve from 0.56260
Epoch 6/10

  64/5677 [..............................] - ETA: 7:36 - loss: 0.6547 - acc: 0.6406
 128/5677 [..............................] - ETA: 7:23 - loss: 0.6618 - acc: 0.5859
 192/5677 [>.............................] - ETA: 7:33 - loss: 0.6651 - acc: 0.5833
 256/5677 [>.............................] - ETA: 7:36 - loss: 0.6564 - acc: 0.6094
 320/5677 [>.............................] - ETA: 8:02 - loss: 0.6671 - acc: 0.5719
 384/5677 [=>............................] - ETA: 8:06 - loss: 0.6661 - acc: 0.5729
 448/5677 [=>............................] - ETA: 8:02 - loss: 0.6663 - acc: 0.5759
 512/5677 [=>............................] - ETA: 7:57 - loss: 0.6671 - acc: 0.5723
 576/5677 [==>...........................] - ETA: 7:50 - loss: 0.6721 - acc: 0.5747
 640/5677 [==>...........................] - ETA: 7:41 - loss: 0.6728 - acc: 0.5750
 704/5677 [==>...........................] - ETA: 7:32 - loss: 0.6728 - acc: 0.5710
 768/5677 [===>..........................] - ETA: 7:22 - loss: 0.6735 - acc: 0.5690
 832/5677 [===>..........................] - ETA: 7:14 - loss: 0.6751 - acc: 0.5685
 896/5677 [===>..........................] - ETA: 7:03 - loss: 0.6762 - acc: 0.5658
 960/5677 [====>.........................] - ETA: 6:56 - loss: 0.6741 - acc: 0.5687
1024/5677 [====>.........................] - ETA: 6:53 - loss: 0.6776 - acc: 0.5615
1088/5677 [====>.........................] - ETA: 6:53 - loss: 0.6765 - acc: 0.5662
1152/5677 [=====>........................] - ETA: 6:47 - loss: 0.6762 - acc: 0.5668
1216/5677 [=====>........................] - ETA: 6:42 - loss: 0.6787 - acc: 0.5641
1280/5677 [=====>........................] - ETA: 6:36 - loss: 0.6798 - acc: 0.5625
1344/5677 [======>.......................] - ETA: 6:30 - loss: 0.6790 - acc: 0.5632
1408/5677 [======>.......................] - ETA: 6:21 - loss: 0.6805 - acc: 0.5618
1472/5677 [======>.......................] - ETA: 6:13 - loss: 0.6809 - acc: 0.5611
1536/5677 [=======>......................] - ETA: 6:05 - loss: 0.6803 - acc: 0.5658
1600/5677 [=======>......................] - ETA: 5:58 - loss: 0.6789 - acc: 0.5694
1664/5677 [=======>......................] - ETA: 5:50 - loss: 0.6784 - acc: 0.5703
1728/5677 [========>.....................] - ETA: 5:44 - loss: 0.6792 - acc: 0.5694
1792/5677 [========>.....................] - ETA: 5:41 - loss: 0.6774 - acc: 0.5737
1856/5677 [========>.....................] - ETA: 5:36 - loss: 0.6779 - acc: 0.5722
1920/5677 [=========>....................] - ETA: 5:31 - loss: 0.6790 - acc: 0.5693
1984/5677 [=========>....................] - ETA: 5:25 - loss: 0.6812 - acc: 0.5660
2048/5677 [=========>....................] - ETA: 5:20 - loss: 0.6815 - acc: 0.5664
2112/5677 [==========>...................] - ETA: 5:16 - loss: 0.6797 - acc: 0.5691
2176/5677 [==========>...................] - ETA: 5:10 - loss: 0.6787 - acc: 0.5703
2240/5677 [==========>...................] - ETA: 5:04 - loss: 0.6789 - acc: 0.5710
2304/5677 [===========>..................] - ETA: 4:59 - loss: 0.6788 - acc: 0.5699
2368/5677 [===========>..................] - ETA: 4:53 - loss: 0.6791 - acc: 0.5684
2432/5677 [===========>..................] - ETA: 4:47 - loss: 0.6800 - acc: 0.5658
2496/5677 [============>.................] - ETA: 4:41 - loss: 0.6812 - acc: 0.5621
2560/5677 [============>.................] - ETA: 4:36 - loss: 0.6810 - acc: 0.5633
2624/5677 [============>.................] - ETA: 4:31 - loss: 0.6819 - acc: 0.5625
2688/5677 [=============>................] - ETA: 4:26 - loss: 0.6825 - acc: 0.5606
2752/5677 [=============>................] - ETA: 4:20 - loss: 0.6819 - acc: 0.5621
2816/5677 [=============>................] - ETA: 4:16 - loss: 0.6821 - acc: 0.5625
2880/5677 [==============>...............] - ETA: 4:11 - loss: 0.6823 - acc: 0.5635
2944/5677 [==============>...............] - ETA: 4:05 - loss: 0.6826 - acc: 0.5635
3008/5677 [==============>...............] - ETA: 4:00 - loss: 0.6834 - acc: 0.5612
3072/5677 [===============>..............] - ETA: 3:55 - loss: 0.6837 - acc: 0.5596
3136/5677 [===============>..............] - ETA: 3:49 - loss: 0.6837 - acc: 0.5599
3200/5677 [===============>..............] - ETA: 3:45 - loss: 0.6840 - acc: 0.5581
3264/5677 [================>.............] - ETA: 3:39 - loss: 0.6844 - acc: 0.5570
3328/5677 [================>.............] - ETA: 3:34 - loss: 0.6842 - acc: 0.5568
3392/5677 [================>.............] - ETA: 3:29 - loss: 0.6841 - acc: 0.5566
3456/5677 [=================>............] - ETA: 3:23 - loss: 0.6842 - acc: 0.5567
3520/5677 [=================>............] - ETA: 3:18 - loss: 0.6837 - acc: 0.5577
3584/5677 [=================>............] - ETA: 3:12 - loss: 0.6833 - acc: 0.5578
3648/5677 [==================>...........] - ETA: 3:06 - loss: 0.6833 - acc: 0.5587
3712/5677 [==================>...........] - ETA: 3:00 - loss: 0.6834 - acc: 0.5577
3776/5677 [==================>...........] - ETA: 2:55 - loss: 0.6831 - acc: 0.5577
3840/5677 [===================>..........] - ETA: 2:49 - loss: 0.6827 - acc: 0.5578
3904/5677 [===================>..........] - ETA: 2:44 - loss: 0.6817 - acc: 0.5605
3968/5677 [===================>..........] - ETA: 2:38 - loss: 0.6816 - acc: 0.5612
4032/5677 [====================>.........] - ETA: 2:32 - loss: 0.6815 - acc: 0.5613
4096/5677 [====================>.........] - ETA: 2:27 - loss: 0.6816 - acc: 0.5603
4160/5677 [====================>.........] - ETA: 2:21 - loss: 0.6813 - acc: 0.5618
4224/5677 [=====================>........] - ETA: 2:15 - loss: 0.6814 - acc: 0.5620
4288/5677 [=====================>........] - ETA: 2:09 - loss: 0.6814 - acc: 0.5620
4352/5677 [=====================>........] - ETA: 2:03 - loss: 0.6815 - acc: 0.5618
4416/5677 [======================>.......] - ETA: 1:58 - loss: 0.6816 - acc: 0.5614
4480/5677 [======================>.......] - ETA: 1:52 - loss: 0.6817 - acc: 0.5612
4544/5677 [=======================>......] - ETA: 1:46 - loss: 0.6821 - acc: 0.5616
4608/5677 [=======================>......] - ETA: 1:40 - loss: 0.6821 - acc: 0.5623
4672/5677 [=======================>......] - ETA: 1:34 - loss: 0.6825 - acc: 0.5616
4736/5677 [========================>.....] - ETA: 1:28 - loss: 0.6821 - acc: 0.5621
4800/5677 [========================>.....] - ETA: 1:22 - loss: 0.6818 - acc: 0.5625
4864/5677 [========================>.....] - ETA: 1:15 - loss: 0.6814 - acc: 0.5627
4928/5677 [=========================>....] - ETA: 1:09 - loss: 0.6810 - acc: 0.5637
4992/5677 [=========================>....] - ETA: 1:03 - loss: 0.6804 - acc: 0.5643
5056/5677 [=========================>....] - ETA: 57s - loss: 0.6803 - acc: 0.5653 
5120/5677 [==========================>...] - ETA: 51s - loss: 0.6810 - acc: 0.5637
5184/5677 [==========================>...] - ETA: 45s - loss: 0.6812 - acc: 0.5631
5248/5677 [==========================>...] - ETA: 39s - loss: 0.6814 - acc: 0.5631
5312/5677 [===========================>..] - ETA: 33s - loss: 0.6814 - acc: 0.5625
5376/5677 [===========================>..] - ETA: 27s - loss: 0.6811 - acc: 0.5634
5440/5677 [===========================>..] - ETA: 21s - loss: 0.6814 - acc: 0.5631
5504/5677 [============================>.] - ETA: 15s - loss: 0.6813 - acc: 0.5636
5568/5677 [============================>.] - ETA: 10s - loss: 0.6811 - acc: 0.5639
5632/5677 [============================>.] - ETA: 4s - loss: 0.6810 - acc: 0.5646 
5677/5677 [==============================] - 543s 96ms/step - loss: 0.6812 - acc: 0.5644 - val_loss: 0.6781 - val_acc: 0.5705

Epoch 00006: val_acc improved from 0.56260 to 0.57052, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window02/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 7/10

  64/5677 [..............................] - ETA: 8:55 - loss: 0.6181 - acc: 0.6719
 128/5677 [..............................] - ETA: 8:20 - loss: 0.6529 - acc: 0.6250
 192/5677 [>.............................] - ETA: 8:04 - loss: 0.6406 - acc: 0.6354
 256/5677 [>.............................] - ETA: 7:56 - loss: 0.6480 - acc: 0.6211
 320/5677 [>.............................] - ETA: 7:50 - loss: 0.6400 - acc: 0.6438
 384/5677 [=>............................] - ETA: 7:36 - loss: 0.6576 - acc: 0.6172
 448/5677 [=>............................] - ETA: 7:33 - loss: 0.6683 - acc: 0.6004
 512/5677 [=>............................] - ETA: 7:25 - loss: 0.6669 - acc: 0.5996
 576/5677 [==>...........................] - ETA: 7:27 - loss: 0.6667 - acc: 0.6024
 640/5677 [==>...........................] - ETA: 7:34 - loss: 0.6690 - acc: 0.5984
 704/5677 [==>...........................] - ETA: 7:27 - loss: 0.6712 - acc: 0.5923
 768/5677 [===>..........................] - ETA: 7:24 - loss: 0.6702 - acc: 0.5924
 832/5677 [===>..........................] - ETA: 7:18 - loss: 0.6714 - acc: 0.5901
 896/5677 [===>..........................] - ETA: 7:13 - loss: 0.6746 - acc: 0.5804
 960/5677 [====>.........................] - ETA: 7:07 - loss: 0.6768 - acc: 0.5771
1024/5677 [====>.........................] - ETA: 6:59 - loss: 0.6796 - acc: 0.5713
1088/5677 [====>.........................] - ETA: 6:51 - loss: 0.6827 - acc: 0.5653
1152/5677 [=====>........................] - ETA: 6:44 - loss: 0.6832 - acc: 0.5616
1216/5677 [=====>........................] - ETA: 6:38 - loss: 0.6826 - acc: 0.5625
1280/5677 [=====>........................] - ETA: 6:37 - loss: 0.6845 - acc: 0.5594
1344/5677 [======>.......................] - ETA: 6:33 - loss: 0.6836 - acc: 0.5603
1408/5677 [======>.......................] - ETA: 6:27 - loss: 0.6819 - acc: 0.5661
1472/5677 [======>.......................] - ETA: 6:22 - loss: 0.6815 - acc: 0.5652
1536/5677 [=======>......................] - ETA: 6:18 - loss: 0.6804 - acc: 0.5703
1600/5677 [=======>......................] - ETA: 6:11 - loss: 0.6790 - acc: 0.5737
1664/5677 [=======>......................] - ETA: 6:05 - loss: 0.6799 - acc: 0.5715
1728/5677 [========>.....................] - ETA: 5:59 - loss: 0.6792 - acc: 0.5729
1792/5677 [========>.....................] - ETA: 5:52 - loss: 0.6805 - acc: 0.5698
1856/5677 [========>.....................] - ETA: 5:47 - loss: 0.6820 - acc: 0.5690
1920/5677 [=========>....................] - ETA: 5:41 - loss: 0.6813 - acc: 0.5682
1984/5677 [=========>....................] - ETA: 5:38 - loss: 0.6824 - acc: 0.5660
2048/5677 [=========>....................] - ETA: 5:32 - loss: 0.6833 - acc: 0.5635
2112/5677 [==========>...................] - ETA: 5:27 - loss: 0.6823 - acc: 0.5653
2176/5677 [==========>...................] - ETA: 5:21 - loss: 0.6836 - acc: 0.5634
2240/5677 [==========>...................] - ETA: 5:16 - loss: 0.6823 - acc: 0.5638
2304/5677 [===========>..................] - ETA: 5:09 - loss: 0.6826 - acc: 0.5638
2368/5677 [===========>..................] - ETA: 5:03 - loss: 0.6827 - acc: 0.5633
2432/5677 [===========>..................] - ETA: 4:57 - loss: 0.6825 - acc: 0.5641
2496/5677 [============>.................] - ETA: 4:51 - loss: 0.6821 - acc: 0.5661
2560/5677 [============>.................] - ETA: 4:45 - loss: 0.6809 - acc: 0.5687
2624/5677 [============>.................] - ETA: 4:40 - loss: 0.6806 - acc: 0.5713
2688/5677 [=============>................] - ETA: 4:35 - loss: 0.6815 - acc: 0.5692
2752/5677 [=============>................] - ETA: 4:29 - loss: 0.6821 - acc: 0.5680
2816/5677 [=============>................] - ETA: 4:25 - loss: 0.6822 - acc: 0.5675
2880/5677 [==============>...............] - ETA: 4:19 - loss: 0.6819 - acc: 0.5687
2944/5677 [==============>...............] - ETA: 4:13 - loss: 0.6818 - acc: 0.5683
3008/5677 [==============>...............] - ETA: 4:07 - loss: 0.6807 - acc: 0.5698
3072/5677 [===============>..............] - ETA: 4:01 - loss: 0.6808 - acc: 0.5687
3136/5677 [===============>..............] - ETA: 3:54 - loss: 0.6809 - acc: 0.5686
3200/5677 [===============>..............] - ETA: 3:48 - loss: 0.6817 - acc: 0.5675
3264/5677 [================>.............] - ETA: 3:42 - loss: 0.6806 - acc: 0.5692
3328/5677 [================>.............] - ETA: 3:36 - loss: 0.6808 - acc: 0.5685
3392/5677 [================>.............] - ETA: 3:31 - loss: 0.6802 - acc: 0.5708
3456/5677 [=================>............] - ETA: 3:25 - loss: 0.6792 - acc: 0.5744
3520/5677 [=================>............] - ETA: 3:19 - loss: 0.6801 - acc: 0.5716
3584/5677 [=================>............] - ETA: 3:13 - loss: 0.6801 - acc: 0.5706
3648/5677 [==================>...........] - ETA: 3:07 - loss: 0.6801 - acc: 0.5702
3712/5677 [==================>...........] - ETA: 3:01 - loss: 0.6803 - acc: 0.5698
3776/5677 [==================>...........] - ETA: 2:55 - loss: 0.6802 - acc: 0.5697
3840/5677 [===================>..........] - ETA: 2:49 - loss: 0.6799 - acc: 0.5698
3904/5677 [===================>..........] - ETA: 2:43 - loss: 0.6799 - acc: 0.5704
3968/5677 [===================>..........] - ETA: 2:37 - loss: 0.6801 - acc: 0.5698
4032/5677 [====================>.........] - ETA: 2:32 - loss: 0.6797 - acc: 0.5697
4096/5677 [====================>.........] - ETA: 2:26 - loss: 0.6791 - acc: 0.5710
4160/5677 [====================>.........] - ETA: 2:20 - loss: 0.6783 - acc: 0.5721
4224/5677 [=====================>........] - ETA: 2:14 - loss: 0.6788 - acc: 0.5715
4288/5677 [=====================>........] - ETA: 2:08 - loss: 0.6789 - acc: 0.5714
4352/5677 [=====================>........] - ETA: 2:03 - loss: 0.6785 - acc: 0.5726
4416/5677 [======================>.......] - ETA: 1:57 - loss: 0.6787 - acc: 0.5722
4480/5677 [======================>.......] - ETA: 1:51 - loss: 0.6784 - acc: 0.5730
4544/5677 [=======================>......] - ETA: 1:45 - loss: 0.6779 - acc: 0.5737
4608/5677 [=======================>......] - ETA: 1:40 - loss: 0.6781 - acc: 0.5720
4672/5677 [=======================>......] - ETA: 1:34 - loss: 0.6776 - acc: 0.5734
4736/5677 [========================>.....] - ETA: 1:28 - loss: 0.6783 - acc: 0.5724
4800/5677 [========================>.....] - ETA: 1:22 - loss: 0.6784 - acc: 0.5727
4864/5677 [========================>.....] - ETA: 1:16 - loss: 0.6785 - acc: 0.5722
4928/5677 [=========================>....] - ETA: 1:10 - loss: 0.6784 - acc: 0.5733
4992/5677 [=========================>....] - ETA: 1:04 - loss: 0.6787 - acc: 0.5729
5056/5677 [=========================>....] - ETA: 58s - loss: 0.6788 - acc: 0.5732 
5120/5677 [==========================>...] - ETA: 52s - loss: 0.6788 - acc: 0.5742
5184/5677 [==========================>...] - ETA: 46s - loss: 0.6785 - acc: 0.5747
5248/5677 [==========================>...] - ETA: 40s - loss: 0.6784 - acc: 0.5747
5312/5677 [===========================>..] - ETA: 34s - loss: 0.6784 - acc: 0.5751
5376/5677 [===========================>..] - ETA: 28s - loss: 0.6783 - acc: 0.5757
5440/5677 [===========================>..] - ETA: 22s - loss: 0.6778 - acc: 0.5768
5504/5677 [============================>.] - ETA: 16s - loss: 0.6780 - acc: 0.5765
5568/5677 [============================>.] - ETA: 10s - loss: 0.6778 - acc: 0.5767
5632/5677 [============================>.] - ETA: 4s - loss: 0.6780 - acc: 0.5762 
5677/5677 [==============================] - 559s 98ms/step - loss: 0.6781 - acc: 0.5765 - val_loss: 0.6919 - val_acc: 0.5515

Epoch 00007: val_acc did not improve from 0.57052
Epoch 8/10

  64/5677 [..............................] - ETA: 8:42 - loss: 0.7137 - acc: 0.5625
 128/5677 [..............................] - ETA: 8:52 - loss: 0.6891 - acc: 0.5859
 192/5677 [>.............................] - ETA: 8:51 - loss: 0.6768 - acc: 0.5885
 256/5677 [>.............................] - ETA: 8:43 - loss: 0.6869 - acc: 0.5625
 320/5677 [>.............................] - ETA: 8:48 - loss: 0.6799 - acc: 0.5781
 384/5677 [=>............................] - ETA: 8:35 - loss: 0.6753 - acc: 0.5807
 448/5677 [=>............................] - ETA: 8:26 - loss: 0.6765 - acc: 0.5759
 512/5677 [=>............................] - ETA: 8:22 - loss: 0.6801 - acc: 0.5781
 576/5677 [==>...........................] - ETA: 8:09 - loss: 0.6809 - acc: 0.5694
 640/5677 [==>...........................] - ETA: 8:01 - loss: 0.6764 - acc: 0.5891
 704/5677 [==>...........................] - ETA: 7:54 - loss: 0.6753 - acc: 0.5895
 768/5677 [===>..........................] - ETA: 7:42 - loss: 0.6734 - acc: 0.5898
 832/5677 [===>..........................] - ETA: 7:40 - loss: 0.6747 - acc: 0.5853
 896/5677 [===>..........................] - ETA: 7:36 - loss: 0.6764 - acc: 0.5815
 960/5677 [====>.........................] - ETA: 7:27 - loss: 0.6769 - acc: 0.5792
1024/5677 [====>.........................] - ETA: 7:19 - loss: 0.6772 - acc: 0.5811
1088/5677 [====>.........................] - ETA: 7:10 - loss: 0.6743 - acc: 0.5846
1152/5677 [=====>........................] - ETA: 7:05 - loss: 0.6749 - acc: 0.5842
1216/5677 [=====>........................] - ETA: 6:58 - loss: 0.6743 - acc: 0.5863
1280/5677 [=====>........................] - ETA: 6:53 - loss: 0.6748 - acc: 0.5836
1344/5677 [======>.......................] - ETA: 6:45 - loss: 0.6730 - acc: 0.5885
1408/5677 [======>.......................] - ETA: 6:38 - loss: 0.6726 - acc: 0.5881
1472/5677 [======>.......................] - ETA: 6:32 - loss: 0.6746 - acc: 0.5856
1536/5677 [=======>......................] - ETA: 6:26 - loss: 0.6726 - acc: 0.5885
1600/5677 [=======>......................] - ETA: 6:18 - loss: 0.6706 - acc: 0.5925
1664/5677 [=======>......................] - ETA: 6:12 - loss: 0.6718 - acc: 0.5883
1728/5677 [========>.....................] - ETA: 6:05 - loss: 0.6705 - acc: 0.5903
1792/5677 [========>.....................] - ETA: 5:59 - loss: 0.6713 - acc: 0.5898
1856/5677 [========>.....................] - ETA: 5:52 - loss: 0.6713 - acc: 0.5889
1920/5677 [=========>....................] - ETA: 5:45 - loss: 0.6706 - acc: 0.5896
1984/5677 [=========>....................] - ETA: 5:38 - loss: 0.6720 - acc: 0.5862
2048/5677 [=========>....................] - ETA: 5:32 - loss: 0.6713 - acc: 0.5884
2112/5677 [==========>...................] - ETA: 5:26 - loss: 0.6722 - acc: 0.5862
2176/5677 [==========>...................] - ETA: 5:20 - loss: 0.6716 - acc: 0.5864
2240/5677 [==========>...................] - ETA: 5:14 - loss: 0.6718 - acc: 0.5844
2304/5677 [===========>..................] - ETA: 5:09 - loss: 0.6730 - acc: 0.5820
2368/5677 [===========>..................] - ETA: 5:02 - loss: 0.6735 - acc: 0.5807
2432/5677 [===========>..................] - ETA: 4:56 - loss: 0.6734 - acc: 0.5806
2496/5677 [============>.................] - ETA: 4:51 - loss: 0.6733 - acc: 0.5809
2560/5677 [============>.................] - ETA: 4:45 - loss: 0.6741 - acc: 0.5781
2624/5677 [============>.................] - ETA: 4:39 - loss: 0.6746 - acc: 0.5785
2688/5677 [=============>................] - ETA: 4:33 - loss: 0.6748 - acc: 0.5789
2752/5677 [=============>................] - ETA: 4:27 - loss: 0.6754 - acc: 0.5767
2816/5677 [=============>................] - ETA: 4:20 - loss: 0.6755 - acc: 0.5756
2880/5677 [==============>...............] - ETA: 4:13 - loss: 0.6745 - acc: 0.5767
2944/5677 [==============>...............] - ETA: 4:07 - loss: 0.6741 - acc: 0.5771
3008/5677 [==============>...............] - ETA: 4:01 - loss: 0.6743 - acc: 0.5768
3072/5677 [===============>..............] - ETA: 3:54 - loss: 0.6741 - acc: 0.5778
3136/5677 [===============>..............] - ETA: 3:48 - loss: 0.6736 - acc: 0.5784
3200/5677 [===============>..............] - ETA: 3:42 - loss: 0.6739 - acc: 0.5778
3264/5677 [================>.............] - ETA: 3:36 - loss: 0.6749 - acc: 0.5760
3328/5677 [================>.............] - ETA: 3:30 - loss: 0.6751 - acc: 0.5757
3392/5677 [================>.............] - ETA: 3:25 - loss: 0.6768 - acc: 0.5734
3456/5677 [=================>............] - ETA: 3:19 - loss: 0.6769 - acc: 0.5729
3520/5677 [=================>............] - ETA: 3:13 - loss: 0.6760 - acc: 0.5750
3584/5677 [=================>............] - ETA: 3:07 - loss: 0.6755 - acc: 0.5762
3648/5677 [==================>...........] - ETA: 3:01 - loss: 0.6755 - acc: 0.5746
3712/5677 [==================>...........] - ETA: 2:55 - loss: 0.6757 - acc: 0.5741
3776/5677 [==================>...........] - ETA: 2:49 - loss: 0.6760 - acc: 0.5731
3840/5677 [===================>..........] - ETA: 2:43 - loss: 0.6762 - acc: 0.5724
3904/5677 [===================>..........] - ETA: 2:37 - loss: 0.6770 - acc: 0.5704
3968/5677 [===================>..........] - ETA: 2:32 - loss: 0.6771 - acc: 0.5698
4032/5677 [====================>.........] - ETA: 2:26 - loss: 0.6773 - acc: 0.5694
4096/5677 [====================>.........] - ETA: 2:20 - loss: 0.6778 - acc: 0.5691
4160/5677 [====================>.........] - ETA: 2:15 - loss: 0.6780 - acc: 0.5690
4224/5677 [=====================>........] - ETA: 2:09 - loss: 0.6784 - acc: 0.5677
4288/5677 [=====================>........] - ETA: 2:03 - loss: 0.6781 - acc: 0.5688
4352/5677 [=====================>........] - ETA: 1:57 - loss: 0.6782 - acc: 0.5689
4416/5677 [======================>.......] - ETA: 1:51 - loss: 0.6785 - acc: 0.5684
4480/5677 [======================>.......] - ETA: 1:45 - loss: 0.6787 - acc: 0.5674
4544/5677 [=======================>......] - ETA: 1:39 - loss: 0.6786 - acc: 0.5678
4608/5677 [=======================>......] - ETA: 1:34 - loss: 0.6783 - acc: 0.5690
4672/5677 [=======================>......] - ETA: 1:28 - loss: 0.6786 - acc: 0.5689
4736/5677 [========================>.....] - ETA: 1:22 - loss: 0.6787 - acc: 0.5686
4800/5677 [========================>.....] - ETA: 1:17 - loss: 0.6785 - acc: 0.5696
4864/5677 [========================>.....] - ETA: 1:11 - loss: 0.6779 - acc: 0.5718
4928/5677 [=========================>....] - ETA: 1:06 - loss: 0.6778 - acc: 0.5724
4992/5677 [=========================>....] - ETA: 1:00 - loss: 0.6778 - acc: 0.5723
5056/5677 [=========================>....] - ETA: 54s - loss: 0.6778 - acc: 0.5716 
5120/5677 [==========================>...] - ETA: 48s - loss: 0.6777 - acc: 0.5721
5184/5677 [==========================>...] - ETA: 43s - loss: 0.6772 - acc: 0.5731
5248/5677 [==========================>...] - ETA: 37s - loss: 0.6773 - acc: 0.5728
5312/5677 [===========================>..] - ETA: 32s - loss: 0.6775 - acc: 0.5719
5376/5677 [===========================>..] - ETA: 26s - loss: 0.6781 - acc: 0.5707
5440/5677 [===========================>..] - ETA: 20s - loss: 0.6783 - acc: 0.5704
5504/5677 [============================>.] - ETA: 15s - loss: 0.6784 - acc: 0.5701
5568/5677 [============================>.] - ETA: 9s - loss: 0.6787 - acc: 0.5690 
5632/5677 [============================>.] - ETA: 3s - loss: 0.6784 - acc: 0.5691
5677/5677 [==============================] - 518s 91ms/step - loss: 0.6780 - acc: 0.5697 - val_loss: 0.6908 - val_acc: 0.5499

Epoch 00008: val_acc did not improve from 0.57052
Epoch 9/10

  64/5677 [..............................] - ETA: 7:28 - loss: 0.6624 - acc: 0.6094
 128/5677 [..............................] - ETA: 7:31 - loss: 0.6705 - acc: 0.5703
 192/5677 [>.............................] - ETA: 7:44 - loss: 0.6610 - acc: 0.6094
 256/5677 [>.............................] - ETA: 7:45 - loss: 0.6613 - acc: 0.6055
 320/5677 [>.............................] - ETA: 8:01 - loss: 0.6571 - acc: 0.6094
 384/5677 [=>............................] - ETA: 7:54 - loss: 0.6508 - acc: 0.6432
 448/5677 [=>............................] - ETA: 7:59 - loss: 0.6519 - acc: 0.6362
 512/5677 [=>............................] - ETA: 7:45 - loss: 0.6567 - acc: 0.6250
 576/5677 [==>...........................] - ETA: 7:36 - loss: 0.6588 - acc: 0.6215
 640/5677 [==>...........................] - ETA: 7:31 - loss: 0.6656 - acc: 0.6016
 704/5677 [==>...........................] - ETA: 7:29 - loss: 0.6688 - acc: 0.5923
 768/5677 [===>..........................] - ETA: 7:28 - loss: 0.6671 - acc: 0.6003
 832/5677 [===>..........................] - ETA: 7:26 - loss: 0.6709 - acc: 0.5962
 896/5677 [===>..........................] - ETA: 7:23 - loss: 0.6745 - acc: 0.5904
 960/5677 [====>.........................] - ETA: 7:23 - loss: 0.6766 - acc: 0.5854
1024/5677 [====>.........................] - ETA: 7:17 - loss: 0.6752 - acc: 0.5850
1088/5677 [====>.........................] - ETA: 7:14 - loss: 0.6772 - acc: 0.5800
1152/5677 [=====>........................] - ETA: 7:09 - loss: 0.6765 - acc: 0.5842
1216/5677 [=====>........................] - ETA: 7:05 - loss: 0.6752 - acc: 0.5855
1280/5677 [=====>........................] - ETA: 6:59 - loss: 0.6736 - acc: 0.5859
1344/5677 [======>.......................] - ETA: 6:53 - loss: 0.6727 - acc: 0.5908
1408/5677 [======>.......................] - ETA: 6:47 - loss: 0.6758 - acc: 0.5845
1472/5677 [======>.......................] - ETA: 6:42 - loss: 0.6787 - acc: 0.5754
1536/5677 [=======>......................] - ETA: 6:36 - loss: 0.6780 - acc: 0.5794
1600/5677 [=======>......................] - ETA: 6:32 - loss: 0.6769 - acc: 0.5806
1664/5677 [=======>......................] - ETA: 6:27 - loss: 0.6755 - acc: 0.5841
1728/5677 [========>.....................] - ETA: 6:21 - loss: 0.6746 - acc: 0.5839
1792/5677 [========>.....................] - ETA: 6:16 - loss: 0.6738 - acc: 0.5837
1856/5677 [========>.....................] - ETA: 6:09 - loss: 0.6747 - acc: 0.5819
1920/5677 [=========>....................] - ETA: 6:03 - loss: 0.6758 - acc: 0.5802
1984/5677 [=========>....................] - ETA: 5:58 - loss: 0.6758 - acc: 0.5776
2048/5677 [=========>....................] - ETA: 5:51 - loss: 0.6773 - acc: 0.5728
2112/5677 [==========>...................] - ETA: 5:45 - loss: 0.6759 - acc: 0.5767
2176/5677 [==========>...................] - ETA: 5:39 - loss: 0.6753 - acc: 0.5781
2240/5677 [==========>...................] - ETA: 5:34 - loss: 0.6757 - acc: 0.5763
2304/5677 [===========>..................] - ETA: 5:29 - loss: 0.6754 - acc: 0.5755
2368/5677 [===========>..................] - ETA: 5:24 - loss: 0.6761 - acc: 0.5743
2432/5677 [===========>..................] - ETA: 5:19 - loss: 0.6771 - acc: 0.5720
2496/5677 [============>.................] - ETA: 5:13 - loss: 0.6770 - acc: 0.5693
2560/5677 [============>.................] - ETA: 5:08 - loss: 0.6759 - acc: 0.5707
2624/5677 [============>.................] - ETA: 5:02 - loss: 0.6760 - acc: 0.5705
2688/5677 [=============>................] - ETA: 4:57 - loss: 0.6759 - acc: 0.5718
2752/5677 [=============>................] - ETA: 4:51 - loss: 0.6757 - acc: 0.5734
2816/5677 [=============>................] - ETA: 4:45 - loss: 0.6756 - acc: 0.5732
2880/5677 [==============>...............] - ETA: 4:39 - loss: 0.6757 - acc: 0.5719
2944/5677 [==============>...............] - ETA: 4:33 - loss: 0.6752 - acc: 0.5740
3008/5677 [==============>...............] - ETA: 4:28 - loss: 0.6760 - acc: 0.5728
3072/5677 [===============>..............] - ETA: 4:22 - loss: 0.6757 - acc: 0.5729
3136/5677 [===============>..............] - ETA: 4:16 - loss: 0.6750 - acc: 0.5753
3200/5677 [===============>..............] - ETA: 4:10 - loss: 0.6745 - acc: 0.5769
3264/5677 [================>.............] - ETA: 4:04 - loss: 0.6753 - acc: 0.5748
3328/5677 [================>.............] - ETA: 3:58 - loss: 0.6753 - acc: 0.5748
3392/5677 [================>.............] - ETA: 3:51 - loss: 0.6750 - acc: 0.5752
3456/5677 [=================>............] - ETA: 3:45 - loss: 0.6743 - acc: 0.5764
3520/5677 [=================>............] - ETA: 3:39 - loss: 0.6742 - acc: 0.5767
3584/5677 [=================>............] - ETA: 3:33 - loss: 0.6744 - acc: 0.5762
3648/5677 [==================>...........] - ETA: 3:26 - loss: 0.6740 - acc: 0.5768
3712/5677 [==================>...........] - ETA: 3:20 - loss: 0.6744 - acc: 0.5762
3776/5677 [==================>...........] - ETA: 3:14 - loss: 0.6748 - acc: 0.5763
3840/5677 [===================>..........] - ETA: 3:08 - loss: 0.6745 - acc: 0.5771
3904/5677 [===================>..........] - ETA: 3:02 - loss: 0.6751 - acc: 0.5768
3968/5677 [===================>..........] - ETA: 2:55 - loss: 0.6752 - acc: 0.5766
4032/5677 [====================>.........] - ETA: 2:49 - loss: 0.6750 - acc: 0.5769
4096/5677 [====================>.........] - ETA: 2:42 - loss: 0.6748 - acc: 0.5779
4160/5677 [====================>.........] - ETA: 2:36 - loss: 0.6744 - acc: 0.5786
4224/5677 [=====================>........] - ETA: 2:29 - loss: 0.6742 - acc: 0.5791
4288/5677 [=====================>........] - ETA: 2:23 - loss: 0.6736 - acc: 0.5798
4352/5677 [=====================>........] - ETA: 2:16 - loss: 0.6737 - acc: 0.5813
4416/5677 [======================>.......] - ETA: 2:10 - loss: 0.6739 - acc: 0.5811
4480/5677 [======================>.......] - ETA: 2:03 - loss: 0.6735 - acc: 0.5819
4544/5677 [=======================>......] - ETA: 1:56 - loss: 0.6731 - acc: 0.5827
4608/5677 [=======================>......] - ETA: 1:50 - loss: 0.6741 - acc: 0.5812
4672/5677 [=======================>......] - ETA: 1:43 - loss: 0.6745 - acc: 0.5815
4736/5677 [========================>.....] - ETA: 1:37 - loss: 0.6745 - acc: 0.5809
4800/5677 [========================>.....] - ETA: 1:30 - loss: 0.6743 - acc: 0.5806
4864/5677 [========================>.....] - ETA: 1:24 - loss: 0.6742 - acc: 0.5810
4928/5677 [=========================>....] - ETA: 1:17 - loss: 0.6741 - acc: 0.5808
4992/5677 [=========================>....] - ETA: 1:11 - loss: 0.6743 - acc: 0.5803
5056/5677 [=========================>....] - ETA: 1:04 - loss: 0.6738 - acc: 0.5811
5120/5677 [==========================>...] - ETA: 57s - loss: 0.6738 - acc: 0.5809 
5184/5677 [==========================>...] - ETA: 51s - loss: 0.6735 - acc: 0.5820
5248/5677 [==========================>...] - ETA: 44s - loss: 0.6735 - acc: 0.5819
5312/5677 [===========================>..] - ETA: 37s - loss: 0.6735 - acc: 0.5821
5376/5677 [===========================>..] - ETA: 31s - loss: 0.6727 - acc: 0.5837
5440/5677 [===========================>..] - ETA: 24s - loss: 0.6723 - acc: 0.5840
5504/5677 [============================>.] - ETA: 18s - loss: 0.6728 - acc: 0.5834
5568/5677 [============================>.] - ETA: 11s - loss: 0.6735 - acc: 0.5824
5632/5677 [============================>.] - ETA: 4s - loss: 0.6738 - acc: 0.5822 
5677/5677 [==============================] - 617s 109ms/step - loss: 0.6740 - acc: 0.5816 - val_loss: 0.6807 - val_acc: 0.5642

Epoch 00009: val_acc did not improve from 0.57052
Epoch 10/10

  64/5677 [..............................] - ETA: 8:49 - loss: 0.6464 - acc: 0.5312
 128/5677 [..............................] - ETA: 9:47 - loss: 0.6693 - acc: 0.5703
 192/5677 [>.............................] - ETA: 9:37 - loss: 0.6709 - acc: 0.5625
 256/5677 [>.............................] - ETA: 9:41 - loss: 0.6857 - acc: 0.5469
 320/5677 [>.............................] - ETA: 9:34 - loss: 0.6859 - acc: 0.5469
 384/5677 [=>............................] - ETA: 9:30 - loss: 0.6884 - acc: 0.5365
 448/5677 [=>............................] - ETA: 9:20 - loss: 0.6846 - acc: 0.5491
 512/5677 [=>............................] - ETA: 9:14 - loss: 0.6811 - acc: 0.5586
 576/5677 [==>...........................] - ETA: 9:04 - loss: 0.6819 - acc: 0.5573
 640/5677 [==>...........................] - ETA: 8:56 - loss: 0.6800 - acc: 0.5609
 704/5677 [==>...........................] - ETA: 8:52 - loss: 0.6777 - acc: 0.5668
 768/5677 [===>..........................] - ETA: 8:46 - loss: 0.6744 - acc: 0.5742
 832/5677 [===>..........................] - ETA: 8:39 - loss: 0.6754 - acc: 0.5733
 896/5677 [===>..........................] - ETA: 8:32 - loss: 0.6748 - acc: 0.5748
 960/5677 [====>.........................] - ETA: 8:22 - loss: 0.6762 - acc: 0.5729
1024/5677 [====>.........................] - ETA: 8:17 - loss: 0.6760 - acc: 0.5732
1088/5677 [====>.........................] - ETA: 8:08 - loss: 0.6755 - acc: 0.5717
1152/5677 [=====>........................] - ETA: 7:59 - loss: 0.6753 - acc: 0.5764
1216/5677 [=====>........................] - ETA: 7:53 - loss: 0.6764 - acc: 0.5740
1280/5677 [=====>........................] - ETA: 7:49 - loss: 0.6746 - acc: 0.5789
1344/5677 [======>.......................] - ETA: 7:43 - loss: 0.6753 - acc: 0.5766
1408/5677 [======>.......................] - ETA: 7:35 - loss: 0.6766 - acc: 0.5760
1472/5677 [======>.......................] - ETA: 7:28 - loss: 0.6766 - acc: 0.5768
1536/5677 [=======>......................] - ETA: 7:21 - loss: 0.6775 - acc: 0.5749
1600/5677 [=======>......................] - ETA: 7:14 - loss: 0.6771 - acc: 0.5750
1664/5677 [=======>......................] - ETA: 7:08 - loss: 0.6760 - acc: 0.5769
1728/5677 [========>.....................] - ETA: 7:02 - loss: 0.6745 - acc: 0.5804
1792/5677 [========>.....................] - ETA: 6:55 - loss: 0.6747 - acc: 0.5820
1856/5677 [========>.....................] - ETA: 6:46 - loss: 0.6758 - acc: 0.5824
1920/5677 [=========>....................] - ETA: 6:38 - loss: 0.6755 - acc: 0.5849
1984/5677 [=========>....................] - ETA: 6:30 - loss: 0.6750 - acc: 0.5882
2048/5677 [=========>....................] - ETA: 6:23 - loss: 0.6751 - acc: 0.5894
2112/5677 [==========>...................] - ETA: 6:17 - loss: 0.6752 - acc: 0.5895
2176/5677 [==========>...................] - ETA: 6:11 - loss: 0.6739 - acc: 0.5915
2240/5677 [==========>...................] - ETA: 6:04 - loss: 0.6746 - acc: 0.5897
2304/5677 [===========>..................] - ETA: 5:57 - loss: 0.6746 - acc: 0.5885
2368/5677 [===========>..................] - ETA: 5:49 - loss: 0.6751 - acc: 0.5887
2432/5677 [===========>..................] - ETA: 5:41 - loss: 0.6772 - acc: 0.5851
2496/5677 [============>.................] - ETA: 5:33 - loss: 0.6778 - acc: 0.5849
2560/5677 [============>.................] - ETA: 5:25 - loss: 0.6779 - acc: 0.5852
2624/5677 [============>.................] - ETA: 5:17 - loss: 0.6780 - acc: 0.5831
2688/5677 [=============>................] - ETA: 5:12 - loss: 0.6781 - acc: 0.5826
2752/5677 [=============>................] - ETA: 5:05 - loss: 0.6780 - acc: 0.5825
2816/5677 [=============>................] - ETA: 4:58 - loss: 0.6775 - acc: 0.5827
2880/5677 [==============>...............] - ETA: 4:51 - loss: 0.6774 - acc: 0.5830
2944/5677 [==============>...............] - ETA: 4:44 - loss: 0.6773 - acc: 0.5839
3008/5677 [==============>...............] - ETA: 4:37 - loss: 0.6766 - acc: 0.5844
3072/5677 [===============>..............] - ETA: 4:30 - loss: 0.6769 - acc: 0.5830
3136/5677 [===============>..............] - ETA: 4:23 - loss: 0.6762 - acc: 0.5842
3200/5677 [===============>..............] - ETA: 4:16 - loss: 0.6752 - acc: 0.5866
3264/5677 [================>.............] - ETA: 4:10 - loss: 0.6750 - acc: 0.5870
3328/5677 [================>.............] - ETA: 4:03 - loss: 0.6752 - acc: 0.5874
3392/5677 [================>.............] - ETA: 3:57 - loss: 0.6747 - acc: 0.5893
3456/5677 [=================>............] - ETA: 3:50 - loss: 0.6742 - acc: 0.5906
3520/5677 [=================>............] - ETA: 3:44 - loss: 0.6744 - acc: 0.5901
3584/5677 [=================>............] - ETA: 3:37 - loss: 0.6749 - acc: 0.5890
3648/5677 [==================>...........] - ETA: 3:30 - loss: 0.6747 - acc: 0.5877
3712/5677 [==================>...........] - ETA: 3:24 - loss: 0.6756 - acc: 0.5862
3776/5677 [==================>...........] - ETA: 3:17 - loss: 0.6756 - acc: 0.5855
3840/5677 [===================>..........] - ETA: 3:11 - loss: 0.6748 - acc: 0.5865
3904/5677 [===================>..........] - ETA: 3:04 - loss: 0.6756 - acc: 0.5843
3968/5677 [===================>..........] - ETA: 2:57 - loss: 0.6761 - acc: 0.5824
4032/5677 [====================>.........] - ETA: 2:51 - loss: 0.6754 - acc: 0.5836
4096/5677 [====================>.........] - ETA: 2:44 - loss: 0.6750 - acc: 0.5845
4160/5677 [====================>.........] - ETA: 2:38 - loss: 0.6757 - acc: 0.5837
4224/5677 [=====================>........] - ETA: 2:31 - loss: 0.6751 - acc: 0.5843
4288/5677 [=====================>........] - ETA: 2:24 - loss: 0.6748 - acc: 0.5847
4352/5677 [=====================>........] - ETA: 2:18 - loss: 0.6746 - acc: 0.5850
4416/5677 [======================>.......] - ETA: 2:11 - loss: 0.6743 - acc: 0.5854
4480/5677 [======================>.......] - ETA: 2:05 - loss: 0.6747 - acc: 0.5846
4544/5677 [=======================>......] - ETA: 1:58 - loss: 0.6745 - acc: 0.5843
4608/5677 [=======================>......] - ETA: 1:51 - loss: 0.6743 - acc: 0.5851
4672/5677 [=======================>......] - ETA: 1:45 - loss: 0.6749 - acc: 0.5835
4736/5677 [========================>.....] - ETA: 1:38 - loss: 0.6750 - acc: 0.5836
4800/5677 [========================>.....] - ETA: 1:31 - loss: 0.6750 - acc: 0.5835
4864/5677 [========================>.....] - ETA: 1:25 - loss: 0.6746 - acc: 0.5843
4928/5677 [=========================>....] - ETA: 1:18 - loss: 0.6745 - acc: 0.5846
4992/5677 [=========================>....] - ETA: 1:11 - loss: 0.6748 - acc: 0.5841
5056/5677 [=========================>....] - ETA: 1:05 - loss: 0.6745 - acc: 0.5847
5120/5677 [==========================>...] - ETA: 58s - loss: 0.6741 - acc: 0.5855 
5184/5677 [==========================>...] - ETA: 51s - loss: 0.6741 - acc: 0.5849
5248/5677 [==========================>...] - ETA: 44s - loss: 0.6744 - acc: 0.5844
5312/5677 [===========================>..] - ETA: 38s - loss: 0.6746 - acc: 0.5841
5376/5677 [===========================>..] - ETA: 31s - loss: 0.6740 - acc: 0.5846
5440/5677 [===========================>..] - ETA: 24s - loss: 0.6740 - acc: 0.5846
5504/5677 [============================>.] - ETA: 18s - loss: 0.6742 - acc: 0.5832
5568/5677 [============================>.] - ETA: 11s - loss: 0.6742 - acc: 0.5835
5632/5677 [============================>.] - ETA: 4s - loss: 0.6741 - acc: 0.5847 
5677/5677 [==============================] - 618s 109ms/step - loss: 0.6743 - acc: 0.5846 - val_loss: 0.6973 - val_acc: 0.5436

Epoch 00010: val_acc did not improve from 0.57052
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f24df404850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f24df404850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f24d51cbf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f24d51cbf50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24df375350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24df375350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24d4f0a690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24d4f0a690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24d4ed3b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24d4ed3b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d4d11350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d4d11350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24d4f05dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24d4f05dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d4e8e610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d4e8e610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24d4e34b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24d4e34b50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24d4cdbe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24d4cdbe10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1db44833d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1db44833d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1db4145610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1db4145610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d4b33450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d4b33450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24d4cdb7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24d4cdb7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24d491bb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24d491bb10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d4977c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d4977c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24d4c84fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24d4c84fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d4966950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d4966950>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24d5989450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24d5989450>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24d4602f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24d4602f50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d46ef6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d46ef6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24d59c14d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24d59c14d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d4501990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d4501990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24d43eb850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24d43eb850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24d43fe4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24d43fe4d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d42f8f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d42f8f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24d43eb250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24d43eb250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d46d7290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d46d7290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24d4116690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24d4116690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24d4228190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24d4228190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d4172a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d4172a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24d4116090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24d4116090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d400b910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d400b910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24d401ef10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24d401ef10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24d3dcca10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24d3dcca10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d3f18c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d3f18c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24d401e110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24d401e110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d3cbced0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d3cbced0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24d3ce64d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24d3ce64d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24cb991810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24cb991810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24cb891150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24cb891150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24d3ce6b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24d3ce6b90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d3a5fa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d3a5fa90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24cb774090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24cb774090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24cb7b8110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24cb7b8110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24cb785110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24cb785110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24cb774110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24cb774110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24cb70f650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24cb70f650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24cb43fe10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24cb43fe10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24cb387750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24cb387750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24cb3ad850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24cb3ad850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24cb34cb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24cb34cb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24cb406ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24cb406ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f23c014cf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f23c014cf90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f23b8fd5a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f23b8fd5a90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24cb39dad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24cb39dad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f23c014cc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f23c014cc50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24cb34f150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24cb34f150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1f10350e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1f10350e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f10295e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1f10295e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f103b0410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f103b0410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f23c00fda10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f23c00fda10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f10265610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1f10265610>>: AttributeError: module 'gast' has no attribute 'Str'
01window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 3:48
 128/1578 [=>............................] - ETA: 2:11
 192/1578 [==>...........................] - ETA: 1:43
 256/1578 [===>..........................] - ETA: 1:23
 320/1578 [=====>........................] - ETA: 1:11
 384/1578 [======>.......................] - ETA: 1:04
 448/1578 [=======>......................] - ETA: 57s 
 512/1578 [========>.....................] - ETA: 51s
 576/1578 [=========>....................] - ETA: 46s
 640/1578 [===========>..................] - ETA: 42s
 704/1578 [============>.................] - ETA: 38s
 768/1578 [=============>................] - ETA: 35s
 832/1578 [==============>...............] - ETA: 32s
 896/1578 [================>.............] - ETA: 28s
 960/1578 [=================>............] - ETA: 25s
1024/1578 [==================>...........] - ETA: 22s
1088/1578 [===================>..........] - ETA: 19s
1152/1578 [====================>.........] - ETA: 17s
1216/1578 [======================>.......] - ETA: 14s
1280/1578 [=======================>......] - ETA: 12s
1344/1578 [========================>.....] - ETA: 9s 
1408/1578 [=========================>....] - ETA: 6s
1472/1578 [==========================>...] - ETA: 4s
1536/1578 [============================>.] - ETA: 1s
1578/1578 [==============================] - 63s 40ms/step
loss: 0.6758630031717443
acc: 0.5735107738104459
样本个数 3154
样本个数 6308
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f1d9027a7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f1d9027a7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f1d381beb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f1d381beb10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24fa652150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24fa652150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1ee019a7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1ee019a7d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24d506bb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24d506bb90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1ee03eadd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1ee03eadd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1ee019a590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1ee019a590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cb8627090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cb8627090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24d5181090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f24d5181090>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24d5108c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f24d5108c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d5191090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d5191090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24df3a55d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24df3a55d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d5056990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24d5056990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1d3073ccd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1d3073ccd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1d38079d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1d38079d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1d30571e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1d30571e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24d505eed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24d505eed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1d307a39d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1d307a39d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1d30651c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1d30651c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1d3054aa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1d3054aa90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1d302606d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1d302606d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24d51f5a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f24d51f5a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1d303d4350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1d303d4350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1d3016b610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1d3016b610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1cf47dff90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1cf47dff90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1d3016bf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1d3016bf10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1d3016b650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1d3016b650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1d300dc7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1d300dc7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1cf45c7d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1cf45c7d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1cf46344d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1cf46344d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f23c0022750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f23c0022750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1cf45d3750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1cf45d3750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cf43f4dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cf43f4dd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1d30254bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1d30254bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1cf42ddb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1cf42ddb90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cf44f3490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cf44f3490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1cf44f0fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1cf44f0fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cf452b790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cf452b790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1cf41efd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1cf41efd10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1cd0640990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1cd0640990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cf42bcd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cf42bcd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1cf41ef290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1cf41ef290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cd073c850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cd073c850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1cd06b0d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1cd06b0d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1cd030ef50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1cd030ef50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cd0228850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cd0228850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1cd0473fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1cd0473fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cd035f650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cd035f650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1cd01718d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1cd01718d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1cd03d03d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1cd03d03d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cd0073510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cd0073510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1cd0171cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1cd0171cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cb86b3fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cb86b3fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1cb87c2b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1cb87c2b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1be463b8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1be463b8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1be44783d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1be44783d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1cb87c2350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1cb87c2350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1be45e1f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1be45e1f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1be440ab50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1be440ab50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1be4390fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1be4390fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1be412b590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1be412b590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1be440a7d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1be440a7d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1be45f08d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1be45f08d0>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 1:08:17 - loss: 0.8647 - acc: 0.3906
 128/5677 [..............................] - ETA: 40:12 - loss: 0.8612 - acc: 0.4766  
 192/5677 [>.............................] - ETA: 31:00 - loss: 0.8516 - acc: 0.4792
 256/5677 [>.............................] - ETA: 25:53 - loss: 0.8227 - acc: 0.4922
 320/5677 [>.............................] - ETA: 22:57 - loss: 0.8080 - acc: 0.4938
 384/5677 [=>............................] - ETA: 20:49 - loss: 0.8018 - acc: 0.4922
 448/5677 [=>............................] - ETA: 19:22 - loss: 0.7996 - acc: 0.4866
 512/5677 [=>............................] - ETA: 18:10 - loss: 0.7979 - acc: 0.4707
 576/5677 [==>...........................] - ETA: 17:17 - loss: 0.7989 - acc: 0.4566
 640/5677 [==>...........................] - ETA: 16:50 - loss: 0.7976 - acc: 0.4562
 704/5677 [==>...........................] - ETA: 16:07 - loss: 0.7943 - acc: 0.4560
 768/5677 [===>..........................] - ETA: 15:28 - loss: 0.7919 - acc: 0.4570
 832/5677 [===>..........................] - ETA: 14:51 - loss: 0.7871 - acc: 0.4579
 896/5677 [===>..........................] - ETA: 14:23 - loss: 0.7856 - acc: 0.4576
 960/5677 [====>.........................] - ETA: 14:00 - loss: 0.7824 - acc: 0.4583
1024/5677 [====>.........................] - ETA: 13:36 - loss: 0.7765 - acc: 0.4639
1088/5677 [====>.........................] - ETA: 13:25 - loss: 0.7714 - acc: 0.4697
1152/5677 [=====>........................] - ETA: 13:05 - loss: 0.7677 - acc: 0.4705
1216/5677 [=====>........................] - ETA: 12:54 - loss: 0.7671 - acc: 0.4720
1280/5677 [=====>........................] - ETA: 12:34 - loss: 0.7637 - acc: 0.4750
1344/5677 [======>.......................] - ETA: 12:17 - loss: 0.7591 - acc: 0.4799
1408/5677 [======>.......................] - ETA: 11:59 - loss: 0.7596 - acc: 0.4780
1472/5677 [======>.......................] - ETA: 11:41 - loss: 0.7572 - acc: 0.4810
1536/5677 [=======>......................] - ETA: 11:25 - loss: 0.7556 - acc: 0.4837
1600/5677 [=======>......................] - ETA: 11:10 - loss: 0.7554 - acc: 0.4806
1664/5677 [=======>......................] - ETA: 10:58 - loss: 0.7535 - acc: 0.4826
1728/5677 [========>.....................] - ETA: 10:41 - loss: 0.7512 - acc: 0.4850
1792/5677 [========>.....................] - ETA: 10:31 - loss: 0.7500 - acc: 0.4849
1856/5677 [========>.....................] - ETA: 10:18 - loss: 0.7486 - acc: 0.4887
1920/5677 [=========>....................] - ETA: 10:04 - loss: 0.7489 - acc: 0.4885
1984/5677 [=========>....................] - ETA: 9:51 - loss: 0.7482 - acc: 0.4874 
2048/5677 [=========>....................] - ETA: 9:39 - loss: 0.7468 - acc: 0.4893
2112/5677 [==========>...................] - ETA: 9:27 - loss: 0.7465 - acc: 0.4872
2176/5677 [==========>...................] - ETA: 9:15 - loss: 0.7454 - acc: 0.4871
2240/5677 [==========>...................] - ETA: 9:03 - loss: 0.7441 - acc: 0.4915
2304/5677 [===========>..................] - ETA: 8:49 - loss: 0.7450 - acc: 0.4900
2368/5677 [===========>..................] - ETA: 8:36 - loss: 0.7444 - acc: 0.4907
2432/5677 [===========>..................] - ETA: 8:25 - loss: 0.7429 - acc: 0.4910
2496/5677 [============>.................] - ETA: 8:13 - loss: 0.7426 - acc: 0.4920
2560/5677 [============>.................] - ETA: 8:01 - loss: 0.7426 - acc: 0.4918
2624/5677 [============>.................] - ETA: 7:50 - loss: 0.7426 - acc: 0.4909
2688/5677 [=============>................] - ETA: 7:38 - loss: 0.7407 - acc: 0.4918
2752/5677 [=============>................] - ETA: 7:25 - loss: 0.7403 - acc: 0.4924
2816/5677 [=============>................] - ETA: 7:14 - loss: 0.7395 - acc: 0.4936
2880/5677 [==============>...............] - ETA: 7:03 - loss: 0.7392 - acc: 0.4931
2944/5677 [==============>...............] - ETA: 6:50 - loss: 0.7382 - acc: 0.4939
3008/5677 [==============>...............] - ETA: 6:39 - loss: 0.7377 - acc: 0.4957
3072/5677 [===============>..............] - ETA: 6:28 - loss: 0.7373 - acc: 0.4964
3136/5677 [===============>..............] - ETA: 6:16 - loss: 0.7364 - acc: 0.4971
3200/5677 [===============>..............] - ETA: 6:06 - loss: 0.7357 - acc: 0.4981
3264/5677 [================>.............] - ETA: 5:56 - loss: 0.7357 - acc: 0.4969
3328/5677 [================>.............] - ETA: 5:45 - loss: 0.7352 - acc: 0.4970
3392/5677 [================>.............] - ETA: 5:35 - loss: 0.7352 - acc: 0.4968
3456/5677 [=================>............] - ETA: 5:25 - loss: 0.7347 - acc: 0.4991
3520/5677 [=================>............] - ETA: 5:15 - loss: 0.7339 - acc: 0.4997
3584/5677 [=================>............] - ETA: 5:04 - loss: 0.7333 - acc: 0.4994
3648/5677 [==================>...........] - ETA: 4:54 - loss: 0.7330 - acc: 0.4997
3712/5677 [==================>...........] - ETA: 4:44 - loss: 0.7320 - acc: 0.5013
3776/5677 [==================>...........] - ETA: 4:34 - loss: 0.7321 - acc: 0.5005
3840/5677 [===================>..........] - ETA: 4:25 - loss: 0.7316 - acc: 0.5010
3904/5677 [===================>..........] - ETA: 4:14 - loss: 0.7307 - acc: 0.5020
3968/5677 [===================>..........] - ETA: 4:04 - loss: 0.7298 - acc: 0.5030
4032/5677 [====================>.........] - ETA: 3:55 - loss: 0.7294 - acc: 0.5032
4096/5677 [====================>.........] - ETA: 3:45 - loss: 0.7287 - acc: 0.5042
4160/5677 [====================>.........] - ETA: 3:35 - loss: 0.7286 - acc: 0.5041
4224/5677 [=====================>........] - ETA: 3:26 - loss: 0.7277 - acc: 0.5045
4288/5677 [=====================>........] - ETA: 3:17 - loss: 0.7266 - acc: 0.5061
4352/5677 [=====================>........] - ETA: 3:07 - loss: 0.7268 - acc: 0.5055
4416/5677 [======================>.......] - ETA: 2:58 - loss: 0.7263 - acc: 0.5061
4480/5677 [======================>.......] - ETA: 2:49 - loss: 0.7262 - acc: 0.5054
4544/5677 [=======================>......] - ETA: 2:40 - loss: 0.7260 - acc: 0.5053
4608/5677 [=======================>......] - ETA: 2:30 - loss: 0.7257 - acc: 0.5054
4672/5677 [=======================>......] - ETA: 2:21 - loss: 0.7253 - acc: 0.5051
4736/5677 [========================>.....] - ETA: 2:12 - loss: 0.7251 - acc: 0.5061
4800/5677 [========================>.....] - ETA: 2:03 - loss: 0.7251 - acc: 0.5052
4864/5677 [========================>.....] - ETA: 1:54 - loss: 0.7249 - acc: 0.5051
4928/5677 [=========================>....] - ETA: 1:45 - loss: 0.7246 - acc: 0.5051
4992/5677 [=========================>....] - ETA: 1:35 - loss: 0.7244 - acc: 0.5048
5056/5677 [=========================>....] - ETA: 1:26 - loss: 0.7240 - acc: 0.5044
5120/5677 [==========================>...] - ETA: 1:17 - loss: 0.7237 - acc: 0.5053
5184/5677 [==========================>...] - ETA: 1:08 - loss: 0.7231 - acc: 0.5062
5248/5677 [==========================>...] - ETA: 59s - loss: 0.7226 - acc: 0.5072 
5312/5677 [===========================>..] - ETA: 50s - loss: 0.7227 - acc: 0.5070
5376/5677 [===========================>..] - ETA: 41s - loss: 0.7229 - acc: 0.5071
5440/5677 [===========================>..] - ETA: 32s - loss: 0.7225 - acc: 0.5070
5504/5677 [============================>.] - ETA: 23s - loss: 0.7225 - acc: 0.5069
5568/5677 [============================>.] - ETA: 15s - loss: 0.7225 - acc: 0.5068
5632/5677 [============================>.] - ETA: 6s - loss: 0.7226 - acc: 0.5059 
5677/5677 [==============================] - 821s 145ms/step - loss: 0.7225 - acc: 0.5063 - val_loss: 0.6862 - val_acc: 0.5436

Epoch 00001: val_acc improved from -inf to 0.54358, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window03/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 12:10 - loss: 0.6924 - acc: 0.6250
 128/5677 [..............................] - ETA: 11:58 - loss: 0.6747 - acc: 0.6250
 192/5677 [>.............................] - ETA: 11:32 - loss: 0.6850 - acc: 0.5833
 256/5677 [>.............................] - ETA: 11:31 - loss: 0.6954 - acc: 0.5469
 320/5677 [>.............................] - ETA: 11:27 - loss: 0.6979 - acc: 0.5344
 384/5677 [=>............................] - ETA: 11:12 - loss: 0.6980 - acc: 0.5312
 448/5677 [=>............................] - ETA: 11:13 - loss: 0.7011 - acc: 0.5179
 512/5677 [=>............................] - ETA: 11:08 - loss: 0.7008 - acc: 0.5195
 576/5677 [==>...........................] - ETA: 11:15 - loss: 0.6991 - acc: 0.5243
 640/5677 [==>...........................] - ETA: 10:57 - loss: 0.6957 - acc: 0.5250
 704/5677 [==>...........................] - ETA: 11:02 - loss: 0.6965 - acc: 0.5256
 768/5677 [===>..........................] - ETA: 10:47 - loss: 0.6912 - acc: 0.5299
 832/5677 [===>..........................] - ETA: 10:40 - loss: 0.6891 - acc: 0.5325
 896/5677 [===>..........................] - ETA: 10:25 - loss: 0.6893 - acc: 0.5346
 960/5677 [====>.........................] - ETA: 10:16 - loss: 0.6908 - acc: 0.5365
1024/5677 [====>.........................] - ETA: 10:07 - loss: 0.6924 - acc: 0.5342
1088/5677 [====>.........................] - ETA: 9:57 - loss: 0.6984 - acc: 0.5257 
1152/5677 [=====>........................] - ETA: 9:46 - loss: 0.6997 - acc: 0.5243
1216/5677 [=====>........................] - ETA: 9:40 - loss: 0.7010 - acc: 0.5206
1280/5677 [=====>........................] - ETA: 9:30 - loss: 0.7006 - acc: 0.5203
1344/5677 [======>.......................] - ETA: 9:21 - loss: 0.7002 - acc: 0.5223
1408/5677 [======>.......................] - ETA: 9:09 - loss: 0.6998 - acc: 0.5185
1472/5677 [======>.......................] - ETA: 8:58 - loss: 0.7002 - acc: 0.5163
1536/5677 [=======>......................] - ETA: 8:50 - loss: 0.6987 - acc: 0.5195
1600/5677 [=======>......................] - ETA: 8:38 - loss: 0.7002 - acc: 0.5188
1664/5677 [=======>......................] - ETA: 8:32 - loss: 0.7014 - acc: 0.5180
1728/5677 [========>.....................] - ETA: 8:21 - loss: 0.6997 - acc: 0.5220
1792/5677 [========>.....................] - ETA: 8:11 - loss: 0.6995 - acc: 0.5190
1856/5677 [========>.....................] - ETA: 8:02 - loss: 0.6996 - acc: 0.5199
1920/5677 [=========>....................] - ETA: 7:53 - loss: 0.6994 - acc: 0.5182
1984/5677 [=========>....................] - ETA: 7:45 - loss: 0.6997 - acc: 0.5197
2048/5677 [=========>....................] - ETA: 7:38 - loss: 0.7003 - acc: 0.5166
2112/5677 [==========>...................] - ETA: 7:29 - loss: 0.7014 - acc: 0.5142
2176/5677 [==========>...................] - ETA: 7:20 - loss: 0.7025 - acc: 0.5115
2240/5677 [==========>...................] - ETA: 7:12 - loss: 0.7028 - acc: 0.5116
2304/5677 [===========>..................] - ETA: 7:03 - loss: 0.7025 - acc: 0.5139
2368/5677 [===========>..................] - ETA: 6:55 - loss: 0.7033 - acc: 0.5127
2432/5677 [===========>..................] - ETA: 6:46 - loss: 0.7032 - acc: 0.5123
2496/5677 [============>.................] - ETA: 6:37 - loss: 0.7036 - acc: 0.5124
2560/5677 [============>.................] - ETA: 6:30 - loss: 0.7038 - acc: 0.5113
2624/5677 [============>.................] - ETA: 6:21 - loss: 0.7052 - acc: 0.5076
2688/5677 [=============>................] - ETA: 6:13 - loss: 0.7053 - acc: 0.5063
2752/5677 [=============>................] - ETA: 6:03 - loss: 0.7047 - acc: 0.5080
2816/5677 [=============>................] - ETA: 5:55 - loss: 0.7052 - acc: 0.5089
2880/5677 [==============>...............] - ETA: 5:47 - loss: 0.7050 - acc: 0.5101
2944/5677 [==============>...............] - ETA: 5:39 - loss: 0.7045 - acc: 0.5109
3008/5677 [==============>...............] - ETA: 5:32 - loss: 0.7049 - acc: 0.5103
3072/5677 [===============>..............] - ETA: 5:24 - loss: 0.7050 - acc: 0.5091
3136/5677 [===============>..............] - ETA: 5:16 - loss: 0.7045 - acc: 0.5099
3200/5677 [===============>..............] - ETA: 5:08 - loss: 0.7044 - acc: 0.5103
3264/5677 [================>.............] - ETA: 5:00 - loss: 0.7042 - acc: 0.5113
3328/5677 [================>.............] - ETA: 4:51 - loss: 0.7052 - acc: 0.5105
3392/5677 [================>.............] - ETA: 4:43 - loss: 0.7053 - acc: 0.5103
3456/5677 [=================>............] - ETA: 4:35 - loss: 0.7054 - acc: 0.5104
3520/5677 [=================>............] - ETA: 4:28 - loss: 0.7054 - acc: 0.5099
3584/5677 [=================>............] - ETA: 4:20 - loss: 0.7053 - acc: 0.5098
3648/5677 [==================>...........] - ETA: 4:12 - loss: 0.7051 - acc: 0.5099
3712/5677 [==================>...........] - ETA: 4:04 - loss: 0.7051 - acc: 0.5097
3776/5677 [==================>...........] - ETA: 3:56 - loss: 0.7048 - acc: 0.5106
3840/5677 [===================>..........] - ETA: 3:48 - loss: 0.7042 - acc: 0.5117
3904/5677 [===================>..........] - ETA: 3:40 - loss: 0.7043 - acc: 0.5115
3968/5677 [===================>..........] - ETA: 3:32 - loss: 0.7038 - acc: 0.5121
4032/5677 [====================>.........] - ETA: 3:24 - loss: 0.7034 - acc: 0.5131
4096/5677 [====================>.........] - ETA: 3:16 - loss: 0.7035 - acc: 0.5127
4160/5677 [====================>.........] - ETA: 3:08 - loss: 0.7032 - acc: 0.5127
4224/5677 [=====================>........] - ETA: 3:00 - loss: 0.7032 - acc: 0.5130
4288/5677 [=====================>........] - ETA: 2:53 - loss: 0.7026 - acc: 0.5142
4352/5677 [=====================>........] - ETA: 2:45 - loss: 0.7025 - acc: 0.5145
4416/5677 [======================>.......] - ETA: 2:37 - loss: 0.7024 - acc: 0.5152
4480/5677 [======================>.......] - ETA: 2:29 - loss: 0.7020 - acc: 0.5167
4544/5677 [=======================>......] - ETA: 2:21 - loss: 0.7019 - acc: 0.5161
4608/5677 [=======================>......] - ETA: 2:13 - loss: 0.7022 - acc: 0.5154
4672/5677 [=======================>......] - ETA: 2:05 - loss: 0.7022 - acc: 0.5154
4736/5677 [========================>.....] - ETA: 1:57 - loss: 0.7020 - acc: 0.5165
4800/5677 [========================>.....] - ETA: 1:49 - loss: 0.7020 - acc: 0.5156
4864/5677 [========================>.....] - ETA: 1:41 - loss: 0.7017 - acc: 0.5162
4928/5677 [=========================>....] - ETA: 1:33 - loss: 0.7016 - acc: 0.5166
4992/5677 [=========================>....] - ETA: 1:25 - loss: 0.7020 - acc: 0.5156
5056/5677 [=========================>....] - ETA: 1:17 - loss: 0.7021 - acc: 0.5156
5120/5677 [==========================>...] - ETA: 1:08 - loss: 0.7019 - acc: 0.5162
5184/5677 [==========================>...] - ETA: 1:01 - loss: 0.7020 - acc: 0.5150
5248/5677 [==========================>...] - ETA: 53s - loss: 0.7019 - acc: 0.5154 
5312/5677 [===========================>..] - ETA: 45s - loss: 0.7018 - acc: 0.5164
5376/5677 [===========================>..] - ETA: 37s - loss: 0.7014 - acc: 0.5179
5440/5677 [===========================>..] - ETA: 29s - loss: 0.7014 - acc: 0.5182
5504/5677 [============================>.] - ETA: 21s - loss: 0.7013 - acc: 0.5184
5568/5677 [============================>.] - ETA: 13s - loss: 0.7011 - acc: 0.5183
5632/5677 [============================>.] - ETA: 5s - loss: 0.7012 - acc: 0.5185 
5677/5677 [==============================] - 724s 127ms/step - loss: 0.7012 - acc: 0.5184 - val_loss: 0.6879 - val_acc: 0.5388

Epoch 00002: val_acc did not improve from 0.54358
Epoch 3/10

  64/5677 [..............................] - ETA: 10:58 - loss: 0.6917 - acc: 0.5781
 128/5677 [..............................] - ETA: 11:32 - loss: 0.6977 - acc: 0.5625
 192/5677 [>.............................] - ETA: 11:19 - loss: 0.6961 - acc: 0.5573
 256/5677 [>.............................] - ETA: 11:14 - loss: 0.6916 - acc: 0.5664
 320/5677 [>.............................] - ETA: 10:40 - loss: 0.6906 - acc: 0.5719
 384/5677 [=>............................] - ETA: 10:47 - loss: 0.6912 - acc: 0.5703
 448/5677 [=>............................] - ETA: 10:28 - loss: 0.6902 - acc: 0.5670
 512/5677 [=>............................] - ETA: 10:17 - loss: 0.6934 - acc: 0.5527
 576/5677 [==>...........................] - ETA: 10:04 - loss: 0.6931 - acc: 0.5469
 640/5677 [==>...........................] - ETA: 9:59 - loss: 0.6920 - acc: 0.5500 
 704/5677 [==>...........................] - ETA: 9:57 - loss: 0.6948 - acc: 0.5440
 768/5677 [===>..........................] - ETA: 9:47 - loss: 0.6971 - acc: 0.5391
 832/5677 [===>..........................] - ETA: 9:34 - loss: 0.6955 - acc: 0.5469
 896/5677 [===>..........................] - ETA: 9:26 - loss: 0.6945 - acc: 0.5469
 960/5677 [====>.........................] - ETA: 9:17 - loss: 0.6940 - acc: 0.5479
1024/5677 [====>.........................] - ETA: 9:10 - loss: 0.6929 - acc: 0.5459
1088/5677 [====>.........................] - ETA: 9:02 - loss: 0.6924 - acc: 0.5506
1152/5677 [=====>........................] - ETA: 8:54 - loss: 0.6924 - acc: 0.5469
1216/5677 [=====>........................] - ETA: 8:53 - loss: 0.6922 - acc: 0.5485
1280/5677 [=====>........................] - ETA: 8:45 - loss: 0.6916 - acc: 0.5492
1344/5677 [======>.......................] - ETA: 8:37 - loss: 0.6920 - acc: 0.5461
1408/5677 [======>.......................] - ETA: 8:29 - loss: 0.6910 - acc: 0.5447
1472/5677 [======>.......................] - ETA: 8:20 - loss: 0.6914 - acc: 0.5442
1536/5677 [=======>......................] - ETA: 8:14 - loss: 0.6908 - acc: 0.5462
1600/5677 [=======>......................] - ETA: 8:08 - loss: 0.6906 - acc: 0.5469
1664/5677 [=======>......................] - ETA: 7:59 - loss: 0.6907 - acc: 0.5445
1728/5677 [========>.....................] - ETA: 7:53 - loss: 0.6907 - acc: 0.5457
1792/5677 [========>.....................] - ETA: 7:46 - loss: 0.6912 - acc: 0.5463
1856/5677 [========>.....................] - ETA: 7:40 - loss: 0.6911 - acc: 0.5463
1920/5677 [=========>....................] - ETA: 7:33 - loss: 0.6919 - acc: 0.5432
1984/5677 [=========>....................] - ETA: 7:24 - loss: 0.6926 - acc: 0.5413
2048/5677 [=========>....................] - ETA: 7:16 - loss: 0.6928 - acc: 0.5410
2112/5677 [==========>...................] - ETA: 7:08 - loss: 0.6926 - acc: 0.5407
2176/5677 [==========>...................] - ETA: 7:00 - loss: 0.6928 - acc: 0.5404
2240/5677 [==========>...................] - ETA: 6:53 - loss: 0.6933 - acc: 0.5393
2304/5677 [===========>..................] - ETA: 6:45 - loss: 0.6923 - acc: 0.5434
2368/5677 [===========>..................] - ETA: 6:39 - loss: 0.6918 - acc: 0.5439
2432/5677 [===========>..................] - ETA: 6:32 - loss: 0.6914 - acc: 0.5448
2496/5677 [============>.................] - ETA: 6:24 - loss: 0.6914 - acc: 0.5449
2560/5677 [============>.................] - ETA: 6:16 - loss: 0.6915 - acc: 0.5426
2624/5677 [============>.................] - ETA: 6:08 - loss: 0.6913 - acc: 0.5434
2688/5677 [=============>................] - ETA: 6:01 - loss: 0.6912 - acc: 0.5446
2752/5677 [=============>................] - ETA: 5:53 - loss: 0.6911 - acc: 0.5429
2816/5677 [=============>................] - ETA: 5:44 - loss: 0.6912 - acc: 0.5423
2880/5677 [==============>...............] - ETA: 5:37 - loss: 0.6916 - acc: 0.5399
2944/5677 [==============>...............] - ETA: 5:30 - loss: 0.6922 - acc: 0.5374
3008/5677 [==============>...............] - ETA: 5:22 - loss: 0.6926 - acc: 0.5356
3072/5677 [===============>..............] - ETA: 5:14 - loss: 0.6924 - acc: 0.5365
3136/5677 [===============>..............] - ETA: 5:06 - loss: 0.6924 - acc: 0.5364
3200/5677 [===============>..............] - ETA: 4:58 - loss: 0.6924 - acc: 0.5359
3264/5677 [================>.............] - ETA: 4:50 - loss: 0.6922 - acc: 0.5383
3328/5677 [================>.............] - ETA: 4:42 - loss: 0.6922 - acc: 0.5388
3392/5677 [================>.............] - ETA: 4:34 - loss: 0.6918 - acc: 0.5395
3456/5677 [=================>............] - ETA: 4:27 - loss: 0.6917 - acc: 0.5394
3520/5677 [=================>............] - ETA: 4:20 - loss: 0.6914 - acc: 0.5415
3584/5677 [=================>............] - ETA: 4:12 - loss: 0.6914 - acc: 0.5405
3648/5677 [==================>...........] - ETA: 4:04 - loss: 0.6923 - acc: 0.5384
3712/5677 [==================>...........] - ETA: 3:57 - loss: 0.6926 - acc: 0.5383
3776/5677 [==================>...........] - ETA: 3:49 - loss: 0.6924 - acc: 0.5397
3840/5677 [===================>..........] - ETA: 3:41 - loss: 0.6923 - acc: 0.5401
3904/5677 [===================>..........] - ETA: 3:33 - loss: 0.6923 - acc: 0.5394
3968/5677 [===================>..........] - ETA: 3:26 - loss: 0.6925 - acc: 0.5401
4032/5677 [====================>.........] - ETA: 3:18 - loss: 0.6924 - acc: 0.5399
4096/5677 [====================>.........] - ETA: 3:11 - loss: 0.6929 - acc: 0.5381
4160/5677 [====================>.........] - ETA: 3:03 - loss: 0.6933 - acc: 0.5363
4224/5677 [=====================>........] - ETA: 2:56 - loss: 0.6933 - acc: 0.5360
4288/5677 [=====================>........] - ETA: 2:48 - loss: 0.6931 - acc: 0.5375
4352/5677 [=====================>........] - ETA: 2:40 - loss: 0.6930 - acc: 0.5375
4416/5677 [======================>.......] - ETA: 2:32 - loss: 0.6929 - acc: 0.5378
4480/5677 [======================>.......] - ETA: 2:25 - loss: 0.6926 - acc: 0.5384
4544/5677 [=======================>......] - ETA: 2:17 - loss: 0.6929 - acc: 0.5370
4608/5677 [=======================>......] - ETA: 2:09 - loss: 0.6929 - acc: 0.5373
4672/5677 [=======================>......] - ETA: 2:01 - loss: 0.6933 - acc: 0.5357
4736/5677 [========================>.....] - ETA: 1:54 - loss: 0.6933 - acc: 0.5357
4800/5677 [========================>.....] - ETA: 1:46 - loss: 0.6934 - acc: 0.5358
4864/5677 [========================>.....] - ETA: 1:38 - loss: 0.6935 - acc: 0.5358
4928/5677 [=========================>....] - ETA: 1:31 - loss: 0.6933 - acc: 0.5359
4992/5677 [=========================>....] - ETA: 1:23 - loss: 0.6932 - acc: 0.5359
5056/5677 [=========================>....] - ETA: 1:15 - loss: 0.6930 - acc: 0.5366
5120/5677 [==========================>...] - ETA: 1:07 - loss: 0.6933 - acc: 0.5359
5184/5677 [==========================>...] - ETA: 1:00 - loss: 0.6935 - acc: 0.5351
5248/5677 [==========================>...] - ETA: 52s - loss: 0.6935 - acc: 0.5358 
5312/5677 [===========================>..] - ETA: 44s - loss: 0.6932 - acc: 0.5363
5376/5677 [===========================>..] - ETA: 36s - loss: 0.6929 - acc: 0.5370
5440/5677 [===========================>..] - ETA: 29s - loss: 0.6926 - acc: 0.5381
5504/5677 [============================>.] - ETA: 21s - loss: 0.6924 - acc: 0.5394
5568/5677 [============================>.] - ETA: 13s - loss: 0.6927 - acc: 0.5379
5632/5677 [============================>.] - ETA: 5s - loss: 0.6925 - acc: 0.5378 
5677/5677 [==============================] - 729s 128ms/step - loss: 0.6925 - acc: 0.5378 - val_loss: 0.6962 - val_acc: 0.4976

Epoch 00003: val_acc did not improve from 0.54358
Epoch 4/10

  64/5677 [..............................] - ETA: 12:54 - loss: 0.6726 - acc: 0.5938
 128/5677 [..............................] - ETA: 13:17 - loss: 0.6713 - acc: 0.5859
 192/5677 [>.............................] - ETA: 12:48 - loss: 0.6798 - acc: 0.5625
 256/5677 [>.............................] - ETA: 12:20 - loss: 0.6756 - acc: 0.5742
 320/5677 [>.............................] - ETA: 11:40 - loss: 0.6718 - acc: 0.5750
 384/5677 [=>............................] - ETA: 11:24 - loss: 0.6766 - acc: 0.5651
 448/5677 [=>............................] - ETA: 11:17 - loss: 0.6797 - acc: 0.5558
 512/5677 [=>............................] - ETA: 11:08 - loss: 0.6743 - acc: 0.5703
 576/5677 [==>...........................] - ETA: 11:00 - loss: 0.6777 - acc: 0.5556
 640/5677 [==>...........................] - ETA: 10:48 - loss: 0.6817 - acc: 0.5516
 704/5677 [==>...........................] - ETA: 10:39 - loss: 0.6816 - acc: 0.5597
 768/5677 [===>..........................] - ETA: 10:28 - loss: 0.6833 - acc: 0.5599
 832/5677 [===>..........................] - ETA: 10:31 - loss: 0.6849 - acc: 0.5577
 896/5677 [===>..........................] - ETA: 10:21 - loss: 0.6845 - acc: 0.5592
 960/5677 [====>.........................] - ETA: 10:12 - loss: 0.6862 - acc: 0.5563
1024/5677 [====>.........................] - ETA: 10:02 - loss: 0.6870 - acc: 0.5527
1088/5677 [====>.........................] - ETA: 9:56 - loss: 0.6875 - acc: 0.5551 
1152/5677 [=====>........................] - ETA: 9:47 - loss: 0.6873 - acc: 0.5590
1216/5677 [=====>........................] - ETA: 9:42 - loss: 0.6863 - acc: 0.5625
1280/5677 [=====>........................] - ETA: 9:33 - loss: 0.6862 - acc: 0.5633
1344/5677 [======>.......................] - ETA: 9:23 - loss: 0.6861 - acc: 0.5640
1408/5677 [======>.......................] - ETA: 9:17 - loss: 0.6852 - acc: 0.5661
1472/5677 [======>.......................] - ETA: 9:08 - loss: 0.6850 - acc: 0.5673
1536/5677 [=======>......................] - ETA: 9:01 - loss: 0.6862 - acc: 0.5651
1600/5677 [=======>......................] - ETA: 8:50 - loss: 0.6875 - acc: 0.5600
1664/5677 [=======>......................] - ETA: 8:41 - loss: 0.6871 - acc: 0.5607
1728/5677 [========>.....................] - ETA: 8:30 - loss: 0.6870 - acc: 0.5608
1792/5677 [========>.....................] - ETA: 8:19 - loss: 0.6859 - acc: 0.5619
1856/5677 [========>.....................] - ETA: 8:11 - loss: 0.6855 - acc: 0.5630
1920/5677 [=========>....................] - ETA: 8:04 - loss: 0.6847 - acc: 0.5646
1984/5677 [=========>....................] - ETA: 7:57 - loss: 0.6855 - acc: 0.5625
2048/5677 [=========>....................] - ETA: 7:47 - loss: 0.6862 - acc: 0.5610
2112/5677 [==========>...................] - ETA: 7:37 - loss: 0.6870 - acc: 0.5592
2176/5677 [==========>...................] - ETA: 7:30 - loss: 0.6874 - acc: 0.5593
2240/5677 [==========>...................] - ETA: 7:23 - loss: 0.6874 - acc: 0.5598
2304/5677 [===========>..................] - ETA: 7:15 - loss: 0.6868 - acc: 0.5612
2368/5677 [===========>..................] - ETA: 7:07 - loss: 0.6862 - acc: 0.5629
2432/5677 [===========>..................] - ETA: 6:59 - loss: 0.6857 - acc: 0.5641
2496/5677 [============>.................] - ETA: 6:51 - loss: 0.6866 - acc: 0.5597
2560/5677 [============>.................] - ETA: 6:43 - loss: 0.6866 - acc: 0.5602
2624/5677 [============>.................] - ETA: 6:35 - loss: 0.6868 - acc: 0.5598
2688/5677 [=============>................] - ETA: 6:26 - loss: 0.6872 - acc: 0.5580
2752/5677 [=============>................] - ETA: 6:19 - loss: 0.6874 - acc: 0.5567
2816/5677 [=============>................] - ETA: 6:10 - loss: 0.6876 - acc: 0.5561
2880/5677 [==============>...............] - ETA: 6:02 - loss: 0.6878 - acc: 0.5542
2944/5677 [==============>...............] - ETA: 5:54 - loss: 0.6878 - acc: 0.5540
3008/5677 [==============>...............] - ETA: 5:46 - loss: 0.6871 - acc: 0.5565
3072/5677 [===============>..............] - ETA: 5:38 - loss: 0.6869 - acc: 0.5573
3136/5677 [===============>..............] - ETA: 5:29 - loss: 0.6869 - acc: 0.5571
3200/5677 [===============>..............] - ETA: 5:21 - loss: 0.6862 - acc: 0.5594
3264/5677 [================>.............] - ETA: 5:13 - loss: 0.6859 - acc: 0.5600
3328/5677 [================>.............] - ETA: 5:04 - loss: 0.6864 - acc: 0.5586
3392/5677 [================>.............] - ETA: 4:57 - loss: 0.6861 - acc: 0.5590
3456/5677 [=================>............] - ETA: 4:48 - loss: 0.6864 - acc: 0.5587
3520/5677 [=================>............] - ETA: 4:40 - loss: 0.6866 - acc: 0.5580
3584/5677 [=================>............] - ETA: 4:32 - loss: 0.6868 - acc: 0.5575
3648/5677 [==================>...........] - ETA: 4:23 - loss: 0.6868 - acc: 0.5554
3712/5677 [==================>...........] - ETA: 4:15 - loss: 0.6865 - acc: 0.5568
3776/5677 [==================>...........] - ETA: 4:07 - loss: 0.6866 - acc: 0.5561
3840/5677 [===================>..........] - ETA: 3:59 - loss: 0.6868 - acc: 0.5565
3904/5677 [===================>..........] - ETA: 3:51 - loss: 0.6872 - acc: 0.5548
3968/5677 [===================>..........] - ETA: 3:42 - loss: 0.6868 - acc: 0.5565
4032/5677 [====================>.........] - ETA: 3:34 - loss: 0.6877 - acc: 0.5541
4096/5677 [====================>.........] - ETA: 3:26 - loss: 0.6881 - acc: 0.5520
4160/5677 [====================>.........] - ETA: 3:18 - loss: 0.6876 - acc: 0.5531
4224/5677 [=====================>........] - ETA: 3:09 - loss: 0.6877 - acc: 0.5526
4288/5677 [=====================>........] - ETA: 3:01 - loss: 0.6882 - acc: 0.5518
4352/5677 [=====================>........] - ETA: 2:53 - loss: 0.6887 - acc: 0.5510
4416/5677 [======================>.......] - ETA: 2:45 - loss: 0.6883 - acc: 0.5512
4480/5677 [======================>.......] - ETA: 2:36 - loss: 0.6880 - acc: 0.5522
4544/5677 [=======================>......] - ETA: 2:28 - loss: 0.6883 - acc: 0.5515
4608/5677 [=======================>......] - ETA: 2:19 - loss: 0.6883 - acc: 0.5512
4672/5677 [=======================>......] - ETA: 2:11 - loss: 0.6884 - acc: 0.5505
4736/5677 [========================>.....] - ETA: 2:03 - loss: 0.6883 - acc: 0.5503
4800/5677 [========================>.....] - ETA: 1:54 - loss: 0.6884 - acc: 0.5500
4864/5677 [========================>.....] - ETA: 1:46 - loss: 0.6885 - acc: 0.5483
4928/5677 [=========================>....] - ETA: 1:38 - loss: 0.6881 - acc: 0.5491
4992/5677 [=========================>....] - ETA: 1:29 - loss: 0.6877 - acc: 0.5505
5056/5677 [=========================>....] - ETA: 1:21 - loss: 0.6880 - acc: 0.5500
5120/5677 [==========================>...] - ETA: 1:12 - loss: 0.6884 - acc: 0.5496
5184/5677 [==========================>...] - ETA: 1:04 - loss: 0.6882 - acc: 0.5502
5248/5677 [==========================>...] - ETA: 56s - loss: 0.6878 - acc: 0.5514 
5312/5677 [===========================>..] - ETA: 47s - loss: 0.6878 - acc: 0.5510
5376/5677 [===========================>..] - ETA: 39s - loss: 0.6878 - acc: 0.5512
5440/5677 [===========================>..] - ETA: 31s - loss: 0.6873 - acc: 0.5524
5504/5677 [============================>.] - ETA: 22s - loss: 0.6872 - acc: 0.5529
5568/5677 [============================>.] - ETA: 14s - loss: 0.6872 - acc: 0.5535
5632/5677 [============================>.] - ETA: 5s - loss: 0.6876 - acc: 0.5527 
5677/5677 [==============================] - 776s 137ms/step - loss: 0.6874 - acc: 0.5538 - val_loss: 0.6768 - val_acc: 0.5737

Epoch 00004: val_acc improved from 0.54358 to 0.57369, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window03/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 5/10

  64/5677 [..............................] - ETA: 11:56 - loss: 0.6872 - acc: 0.5469
 128/5677 [..............................] - ETA: 12:52 - loss: 0.7035 - acc: 0.5312
 192/5677 [>.............................] - ETA: 12:34 - loss: 0.7013 - acc: 0.5312
 256/5677 [>.............................] - ETA: 12:25 - loss: 0.6933 - acc: 0.5508
 320/5677 [>.............................] - ETA: 12:06 - loss: 0.6933 - acc: 0.5312
 384/5677 [=>............................] - ETA: 11:45 - loss: 0.6982 - acc: 0.5130
 448/5677 [=>............................] - ETA: 11:36 - loss: 0.6996 - acc: 0.5045
 512/5677 [=>............................] - ETA: 11:26 - loss: 0.7026 - acc: 0.5000
 576/5677 [==>...........................] - ETA: 11:23 - loss: 0.6962 - acc: 0.5156
 640/5677 [==>...........................] - ETA: 11:11 - loss: 0.6945 - acc: 0.5234
 704/5677 [==>...........................] - ETA: 11:07 - loss: 0.6966 - acc: 0.5156
 768/5677 [===>..........................] - ETA: 10:53 - loss: 0.6928 - acc: 0.5326
 832/5677 [===>..........................] - ETA: 10:48 - loss: 0.6928 - acc: 0.5325
 896/5677 [===>..........................] - ETA: 10:36 - loss: 0.6921 - acc: 0.5357
 960/5677 [====>.........................] - ETA: 10:28 - loss: 0.6906 - acc: 0.5406
1024/5677 [====>.........................] - ETA: 10:22 - loss: 0.6885 - acc: 0.5469
1088/5677 [====>.........................] - ETA: 10:10 - loss: 0.6879 - acc: 0.5469
1152/5677 [=====>........................] - ETA: 10:04 - loss: 0.6866 - acc: 0.5477
1216/5677 [=====>........................] - ETA: 9:53 - loss: 0.6870 - acc: 0.5444 
1280/5677 [=====>........................] - ETA: 9:46 - loss: 0.6890 - acc: 0.5367
1344/5677 [======>.......................] - ETA: 9:37 - loss: 0.6905 - acc: 0.5327
1408/5677 [======>.......................] - ETA: 9:26 - loss: 0.6890 - acc: 0.5369
1472/5677 [======>.......................] - ETA: 9:19 - loss: 0.6897 - acc: 0.5340
1536/5677 [=======>......................] - ETA: 9:09 - loss: 0.6899 - acc: 0.5352
1600/5677 [=======>......................] - ETA: 9:02 - loss: 0.6885 - acc: 0.5375
1664/5677 [=======>......................] - ETA: 8:51 - loss: 0.6891 - acc: 0.5379
1728/5677 [========>.....................] - ETA: 8:42 - loss: 0.6906 - acc: 0.5347
1792/5677 [========>.....................] - ETA: 8:35 - loss: 0.6921 - acc: 0.5324
1856/5677 [========>.....................] - ETA: 8:27 - loss: 0.6918 - acc: 0.5334
1920/5677 [=========>....................] - ETA: 8:19 - loss: 0.6920 - acc: 0.5333
1984/5677 [=========>....................] - ETA: 8:09 - loss: 0.6932 - acc: 0.5307
2048/5677 [=========>....................] - ETA: 8:00 - loss: 0.6930 - acc: 0.5322
2112/5677 [==========>...................] - ETA: 7:51 - loss: 0.6922 - acc: 0.5341
2176/5677 [==========>...................] - ETA: 7:42 - loss: 0.6924 - acc: 0.5331
2240/5677 [==========>...................] - ETA: 7:34 - loss: 0.6912 - acc: 0.5344
2304/5677 [===========>..................] - ETA: 7:26 - loss: 0.6915 - acc: 0.5334
2368/5677 [===========>..................] - ETA: 7:19 - loss: 0.6921 - acc: 0.5312
2432/5677 [===========>..................] - ETA: 7:09 - loss: 0.6923 - acc: 0.5321
2496/5677 [============>.................] - ETA: 7:01 - loss: 0.6919 - acc: 0.5321
2560/5677 [============>.................] - ETA: 6:53 - loss: 0.6920 - acc: 0.5324
2624/5677 [============>.................] - ETA: 6:45 - loss: 0.6920 - acc: 0.5324
2688/5677 [=============>................] - ETA: 6:36 - loss: 0.6917 - acc: 0.5327
2752/5677 [=============>................] - ETA: 6:28 - loss: 0.6918 - acc: 0.5316
2816/5677 [=============>................] - ETA: 6:19 - loss: 0.6912 - acc: 0.5348
2880/5677 [==============>...............] - ETA: 6:10 - loss: 0.6912 - acc: 0.5351
2944/5677 [==============>...............] - ETA: 6:02 - loss: 0.6905 - acc: 0.5370
3008/5677 [==============>...............] - ETA: 5:53 - loss: 0.6904 - acc: 0.5356
3072/5677 [===============>..............] - ETA: 5:45 - loss: 0.6907 - acc: 0.5348
3136/5677 [===============>..............] - ETA: 5:37 - loss: 0.6906 - acc: 0.5351
3200/5677 [===============>..............] - ETA: 5:28 - loss: 0.6907 - acc: 0.5363
3264/5677 [================>.............] - ETA: 5:20 - loss: 0.6900 - acc: 0.5377
3328/5677 [================>.............] - ETA: 5:11 - loss: 0.6896 - acc: 0.5388
3392/5677 [================>.............] - ETA: 5:03 - loss: 0.6892 - acc: 0.5392
3456/5677 [=================>............] - ETA: 4:54 - loss: 0.6895 - acc: 0.5394
3520/5677 [=================>............] - ETA: 4:46 - loss: 0.6905 - acc: 0.5381
3584/5677 [=================>............] - ETA: 4:37 - loss: 0.6907 - acc: 0.5379
3648/5677 [==================>...........] - ETA: 4:29 - loss: 0.6898 - acc: 0.5397
3712/5677 [==================>...........] - ETA: 4:20 - loss: 0.6898 - acc: 0.5396
3776/5677 [==================>...........] - ETA: 4:12 - loss: 0.6893 - acc: 0.5418
3840/5677 [===================>..........] - ETA: 4:03 - loss: 0.6891 - acc: 0.5422
3904/5677 [===================>..........] - ETA: 3:54 - loss: 0.6893 - acc: 0.5410
3968/5677 [===================>..........] - ETA: 3:46 - loss: 0.6897 - acc: 0.5406
4032/5677 [====================>.........] - ETA: 3:38 - loss: 0.6896 - acc: 0.5419
4096/5677 [====================>.........] - ETA: 3:29 - loss: 0.6899 - acc: 0.5413
4160/5677 [====================>.........] - ETA: 3:20 - loss: 0.6899 - acc: 0.5413
4224/5677 [=====================>........] - ETA: 3:12 - loss: 0.6892 - acc: 0.5429
4288/5677 [=====================>........] - ETA: 3:03 - loss: 0.6895 - acc: 0.5424
4352/5677 [=====================>........] - ETA: 2:55 - loss: 0.6896 - acc: 0.5420
4416/5677 [======================>.......] - ETA: 2:46 - loss: 0.6892 - acc: 0.5435
4480/5677 [======================>.......] - ETA: 2:38 - loss: 0.6893 - acc: 0.5435
4544/5677 [=======================>......] - ETA: 2:29 - loss: 0.6891 - acc: 0.5434
4608/5677 [=======================>......] - ETA: 2:21 - loss: 0.6886 - acc: 0.5436
4672/5677 [=======================>......] - ETA: 2:12 - loss: 0.6886 - acc: 0.5445
4736/5677 [========================>.....] - ETA: 2:04 - loss: 0.6886 - acc: 0.5452
4800/5677 [========================>.....] - ETA: 1:55 - loss: 0.6882 - acc: 0.5465
4864/5677 [========================>.....] - ETA: 1:47 - loss: 0.6881 - acc: 0.5473
4928/5677 [=========================>....] - ETA: 1:38 - loss: 0.6877 - acc: 0.5479
4992/5677 [=========================>....] - ETA: 1:30 - loss: 0.6878 - acc: 0.5485
5056/5677 [=========================>....] - ETA: 1:21 - loss: 0.6875 - acc: 0.5494
5120/5677 [==========================>...] - ETA: 1:13 - loss: 0.6874 - acc: 0.5496
5184/5677 [==========================>...] - ETA: 1:04 - loss: 0.6869 - acc: 0.5511
5248/5677 [==========================>...] - ETA: 56s - loss: 0.6866 - acc: 0.5514 
5312/5677 [===========================>..] - ETA: 47s - loss: 0.6866 - acc: 0.5512
5376/5677 [===========================>..] - ETA: 39s - loss: 0.6865 - acc: 0.5517
5440/5677 [===========================>..] - ETA: 31s - loss: 0.6866 - acc: 0.5517
5504/5677 [============================>.] - ETA: 22s - loss: 0.6866 - acc: 0.5518
5568/5677 [============================>.] - ETA: 14s - loss: 0.6867 - acc: 0.5523
5632/5677 [============================>.] - ETA: 5s - loss: 0.6868 - acc: 0.5520 
5677/5677 [==============================] - 768s 135ms/step - loss: 0.6863 - acc: 0.5528 - val_loss: 0.6764 - val_acc: 0.5959

Epoch 00005: val_acc improved from 0.57369 to 0.59588, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window03/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 6/10

  64/5677 [..............................] - ETA: 10:30 - loss: 0.6244 - acc: 0.6250
 128/5677 [..............................] - ETA: 10:36 - loss: 0.6714 - acc: 0.5625
 192/5677 [>.............................] - ETA: 10:15 - loss: 0.6779 - acc: 0.5677
 256/5677 [>.............................] - ETA: 10:19 - loss: 0.6839 - acc: 0.5625
 320/5677 [>.............................] - ETA: 10:20 - loss: 0.6864 - acc: 0.5563
 384/5677 [=>............................] - ETA: 10:17 - loss: 0.6893 - acc: 0.5573
 448/5677 [=>............................] - ETA: 9:57 - loss: 0.6921 - acc: 0.5491 
 512/5677 [=>............................] - ETA: 9:40 - loss: 0.6896 - acc: 0.5566
 576/5677 [==>...........................] - ETA: 9:28 - loss: 0.6921 - acc: 0.5486
 640/5677 [==>...........................] - ETA: 9:16 - loss: 0.6934 - acc: 0.5500
 704/5677 [==>...........................] - ETA: 9:07 - loss: 0.6931 - acc: 0.5440
 768/5677 [===>..........................] - ETA: 9:03 - loss: 0.6906 - acc: 0.5469
 832/5677 [===>..........................] - ETA: 8:52 - loss: 0.6906 - acc: 0.5457
 896/5677 [===>..........................] - ETA: 8:46 - loss: 0.6894 - acc: 0.5435
 960/5677 [====>.........................] - ETA: 8:40 - loss: 0.6893 - acc: 0.5427
1024/5677 [====>.........................] - ETA: 8:31 - loss: 0.6876 - acc: 0.5459
1088/5677 [====>.........................] - ETA: 8:22 - loss: 0.6884 - acc: 0.5441
1152/5677 [=====>........................] - ETA: 8:15 - loss: 0.6860 - acc: 0.5495
1216/5677 [=====>........................] - ETA: 8:11 - loss: 0.6865 - acc: 0.5518
1280/5677 [=====>........................] - ETA: 8:11 - loss: 0.6862 - acc: 0.5531
1344/5677 [======>.......................] - ETA: 8:06 - loss: 0.6866 - acc: 0.5521
1408/5677 [======>.......................] - ETA: 7:57 - loss: 0.6856 - acc: 0.5526
1472/5677 [======>.......................] - ETA: 7:51 - loss: 0.6867 - acc: 0.5489
1536/5677 [=======>......................] - ETA: 7:44 - loss: 0.6869 - acc: 0.5508
1600/5677 [=======>......................] - ETA: 7:37 - loss: 0.6857 - acc: 0.5531
1664/5677 [=======>......................] - ETA: 7:30 - loss: 0.6843 - acc: 0.5547
1728/5677 [========>.....................] - ETA: 7:22 - loss: 0.6846 - acc: 0.5550
1792/5677 [========>.....................] - ETA: 7:15 - loss: 0.6837 - acc: 0.5586
1856/5677 [========>.....................] - ETA: 7:07 - loss: 0.6824 - acc: 0.5609
1920/5677 [=========>....................] - ETA: 6:59 - loss: 0.6826 - acc: 0.5583
1984/5677 [=========>....................] - ETA: 6:52 - loss: 0.6835 - acc: 0.5554
2048/5677 [=========>....................] - ETA: 6:44 - loss: 0.6854 - acc: 0.5532
2112/5677 [==========>...................] - ETA: 6:37 - loss: 0.6839 - acc: 0.5563
2176/5677 [==========>...................] - ETA: 6:30 - loss: 0.6831 - acc: 0.5579
2240/5677 [==========>...................] - ETA: 6:23 - loss: 0.6840 - acc: 0.5567
2304/5677 [===========>..................] - ETA: 6:16 - loss: 0.6839 - acc: 0.5564
2368/5677 [===========>..................] - ETA: 6:10 - loss: 0.6836 - acc: 0.5566
2432/5677 [===========>..................] - ETA: 6:03 - loss: 0.6842 - acc: 0.5563
2496/5677 [============>.................] - ETA: 5:55 - loss: 0.6848 - acc: 0.5541
2560/5677 [============>.................] - ETA: 5:48 - loss: 0.6850 - acc: 0.5531
2624/5677 [============>.................] - ETA: 5:40 - loss: 0.6846 - acc: 0.5534
2688/5677 [=============>................] - ETA: 5:34 - loss: 0.6843 - acc: 0.5543
2752/5677 [=============>................] - ETA: 5:27 - loss: 0.6846 - acc: 0.5541
2816/5677 [=============>................] - ETA: 5:20 - loss: 0.6852 - acc: 0.5526
2880/5677 [==============>...............] - ETA: 5:14 - loss: 0.6854 - acc: 0.5510
2944/5677 [==============>...............] - ETA: 5:07 - loss: 0.6858 - acc: 0.5493
3008/5677 [==============>...............] - ETA: 5:00 - loss: 0.6859 - acc: 0.5502
3072/5677 [===============>..............] - ETA: 4:54 - loss: 0.6860 - acc: 0.5505
3136/5677 [===============>..............] - ETA: 4:47 - loss: 0.6867 - acc: 0.5482
3200/5677 [===============>..............] - ETA: 4:41 - loss: 0.6869 - acc: 0.5466
3264/5677 [================>.............] - ETA: 4:34 - loss: 0.6864 - acc: 0.5487
3328/5677 [================>.............] - ETA: 4:27 - loss: 0.6860 - acc: 0.5499
3392/5677 [================>.............] - ETA: 4:20 - loss: 0.6860 - acc: 0.5504
3456/5677 [=================>............] - ETA: 4:13 - loss: 0.6867 - acc: 0.5486
3520/5677 [=================>............] - ETA: 4:06 - loss: 0.6870 - acc: 0.5486
3584/5677 [=================>............] - ETA: 3:58 - loss: 0.6866 - acc: 0.5491
3648/5677 [==================>...........] - ETA: 3:51 - loss: 0.6860 - acc: 0.5496
3712/5677 [==================>...........] - ETA: 3:43 - loss: 0.6861 - acc: 0.5482
3776/5677 [==================>...........] - ETA: 3:36 - loss: 0.6861 - acc: 0.5482
3840/5677 [===================>..........] - ETA: 3:29 - loss: 0.6857 - acc: 0.5500
3904/5677 [===================>..........] - ETA: 3:22 - loss: 0.6858 - acc: 0.5497
3968/5677 [===================>..........] - ETA: 3:15 - loss: 0.6855 - acc: 0.5507
4032/5677 [====================>.........] - ETA: 3:08 - loss: 0.6857 - acc: 0.5511
4096/5677 [====================>.........] - ETA: 3:00 - loss: 0.6855 - acc: 0.5508
4160/5677 [====================>.........] - ETA: 2:53 - loss: 0.6853 - acc: 0.5519
4224/5677 [=====================>........] - ETA: 2:46 - loss: 0.6847 - acc: 0.5542
4288/5677 [=====================>........] - ETA: 2:38 - loss: 0.6844 - acc: 0.5560
4352/5677 [=====================>........] - ETA: 2:31 - loss: 0.6850 - acc: 0.5554
4416/5677 [======================>.......] - ETA: 2:24 - loss: 0.6856 - acc: 0.5543
4480/5677 [======================>.......] - ETA: 2:16 - loss: 0.6857 - acc: 0.5549
4544/5677 [=======================>......] - ETA: 2:09 - loss: 0.6851 - acc: 0.5559
4608/5677 [=======================>......] - ETA: 2:02 - loss: 0.6848 - acc: 0.5566
4672/5677 [=======================>......] - ETA: 1:54 - loss: 0.6846 - acc: 0.5567
4736/5677 [========================>.....] - ETA: 1:47 - loss: 0.6851 - acc: 0.5562
4800/5677 [========================>.....] - ETA: 1:40 - loss: 0.6845 - acc: 0.5577
4864/5677 [========================>.....] - ETA: 1:32 - loss: 0.6847 - acc: 0.5582
4928/5677 [=========================>....] - ETA: 1:25 - loss: 0.6841 - acc: 0.5607
4992/5677 [=========================>....] - ETA: 1:18 - loss: 0.6843 - acc: 0.5607
5056/5677 [=========================>....] - ETA: 1:11 - loss: 0.6842 - acc: 0.5605
5120/5677 [==========================>...] - ETA: 1:03 - loss: 0.6840 - acc: 0.5617
5184/5677 [==========================>...] - ETA: 56s - loss: 0.6838 - acc: 0.5617 
5248/5677 [==========================>...] - ETA: 49s - loss: 0.6838 - acc: 0.5623
5312/5677 [===========================>..] - ETA: 41s - loss: 0.6838 - acc: 0.5629
5376/5677 [===========================>..] - ETA: 34s - loss: 0.6837 - acc: 0.5632
5440/5677 [===========================>..] - ETA: 27s - loss: 0.6837 - acc: 0.5636
5504/5677 [============================>.] - ETA: 19s - loss: 0.6833 - acc: 0.5641
5568/5677 [============================>.] - ETA: 12s - loss: 0.6830 - acc: 0.5647
5632/5677 [============================>.] - ETA: 5s - loss: 0.6830 - acc: 0.5641 
5677/5677 [==============================] - 681s 120ms/step - loss: 0.6831 - acc: 0.5639 - val_loss: 0.6729 - val_acc: 0.5943

Epoch 00006: val_acc did not improve from 0.59588
Epoch 7/10

  64/5677 [..............................] - ETA: 10:26 - loss: 0.6704 - acc: 0.6094
 128/5677 [..............................] - ETA: 10:37 - loss: 0.6795 - acc: 0.5703
 192/5677 [>.............................] - ETA: 10:45 - loss: 0.6875 - acc: 0.5260
 256/5677 [>.............................] - ETA: 10:37 - loss: 0.6832 - acc: 0.5195
 320/5677 [>.............................] - ETA: 11:06 - loss: 0.6818 - acc: 0.5344
 384/5677 [=>............................] - ETA: 11:02 - loss: 0.6845 - acc: 0.5339
 448/5677 [=>............................] - ETA: 10:45 - loss: 0.6854 - acc: 0.5268
 512/5677 [=>............................] - ETA: 10:38 - loss: 0.6830 - acc: 0.5352
 576/5677 [==>...........................] - ETA: 10:30 - loss: 0.6824 - acc: 0.5330
 640/5677 [==>...........................] - ETA: 10:18 - loss: 0.6849 - acc: 0.5312
 704/5677 [==>...........................] - ETA: 10:08 - loss: 0.6833 - acc: 0.5298
 768/5677 [===>..........................] - ETA: 10:03 - loss: 0.6850 - acc: 0.5326
 832/5677 [===>..........................] - ETA: 9:50 - loss: 0.6839 - acc: 0.5325 
 896/5677 [===>..........................] - ETA: 9:43 - loss: 0.6845 - acc: 0.5324
 960/5677 [====>.........................] - ETA: 9:31 - loss: 0.6847 - acc: 0.5312
1024/5677 [====>.........................] - ETA: 9:28 - loss: 0.6845 - acc: 0.5322
1088/5677 [====>.........................] - ETA: 9:18 - loss: 0.6863 - acc: 0.5285
1152/5677 [=====>........................] - ETA: 9:15 - loss: 0.6859 - acc: 0.5295
1216/5677 [=====>........................] - ETA: 9:06 - loss: 0.6848 - acc: 0.5329
1280/5677 [=====>........................] - ETA: 8:55 - loss: 0.6838 - acc: 0.5383
1344/5677 [======>.......................] - ETA: 8:45 - loss: 0.6838 - acc: 0.5402
1408/5677 [======>.......................] - ETA: 8:37 - loss: 0.6820 - acc: 0.5462
1472/5677 [======>.......................] - ETA: 8:28 - loss: 0.6820 - acc: 0.5442
1536/5677 [=======>......................] - ETA: 8:20 - loss: 0.6822 - acc: 0.5436
1600/5677 [=======>......................] - ETA: 8:13 - loss: 0.6823 - acc: 0.5437
1664/5677 [=======>......................] - ETA: 8:10 - loss: 0.6833 - acc: 0.5421
1728/5677 [========>.....................] - ETA: 8:03 - loss: 0.6837 - acc: 0.5422
1792/5677 [========>.....................] - ETA: 7:54 - loss: 0.6835 - acc: 0.5458
1856/5677 [========>.....................] - ETA: 7:45 - loss: 0.6832 - acc: 0.5469
1920/5677 [=========>....................] - ETA: 7:35 - loss: 0.6833 - acc: 0.5448
1984/5677 [=========>....................] - ETA: 7:28 - loss: 0.6828 - acc: 0.5433
2048/5677 [=========>....................] - ETA: 7:21 - loss: 0.6836 - acc: 0.5439
2112/5677 [==========>...................] - ETA: 7:14 - loss: 0.6833 - acc: 0.5459
2176/5677 [==========>...................] - ETA: 7:06 - loss: 0.6832 - acc: 0.5460
2240/5677 [==========>...................] - ETA: 6:58 - loss: 0.6831 - acc: 0.5464
2304/5677 [===========>..................] - ETA: 6:50 - loss: 0.6826 - acc: 0.5477
2368/5677 [===========>..................] - ETA: 6:43 - loss: 0.6831 - acc: 0.5456
2432/5677 [===========>..................] - ETA: 6:35 - loss: 0.6828 - acc: 0.5469
2496/5677 [============>.................] - ETA: 6:25 - loss: 0.6832 - acc: 0.5465
2560/5677 [============>.................] - ETA: 6:17 - loss: 0.6835 - acc: 0.5469
2624/5677 [============>.................] - ETA: 6:08 - loss: 0.6835 - acc: 0.5469
2688/5677 [=============>................] - ETA: 6:00 - loss: 0.6835 - acc: 0.5472
2752/5677 [=============>................] - ETA: 5:53 - loss: 0.6833 - acc: 0.5465
2816/5677 [=============>................] - ETA: 5:46 - loss: 0.6845 - acc: 0.5451
2880/5677 [==============>...............] - ETA: 5:38 - loss: 0.6839 - acc: 0.5469
2944/5677 [==============>...............] - ETA: 5:30 - loss: 0.6835 - acc: 0.5479
3008/5677 [==============>...............] - ETA: 5:22 - loss: 0.6841 - acc: 0.5475
3072/5677 [===============>..............] - ETA: 5:14 - loss: 0.6839 - acc: 0.5479
3136/5677 [===============>..............] - ETA: 5:06 - loss: 0.6852 - acc: 0.5453
3200/5677 [===============>..............] - ETA: 4:58 - loss: 0.6853 - acc: 0.5444
3264/5677 [================>.............] - ETA: 4:51 - loss: 0.6855 - acc: 0.5441
3328/5677 [================>.............] - ETA: 4:43 - loss: 0.6845 - acc: 0.5472
3392/5677 [================>.............] - ETA: 4:35 - loss: 0.6846 - acc: 0.5472
3456/5677 [=================>............] - ETA: 4:27 - loss: 0.6848 - acc: 0.5466
3520/5677 [=================>............] - ETA: 4:20 - loss: 0.6843 - acc: 0.5474
3584/5677 [=================>............] - ETA: 4:12 - loss: 0.6837 - acc: 0.5499
3648/5677 [==================>...........] - ETA: 4:03 - loss: 0.6834 - acc: 0.5499
3712/5677 [==================>...........] - ETA: 3:55 - loss: 0.6829 - acc: 0.5506
3776/5677 [==================>...........] - ETA: 3:48 - loss: 0.6827 - acc: 0.5501
3840/5677 [===================>..........] - ETA: 3:40 - loss: 0.6828 - acc: 0.5505
3904/5677 [===================>..........] - ETA: 3:32 - loss: 0.6826 - acc: 0.5520
3968/5677 [===================>..........] - ETA: 3:24 - loss: 0.6822 - acc: 0.5529
4032/5677 [====================>.........] - ETA: 3:17 - loss: 0.6817 - acc: 0.5546
4096/5677 [====================>.........] - ETA: 3:09 - loss: 0.6822 - acc: 0.5544
4160/5677 [====================>.........] - ETA: 3:01 - loss: 0.6818 - acc: 0.5553
4224/5677 [=====================>........] - ETA: 2:54 - loss: 0.6820 - acc: 0.5545
4288/5677 [=====================>........] - ETA: 2:46 - loss: 0.6828 - acc: 0.5546
4352/5677 [=====================>........] - ETA: 2:38 - loss: 0.6827 - acc: 0.5545
4416/5677 [======================>.......] - ETA: 2:30 - loss: 0.6828 - acc: 0.5541
4480/5677 [======================>.......] - ETA: 2:22 - loss: 0.6831 - acc: 0.5540
4544/5677 [=======================>......] - ETA: 2:15 - loss: 0.6829 - acc: 0.5546
4608/5677 [=======================>......] - ETA: 2:07 - loss: 0.6834 - acc: 0.5534
4672/5677 [=======================>......] - ETA: 2:00 - loss: 0.6831 - acc: 0.5531
4736/5677 [========================>.....] - ETA: 1:52 - loss: 0.6831 - acc: 0.5528
4800/5677 [========================>.....] - ETA: 1:44 - loss: 0.6835 - acc: 0.5517
4864/5677 [========================>.....] - ETA: 1:36 - loss: 0.6835 - acc: 0.5502
4928/5677 [=========================>....] - ETA: 1:29 - loss: 0.6835 - acc: 0.5505
4992/5677 [=========================>....] - ETA: 1:21 - loss: 0.6840 - acc: 0.5493
5056/5677 [=========================>....] - ETA: 1:13 - loss: 0.6834 - acc: 0.5504
5120/5677 [==========================>...] - ETA: 1:06 - loss: 0.6830 - acc: 0.5521
5184/5677 [==========================>...] - ETA: 58s - loss: 0.6825 - acc: 0.5538 
5248/5677 [==========================>...] - ETA: 51s - loss: 0.6825 - acc: 0.5547
5312/5677 [===========================>..] - ETA: 43s - loss: 0.6825 - acc: 0.5555
5376/5677 [===========================>..] - ETA: 35s - loss: 0.6823 - acc: 0.5556
5440/5677 [===========================>..] - ETA: 28s - loss: 0.6823 - acc: 0.5559
5504/5677 [============================>.] - ETA: 20s - loss: 0.6823 - acc: 0.5563
5568/5677 [============================>.] - ETA: 12s - loss: 0.6824 - acc: 0.5566
5632/5677 [============================>.] - ETA: 5s - loss: 0.6821 - acc: 0.5574 
5677/5677 [==============================] - 701s 124ms/step - loss: 0.6822 - acc: 0.5573 - val_loss: 0.6747 - val_acc: 0.5943

Epoch 00007: val_acc did not improve from 0.59588
Epoch 8/10

  64/5677 [..............................] - ETA: 11:39 - loss: 0.6959 - acc: 0.5469
 128/5677 [..............................] - ETA: 10:38 - loss: 0.6746 - acc: 0.6172
 192/5677 [>.............................] - ETA: 10:44 - loss: 0.6735 - acc: 0.6146
 256/5677 [>.............................] - ETA: 10:25 - loss: 0.6676 - acc: 0.6133
 320/5677 [>.............................] - ETA: 10:09 - loss: 0.6727 - acc: 0.6062
 384/5677 [=>............................] - ETA: 10:00 - loss: 0.6796 - acc: 0.5885
 448/5677 [=>............................] - ETA: 9:58 - loss: 0.6759 - acc: 0.5960 
 512/5677 [=>............................] - ETA: 9:49 - loss: 0.6765 - acc: 0.5898
 576/5677 [==>...........................] - ETA: 9:42 - loss: 0.6754 - acc: 0.5955
 640/5677 [==>...........................] - ETA: 9:39 - loss: 0.6715 - acc: 0.6000
 704/5677 [==>...........................] - ETA: 9:35 - loss: 0.6714 - acc: 0.5938
 768/5677 [===>..........................] - ETA: 9:27 - loss: 0.6707 - acc: 0.5951
 832/5677 [===>..........................] - ETA: 9:29 - loss: 0.6749 - acc: 0.5901
 896/5677 [===>..........................] - ETA: 9:25 - loss: 0.6760 - acc: 0.5938
 960/5677 [====>.........................] - ETA: 9:17 - loss: 0.6774 - acc: 0.5906
1024/5677 [====>.........................] - ETA: 9:08 - loss: 0.6797 - acc: 0.5811
1088/5677 [====>.........................] - ETA: 8:56 - loss: 0.6799 - acc: 0.5781
1152/5677 [=====>........................] - ETA: 8:45 - loss: 0.6798 - acc: 0.5790
1216/5677 [=====>........................] - ETA: 8:38 - loss: 0.6820 - acc: 0.5732
1280/5677 [=====>........................] - ETA: 8:31 - loss: 0.6826 - acc: 0.5711
1344/5677 [======>.......................] - ETA: 8:23 - loss: 0.6828 - acc: 0.5685
1408/5677 [======>.......................] - ETA: 8:16 - loss: 0.6822 - acc: 0.5732
1472/5677 [======>.......................] - ETA: 8:07 - loss: 0.6834 - acc: 0.5713
1536/5677 [=======>......................] - ETA: 7:58 - loss: 0.6847 - acc: 0.5651
1600/5677 [=======>......................] - ETA: 7:50 - loss: 0.6857 - acc: 0.5606
1664/5677 [=======>......................] - ETA: 7:43 - loss: 0.6849 - acc: 0.5625
1728/5677 [========>.....................] - ETA: 7:37 - loss: 0.6863 - acc: 0.5596
1792/5677 [========>.....................] - ETA: 7:28 - loss: 0.6873 - acc: 0.5558
1856/5677 [========>.....................] - ETA: 7:21 - loss: 0.6882 - acc: 0.5544
1920/5677 [=========>....................] - ETA: 7:14 - loss: 0.6874 - acc: 0.5568
1984/5677 [=========>....................] - ETA: 7:07 - loss: 0.6869 - acc: 0.5559
2048/5677 [=========>....................] - ETA: 7:01 - loss: 0.6862 - acc: 0.5566
2112/5677 [==========>...................] - ETA: 6:56 - loss: 0.6882 - acc: 0.5530
2176/5677 [==========>...................] - ETA: 6:51 - loss: 0.6882 - acc: 0.5510
2240/5677 [==========>...................] - ETA: 6:45 - loss: 0.6874 - acc: 0.5531
2304/5677 [===========>..................] - ETA: 6:39 - loss: 0.6872 - acc: 0.5521
2368/5677 [===========>..................] - ETA: 6:33 - loss: 0.6876 - acc: 0.5511
2432/5677 [===========>..................] - ETA: 6:26 - loss: 0.6878 - acc: 0.5493
2496/5677 [============>.................] - ETA: 6:20 - loss: 0.6875 - acc: 0.5501
2560/5677 [============>.................] - ETA: 6:13 - loss: 0.6868 - acc: 0.5512
2624/5677 [============>.................] - ETA: 6:07 - loss: 0.6868 - acc: 0.5511
2688/5677 [=============>................] - ETA: 6:00 - loss: 0.6867 - acc: 0.5521
2752/5677 [=============>................] - ETA: 5:53 - loss: 0.6865 - acc: 0.5520
2816/5677 [=============>................] - ETA: 5:45 - loss: 0.6866 - acc: 0.5511
2880/5677 [==============>...............] - ETA: 5:38 - loss: 0.6868 - acc: 0.5514
2944/5677 [==============>...............] - ETA: 5:30 - loss: 0.6863 - acc: 0.5516
3008/5677 [==============>...............] - ETA: 5:23 - loss: 0.6860 - acc: 0.5529
3072/5677 [===============>..............] - ETA: 5:15 - loss: 0.6867 - acc: 0.5501
3136/5677 [===============>..............] - ETA: 5:08 - loss: 0.6872 - acc: 0.5475
3200/5677 [===============>..............] - ETA: 5:02 - loss: 0.6874 - acc: 0.5484
3264/5677 [================>.............] - ETA: 4:55 - loss: 0.6876 - acc: 0.5481
3328/5677 [================>.............] - ETA: 4:48 - loss: 0.6874 - acc: 0.5481
3392/5677 [================>.............] - ETA: 4:40 - loss: 0.6878 - acc: 0.5478
3456/5677 [=================>............] - ETA: 4:33 - loss: 0.6878 - acc: 0.5486
3520/5677 [=================>............] - ETA: 4:26 - loss: 0.6873 - acc: 0.5506
3584/5677 [=================>............] - ETA: 4:18 - loss: 0.6878 - acc: 0.5491
3648/5677 [==================>...........] - ETA: 4:10 - loss: 0.6879 - acc: 0.5488
3712/5677 [==================>...........] - ETA: 4:03 - loss: 0.6877 - acc: 0.5504
3776/5677 [==================>...........] - ETA: 3:55 - loss: 0.6873 - acc: 0.5511
3840/5677 [===================>..........] - ETA: 3:47 - loss: 0.6874 - acc: 0.5503
3904/5677 [===================>..........] - ETA: 3:40 - loss: 0.6873 - acc: 0.5502
3968/5677 [===================>..........] - ETA: 3:32 - loss: 0.6871 - acc: 0.5502
4032/5677 [====================>.........] - ETA: 3:24 - loss: 0.6868 - acc: 0.5506
4096/5677 [====================>.........] - ETA: 3:17 - loss: 0.6867 - acc: 0.5508
4160/5677 [====================>.........] - ETA: 3:08 - loss: 0.6865 - acc: 0.5519
4224/5677 [=====================>........] - ETA: 3:01 - loss: 0.6865 - acc: 0.5516
4288/5677 [=====================>........] - ETA: 2:53 - loss: 0.6868 - acc: 0.5508
4352/5677 [=====================>........] - ETA: 2:45 - loss: 0.6866 - acc: 0.5522
4416/5677 [======================>.......] - ETA: 2:37 - loss: 0.6866 - acc: 0.5519
4480/5677 [======================>.......] - ETA: 2:29 - loss: 0.6865 - acc: 0.5518
4544/5677 [=======================>......] - ETA: 2:21 - loss: 0.6864 - acc: 0.5506
4608/5677 [=======================>......] - ETA: 2:13 - loss: 0.6865 - acc: 0.5503
4672/5677 [=======================>......] - ETA: 2:05 - loss: 0.6858 - acc: 0.5516
4736/5677 [========================>.....] - ETA: 1:57 - loss: 0.6858 - acc: 0.5511
4800/5677 [========================>.....] - ETA: 1:49 - loss: 0.6859 - acc: 0.5506
4864/5677 [========================>.....] - ETA: 1:41 - loss: 0.6858 - acc: 0.5512
4928/5677 [=========================>....] - ETA: 1:34 - loss: 0.6861 - acc: 0.5505
4992/5677 [=========================>....] - ETA: 1:26 - loss: 0.6863 - acc: 0.5497
5056/5677 [=========================>....] - ETA: 1:18 - loss: 0.6858 - acc: 0.5514
5120/5677 [==========================>...] - ETA: 1:10 - loss: 0.6857 - acc: 0.5518
5184/5677 [==========================>...] - ETA: 1:02 - loss: 0.6853 - acc: 0.5525
5248/5677 [==========================>...] - ETA: 53s - loss: 0.6854 - acc: 0.5520 
5312/5677 [===========================>..] - ETA: 45s - loss: 0.6853 - acc: 0.5518
5376/5677 [===========================>..] - ETA: 37s - loss: 0.6851 - acc: 0.5519
5440/5677 [===========================>..] - ETA: 29s - loss: 0.6850 - acc: 0.5522
5504/5677 [============================>.] - ETA: 21s - loss: 0.6851 - acc: 0.5521
5568/5677 [============================>.] - ETA: 13s - loss: 0.6852 - acc: 0.5523
5632/5677 [============================>.] - ETA: 5s - loss: 0.6851 - acc: 0.5526 
5677/5677 [==============================] - 744s 131ms/step - loss: 0.6850 - acc: 0.5529 - val_loss: 0.6914 - val_acc: 0.5452

Epoch 00008: val_acc did not improve from 0.59588
Epoch 9/10

  64/5677 [..............................] - ETA: 13:22 - loss: 0.6647 - acc: 0.6094
 128/5677 [..............................] - ETA: 12:33 - loss: 0.6515 - acc: 0.6328
 192/5677 [>.............................] - ETA: 12:26 - loss: 0.6574 - acc: 0.6198
 256/5677 [>.............................] - ETA: 12:04 - loss: 0.6594 - acc: 0.6055
 320/5677 [>.............................] - ETA: 11:52 - loss: 0.6674 - acc: 0.5969
 384/5677 [=>............................] - ETA: 11:38 - loss: 0.6692 - acc: 0.5964
 448/5677 [=>............................] - ETA: 11:28 - loss: 0.6737 - acc: 0.5893
 512/5677 [=>............................] - ETA: 11:30 - loss: 0.6736 - acc: 0.5938
 576/5677 [==>...........................] - ETA: 11:19 - loss: 0.6688 - acc: 0.6024
 640/5677 [==>...........................] - ETA: 11:14 - loss: 0.6716 - acc: 0.5953
 704/5677 [==>...........................] - ETA: 11:00 - loss: 0.6754 - acc: 0.5881
 768/5677 [===>..........................] - ETA: 10:52 - loss: 0.6760 - acc: 0.5859
 832/5677 [===>..........................] - ETA: 10:40 - loss: 0.6767 - acc: 0.5841
 896/5677 [===>..........................] - ETA: 10:28 - loss: 0.6785 - acc: 0.5804
 960/5677 [====>.........................] - ETA: 10:18 - loss: 0.6766 - acc: 0.5865
1024/5677 [====>.........................] - ETA: 10:10 - loss: 0.6789 - acc: 0.5830
1088/5677 [====>.........................] - ETA: 9:59 - loss: 0.6784 - acc: 0.5818 
1152/5677 [=====>........................] - ETA: 9:50 - loss: 0.6785 - acc: 0.5807
1216/5677 [=====>........................] - ETA: 9:43 - loss: 0.6761 - acc: 0.5872
1280/5677 [=====>........................] - ETA: 9:36 - loss: 0.6780 - acc: 0.5852
1344/5677 [======>.......................] - ETA: 9:28 - loss: 0.6790 - acc: 0.5804
1408/5677 [======>.......................] - ETA: 9:19 - loss: 0.6776 - acc: 0.5845
1472/5677 [======>.......................] - ETA: 9:11 - loss: 0.6783 - acc: 0.5829
1536/5677 [=======>......................] - ETA: 9:02 - loss: 0.6773 - acc: 0.5853
1600/5677 [=======>......................] - ETA: 8:53 - loss: 0.6779 - acc: 0.5813
1664/5677 [=======>......................] - ETA: 8:46 - loss: 0.6782 - acc: 0.5781
1728/5677 [========>.....................] - ETA: 8:37 - loss: 0.6768 - acc: 0.5822
1792/5677 [========>.....................] - ETA: 8:27 - loss: 0.6785 - acc: 0.5781
1856/5677 [========>.....................] - ETA: 8:18 - loss: 0.6784 - acc: 0.5781
1920/5677 [=========>....................] - ETA: 8:10 - loss: 0.6781 - acc: 0.5802
1984/5677 [=========>....................] - ETA: 8:01 - loss: 0.6777 - acc: 0.5801
2048/5677 [=========>....................] - ETA: 7:53 - loss: 0.6773 - acc: 0.5825
2112/5677 [==========>...................] - ETA: 7:46 - loss: 0.6777 - acc: 0.5819
2176/5677 [==========>...................] - ETA: 7:38 - loss: 0.6770 - acc: 0.5841
2240/5677 [==========>...................] - ETA: 7:29 - loss: 0.6777 - acc: 0.5839
2304/5677 [===========>..................] - ETA: 7:21 - loss: 0.6785 - acc: 0.5825
2368/5677 [===========>..................] - ETA: 7:13 - loss: 0.6792 - acc: 0.5807
2432/5677 [===========>..................] - ETA: 7:05 - loss: 0.6802 - acc: 0.5798
2496/5677 [============>.................] - ETA: 6:56 - loss: 0.6794 - acc: 0.5809
2560/5677 [============>.................] - ETA: 6:48 - loss: 0.6788 - acc: 0.5813
2624/5677 [============>.................] - ETA: 6:40 - loss: 0.6786 - acc: 0.5812
2688/5677 [=============>................] - ETA: 6:31 - loss: 0.6772 - acc: 0.5844
2752/5677 [=============>................] - ETA: 6:23 - loss: 0.6769 - acc: 0.5843
2816/5677 [=============>................] - ETA: 6:16 - loss: 0.6776 - acc: 0.5824
2880/5677 [==============>...............] - ETA: 6:07 - loss: 0.6779 - acc: 0.5823
2944/5677 [==============>...............] - ETA: 5:59 - loss: 0.6780 - acc: 0.5815
3008/5677 [==============>...............] - ETA: 5:51 - loss: 0.6778 - acc: 0.5824
3072/5677 [===============>..............] - ETA: 5:42 - loss: 0.6775 - acc: 0.5840
3136/5677 [===============>..............] - ETA: 5:33 - loss: 0.6784 - acc: 0.5823
3200/5677 [===============>..............] - ETA: 5:26 - loss: 0.6787 - acc: 0.5825
3264/5677 [================>.............] - ETA: 5:17 - loss: 0.6787 - acc: 0.5815
3328/5677 [================>.............] - ETA: 5:09 - loss: 0.6782 - acc: 0.5811
3392/5677 [================>.............] - ETA: 5:01 - loss: 0.6776 - acc: 0.5817
3456/5677 [=================>............] - ETA: 4:52 - loss: 0.6777 - acc: 0.5828
3520/5677 [=================>............] - ETA: 4:44 - loss: 0.6776 - acc: 0.5835
3584/5677 [=================>............] - ETA: 4:36 - loss: 0.6772 - acc: 0.5843
3648/5677 [==================>...........] - ETA: 4:28 - loss: 0.6772 - acc: 0.5844
3712/5677 [==================>...........] - ETA: 4:19 - loss: 0.6773 - acc: 0.5841
3776/5677 [==================>...........] - ETA: 4:11 - loss: 0.6780 - acc: 0.5816
3840/5677 [===================>..........] - ETA: 4:02 - loss: 0.6781 - acc: 0.5818
3904/5677 [===================>..........] - ETA: 3:54 - loss: 0.6779 - acc: 0.5817
3968/5677 [===================>..........] - ETA: 3:45 - loss: 0.6779 - acc: 0.5811
4032/5677 [====================>.........] - ETA: 3:37 - loss: 0.6775 - acc: 0.5818
4096/5677 [====================>.........] - ETA: 3:29 - loss: 0.6776 - acc: 0.5823
4160/5677 [====================>.........] - ETA: 3:20 - loss: 0.6774 - acc: 0.5822
4224/5677 [=====================>........] - ETA: 3:12 - loss: 0.6776 - acc: 0.5812
4288/5677 [=====================>........] - ETA: 3:03 - loss: 0.6777 - acc: 0.5807
4352/5677 [=====================>........] - ETA: 2:55 - loss: 0.6776 - acc: 0.5816
4416/5677 [======================>.......] - ETA: 2:46 - loss: 0.6779 - acc: 0.5804
4480/5677 [======================>.......] - ETA: 2:38 - loss: 0.6776 - acc: 0.5813
4544/5677 [=======================>......] - ETA: 2:29 - loss: 0.6772 - acc: 0.5823
4608/5677 [=======================>......] - ETA: 2:21 - loss: 0.6773 - acc: 0.5816
4672/5677 [=======================>......] - ETA: 2:12 - loss: 0.6776 - acc: 0.5805
4736/5677 [========================>.....] - ETA: 2:04 - loss: 0.6773 - acc: 0.5807
4800/5677 [========================>.....] - ETA: 1:55 - loss: 0.6771 - acc: 0.5815
4864/5677 [========================>.....] - ETA: 1:47 - loss: 0.6773 - acc: 0.5806
4928/5677 [=========================>....] - ETA: 1:39 - loss: 0.6781 - acc: 0.5791
4992/5677 [=========================>....] - ETA: 1:30 - loss: 0.6780 - acc: 0.5799
5056/5677 [=========================>....] - ETA: 1:22 - loss: 0.6777 - acc: 0.5807
5120/5677 [==========================>...] - ETA: 1:13 - loss: 0.6775 - acc: 0.5807
5184/5677 [==========================>...] - ETA: 1:05 - loss: 0.6778 - acc: 0.5799
5248/5677 [==========================>...] - ETA: 56s - loss: 0.6781 - acc: 0.5791 
5312/5677 [===========================>..] - ETA: 48s - loss: 0.6780 - acc: 0.5798
5376/5677 [===========================>..] - ETA: 39s - loss: 0.6778 - acc: 0.5794
5440/5677 [===========================>..] - ETA: 31s - loss: 0.6775 - acc: 0.5801
5504/5677 [============================>.] - ETA: 22s - loss: 0.6778 - acc: 0.5796
5568/5677 [============================>.] - ETA: 14s - loss: 0.6779 - acc: 0.5796
5632/5677 [============================>.] - ETA: 5s - loss: 0.6781 - acc: 0.5792 
5677/5677 [==============================] - 782s 138ms/step - loss: 0.6783 - acc: 0.5785 - val_loss: 0.6780 - val_acc: 0.5436

Epoch 00009: val_acc did not improve from 0.59588
Epoch 10/10

  64/5677 [..............................] - ETA: 11:42 - loss: 0.6582 - acc: 0.6094
 128/5677 [..............................] - ETA: 12:04 - loss: 0.6780 - acc: 0.5859
 192/5677 [>.............................] - ETA: 11:46 - loss: 0.6789 - acc: 0.5677
 256/5677 [>.............................] - ETA: 11:37 - loss: 0.6813 - acc: 0.5664
 320/5677 [>.............................] - ETA: 11:26 - loss: 0.6801 - acc: 0.5563
 384/5677 [=>............................] - ETA: 11:25 - loss: 0.6814 - acc: 0.5521
 448/5677 [=>............................] - ETA: 11:14 - loss: 0.6819 - acc: 0.5469
 512/5677 [=>............................] - ETA: 11:12 - loss: 0.6750 - acc: 0.5703
 576/5677 [==>...........................] - ETA: 11:07 - loss: 0.6755 - acc: 0.5694
 640/5677 [==>...........................] - ETA: 10:57 - loss: 0.6716 - acc: 0.5859
 704/5677 [==>...........................] - ETA: 10:48 - loss: 0.6709 - acc: 0.5866
 768/5677 [===>..........................] - ETA: 10:36 - loss: 0.6746 - acc: 0.5794
 832/5677 [===>..........................] - ETA: 10:27 - loss: 0.6744 - acc: 0.5841
 896/5677 [===>..........................] - ETA: 10:30 - loss: 0.6753 - acc: 0.5848
 960/5677 [====>.........................] - ETA: 10:26 - loss: 0.6745 - acc: 0.5906
1024/5677 [====>.........................] - ETA: 10:17 - loss: 0.6745 - acc: 0.5928
1088/5677 [====>.........................] - ETA: 10:08 - loss: 0.6771 - acc: 0.5864
1152/5677 [=====>........................] - ETA: 9:57 - loss: 0.6775 - acc: 0.5842 
1216/5677 [=====>........................] - ETA: 9:47 - loss: 0.6775 - acc: 0.5863
1280/5677 [=====>........................] - ETA: 9:37 - loss: 0.6793 - acc: 0.5820
1344/5677 [======>.......................] - ETA: 9:28 - loss: 0.6785 - acc: 0.5848
1408/5677 [======>.......................] - ETA: 9:19 - loss: 0.6796 - acc: 0.5824
1472/5677 [======>.......................] - ETA: 9:13 - loss: 0.6796 - acc: 0.5822
1536/5677 [=======>......................] - ETA: 9:03 - loss: 0.6780 - acc: 0.5846
1600/5677 [=======>......................] - ETA: 8:56 - loss: 0.6790 - acc: 0.5831
1664/5677 [=======>......................] - ETA: 8:48 - loss: 0.6788 - acc: 0.5823
1728/5677 [========>.....................] - ETA: 8:40 - loss: 0.6789 - acc: 0.5799
1792/5677 [========>.....................] - ETA: 8:34 - loss: 0.6789 - acc: 0.5804
1856/5677 [========>.....................] - ETA: 8:24 - loss: 0.6801 - acc: 0.5781
1920/5677 [=========>....................] - ETA: 8:16 - loss: 0.6818 - acc: 0.5729
1984/5677 [=========>....................] - ETA: 8:06 - loss: 0.6809 - acc: 0.5746
2048/5677 [=========>....................] - ETA: 7:56 - loss: 0.6815 - acc: 0.5732
2112/5677 [==========>...................] - ETA: 7:47 - loss: 0.6819 - acc: 0.5739
2176/5677 [==========>...................] - ETA: 7:38 - loss: 0.6820 - acc: 0.5740
2240/5677 [==========>...................] - ETA: 7:28 - loss: 0.6804 - acc: 0.5777
2304/5677 [===========>..................] - ETA: 7:19 - loss: 0.6805 - acc: 0.5755
2368/5677 [===========>..................] - ETA: 7:12 - loss: 0.6814 - acc: 0.5718
2432/5677 [===========>..................] - ETA: 7:03 - loss: 0.6812 - acc: 0.5720
2496/5677 [============>.................] - ETA: 6:54 - loss: 0.6800 - acc: 0.5757
2560/5677 [============>.................] - ETA: 6:46 - loss: 0.6807 - acc: 0.5742
2624/5677 [============>.................] - ETA: 6:39 - loss: 0.6804 - acc: 0.5747
2688/5677 [=============>................] - ETA: 6:31 - loss: 0.6801 - acc: 0.5763
2752/5677 [=============>................] - ETA: 6:22 - loss: 0.6796 - acc: 0.5781
2816/5677 [=============>................] - ETA: 6:13 - loss: 0.6789 - acc: 0.5788
2880/5677 [==============>...............] - ETA: 6:05 - loss: 0.6788 - acc: 0.5802
2944/5677 [==============>...............] - ETA: 5:57 - loss: 0.6791 - acc: 0.5791
3008/5677 [==============>...............] - ETA: 5:49 - loss: 0.6790 - acc: 0.5798
3072/5677 [===============>..............] - ETA: 5:40 - loss: 0.6790 - acc: 0.5798
3136/5677 [===============>..............] - ETA: 5:31 - loss: 0.6789 - acc: 0.5794
3200/5677 [===============>..............] - ETA: 5:23 - loss: 0.6784 - acc: 0.5797
3264/5677 [================>.............] - ETA: 5:15 - loss: 0.6784 - acc: 0.5806
3328/5677 [================>.............] - ETA: 5:06 - loss: 0.6781 - acc: 0.5802
3392/5677 [================>.............] - ETA: 4:58 - loss: 0.6786 - acc: 0.5793
3456/5677 [=================>............] - ETA: 4:49 - loss: 0.6777 - acc: 0.5802
3520/5677 [=================>............] - ETA: 4:41 - loss: 0.6776 - acc: 0.5798
3584/5677 [=================>............] - ETA: 4:32 - loss: 0.6779 - acc: 0.5781
3648/5677 [==================>...........] - ETA: 4:23 - loss: 0.6778 - acc: 0.5781
3712/5677 [==================>...........] - ETA: 4:15 - loss: 0.6784 - acc: 0.5752
3776/5677 [==================>...........] - ETA: 4:07 - loss: 0.6782 - acc: 0.5755
3840/5677 [===================>..........] - ETA: 3:59 - loss: 0.6785 - acc: 0.5750
3904/5677 [===================>..........] - ETA: 3:50 - loss: 0.6785 - acc: 0.5761
3968/5677 [===================>..........] - ETA: 3:42 - loss: 0.6779 - acc: 0.5766
4032/5677 [====================>.........] - ETA: 3:34 - loss: 0.6778 - acc: 0.5769
4096/5677 [====================>.........] - ETA: 3:25 - loss: 0.6770 - acc: 0.5786
4160/5677 [====================>.........] - ETA: 3:17 - loss: 0.6766 - acc: 0.5796
4224/5677 [=====================>........] - ETA: 3:08 - loss: 0.6769 - acc: 0.5795
4288/5677 [=====================>........] - ETA: 3:00 - loss: 0.6775 - acc: 0.5777
4352/5677 [=====================>........] - ETA: 2:52 - loss: 0.6770 - acc: 0.5784
4416/5677 [======================>.......] - ETA: 2:44 - loss: 0.6775 - acc: 0.5774
4480/5677 [======================>.......] - ETA: 2:35 - loss: 0.6781 - acc: 0.5766
4544/5677 [=======================>......] - ETA: 2:27 - loss: 0.6778 - acc: 0.5764
4608/5677 [=======================>......] - ETA: 2:18 - loss: 0.6774 - acc: 0.5777
4672/5677 [=======================>......] - ETA: 2:10 - loss: 0.6773 - acc: 0.5775
4736/5677 [========================>.....] - ETA: 2:02 - loss: 0.6776 - acc: 0.5773
4800/5677 [========================>.....] - ETA: 1:53 - loss: 0.6778 - acc: 0.5771
4864/5677 [========================>.....] - ETA: 1:45 - loss: 0.6773 - acc: 0.5783
4928/5677 [=========================>....] - ETA: 1:37 - loss: 0.6774 - acc: 0.5785
4992/5677 [=========================>....] - ETA: 1:28 - loss: 0.6773 - acc: 0.5783
5056/5677 [=========================>....] - ETA: 1:20 - loss: 0.6775 - acc: 0.5783
5120/5677 [==========================>...] - ETA: 1:12 - loss: 0.6774 - acc: 0.5773
5184/5677 [==========================>...] - ETA: 1:03 - loss: 0.6774 - acc: 0.5766
5248/5677 [==========================>...] - ETA: 55s - loss: 0.6778 - acc: 0.5751 
5312/5677 [===========================>..] - ETA: 47s - loss: 0.6778 - acc: 0.5747
5376/5677 [===========================>..] - ETA: 38s - loss: 0.6774 - acc: 0.5755
5440/5677 [===========================>..] - ETA: 30s - loss: 0.6774 - acc: 0.5763
5504/5677 [============================>.] - ETA: 22s - loss: 0.6777 - acc: 0.5759
5568/5677 [============================>.] - ETA: 14s - loss: 0.6775 - acc: 0.5767
5632/5677 [============================>.] - ETA: 5s - loss: 0.6778 - acc: 0.5760 
5677/5677 [==============================] - 758s 134ms/step - loss: 0.6777 - acc: 0.5757 - val_loss: 0.6856 - val_acc: 0.5341

Epoch 00010: val_acc did not improve from 0.59588
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f1ee0460f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f1ee0460f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f1ee040bd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f1ee040bd10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1ee021afd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1ee021afd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1eb46f9410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1eb46f9410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1d380b94d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1d380b94d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1eb469c050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1eb469c050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1eb46f98d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1eb46f98d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1eb444dd50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1eb444dd50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1eb46252d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1eb46252d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1eb4505710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1eb4505710>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1eb4552e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1eb4552e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1eb4625190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1eb4625190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1eb44afc10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1eb44afc10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1eb426e650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1eb426e650>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1eb428b890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1eb428b890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1eb41577d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1eb41577d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1eb4483ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1eb4483ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1eb423f6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1eb423f6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1e90695350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1e90695350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1e90614b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1e90614b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1eb415ed50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1eb415ed50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1e906953d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1e906953d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e90589e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e90589e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1e903b4190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1e903b4190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1e903b7e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1e903b7e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e902a7550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e902a7550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1e904b5fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1e904b5fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e90146290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e90146290>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1eb4151d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1eb4151d90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1e7470b290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1e7470b290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e745bde90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e745bde90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1eb4151d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1eb4151d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e74738b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e74738b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1e744cffd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1e744cffd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1e744422d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1e744422d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e74423390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e74423390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1e744cfd10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1e744cfd10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e742cde10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e742cde10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1e743c5fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1e743c5fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1e74091f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1e74091f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e741f4090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e741f4090>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1e743c5190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1e743c5190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e740d0d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e740d0d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1e74126cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1e74126cd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1e6c6d6690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1e6c6d6690>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e6c5a8250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e6c5a8250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1e74085050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1e74085050>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e6c6322d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e6c6322d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1e6c3908d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1e6c3908d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1e6c2d1f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1e6c2d1f10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e6c256610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e6c256610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1e6c390490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1e6c390490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e6c0a5150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e6c0a5150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1e307d8b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1e307d8b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1e307cdbd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1e307cdbd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e307d2710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e307d2710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1e307d8c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1e307d8c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e6c059a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e6c059a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1e307cdf50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1e307cdf50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1e3046fd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1e3046fd90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e303db2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e303db2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1e307cdad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1e307cdad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e3046b8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e3046b8d0>>: AttributeError: module 'gast' has no attribute 'Str'
01window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 6:03
 128/1578 [=>............................] - ETA: 3:32
 192/1578 [==>...........................] - ETA: 2:37
 256/1578 [===>..........................] - ETA: 2:07
 320/1578 [=====>........................] - ETA: 1:51
 384/1578 [======>.......................] - ETA: 1:37
 448/1578 [=======>......................] - ETA: 1:27
 512/1578 [========>.....................] - ETA: 1:17
 576/1578 [=========>....................] - ETA: 1:09
 640/1578 [===========>..................] - ETA: 1:04
 704/1578 [============>.................] - ETA: 58s 
 768/1578 [=============>................] - ETA: 51s
 832/1578 [==============>...............] - ETA: 46s
 896/1578 [================>.............] - ETA: 41s
 960/1578 [=================>............] - ETA: 36s
1024/1578 [==================>...........] - ETA: 32s
1088/1578 [===================>..........] - ETA: 28s
1152/1578 [====================>.........] - ETA: 24s
1216/1578 [======================>.......] - ETA: 20s
1280/1578 [=======================>......] - ETA: 16s
1344/1578 [========================>.....] - ETA: 13s
1408/1578 [=========================>....] - ETA: 9s 
1472/1578 [==========================>...] - ETA: 5s
1536/1578 [============================>.] - ETA: 2s
1578/1578 [==============================] - 86s 54ms/step
loss: 0.681440585315303
acc: 0.5671736381957437
样本个数 3154
样本个数 6308
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f1b3042ad10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f1b3042ad10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f1ad0057290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f1ad0057290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24df37be90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f24df37be90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1df00dd990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1df00dd990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1ee013ad10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1ee013ad10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1df00d2350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1df00d2350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1df00dd850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1df00dd850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e301dfa50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e301dfa50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1ee0329bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1ee0329bd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1b303d3e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1b303d3e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1b30388810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1b30388810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1ee0296fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1ee0296fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1b302f8ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1b302f8ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1ee020bc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1ee020bc90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1b301b2b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1b301b2b10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1b301a4350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1b301a4350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1b30298650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1b30298650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1b3008b910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1b3008b910>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1b10646950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1b10646950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1b1051f350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1b1051f350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1b1041cb10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1b1041cb10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1b3010f810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1b3010f810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1b3045cf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1b3045cf90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1b30470d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1b30470d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1b102b56d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1b102b56d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e304732d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e304732d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1b30470bd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1b30470bd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1b10243ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1b10243ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1b10078d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1b10078d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1b10061750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1b10061750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1b10053f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1b10053f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1b10078c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1b10078c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1af06e8250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1af06e8250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1b1056c390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1b1056c390>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1af03f0c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1af03f0c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1af03bf690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1af03bf690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1af05cc150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1af05cc150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1af0433fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1af0433fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1af0297a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1af0297a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1af00c1b90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1af00c1b90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1af01f7cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1af01f7cd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1af02c7fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1af02c7fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1ad06f2f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1ad06f2f90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1af01620d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1af01620d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1ad064dcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1ad064dcd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1ad05c9110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1ad05c9110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1ad065bd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1ad065bd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1b30186e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1b30186e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1ad0383290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1ad0383290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1ad034c210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1ad034c210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1ad05c2fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1ad05c2fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1ad056d410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1ad056d410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1ad025fa90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1ad025fa90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f19f40530d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f19f40530d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f19d06fb8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f19d06fb8d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f19d0728390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f19d0728390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f19f4053ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f19f4053ed0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f19d0627350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f19d0627350>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f19d050ba90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f19d050ba90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f19d047cc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f19d047cc50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f19d0707490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f19d0707490>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f19d0787850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f19d0787850>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f19d0527050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f19d0527050>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 1:51:46 - loss: 0.7358 - acc: 0.5000
 128/5677 [..............................] - ETA: 1:02:45 - loss: 0.7500 - acc: 0.5000
 192/5677 [>.............................] - ETA: 46:15 - loss: 0.7407 - acc: 0.5260  
 256/5677 [>.............................] - ETA: 37:50 - loss: 0.7465 - acc: 0.5078
 320/5677 [>.............................] - ETA: 32:52 - loss: 0.7561 - acc: 0.5094
 384/5677 [=>............................] - ETA: 29:18 - loss: 0.7477 - acc: 0.5182
 448/5677 [=>............................] - ETA: 26:46 - loss: 0.7414 - acc: 0.5179
 512/5677 [=>............................] - ETA: 24:52 - loss: 0.7352 - acc: 0.5234
 576/5677 [==>...........................] - ETA: 23:14 - loss: 0.7305 - acc: 0.5260
 640/5677 [==>...........................] - ETA: 22:04 - loss: 0.7273 - acc: 0.5344
 704/5677 [==>...........................] - ETA: 21:00 - loss: 0.7269 - acc: 0.5355
 768/5677 [===>..........................] - ETA: 20:06 - loss: 0.7208 - acc: 0.5430
 832/5677 [===>..........................] - ETA: 19:13 - loss: 0.7233 - acc: 0.5409
 896/5677 [===>..........................] - ETA: 18:28 - loss: 0.7278 - acc: 0.5368
 960/5677 [====>.........................] - ETA: 17:45 - loss: 0.7275 - acc: 0.5354
1024/5677 [====>.........................] - ETA: 17:08 - loss: 0.7262 - acc: 0.5332
1088/5677 [====>.........................] - ETA: 16:36 - loss: 0.7223 - acc: 0.5331
1152/5677 [=====>........................] - ETA: 16:09 - loss: 0.7197 - acc: 0.5391
1216/5677 [=====>........................] - ETA: 15:44 - loss: 0.7220 - acc: 0.5337
1280/5677 [=====>........................] - ETA: 15:16 - loss: 0.7227 - acc: 0.5297
1344/5677 [======>.......................] - ETA: 14:52 - loss: 0.7261 - acc: 0.5238
1408/5677 [======>.......................] - ETA: 14:30 - loss: 0.7261 - acc: 0.5213
1472/5677 [======>.......................] - ETA: 14:08 - loss: 0.7252 - acc: 0.5217
1536/5677 [=======>......................] - ETA: 13:48 - loss: 0.7246 - acc: 0.5241
1600/5677 [=======>......................] - ETA: 13:27 - loss: 0.7247 - acc: 0.5212
1664/5677 [=======>......................] - ETA: 13:07 - loss: 0.7238 - acc: 0.5204
1728/5677 [========>.....................] - ETA: 12:49 - loss: 0.7234 - acc: 0.5208
1792/5677 [========>.....................] - ETA: 12:31 - loss: 0.7236 - acc: 0.5184
1856/5677 [========>.....................] - ETA: 12:13 - loss: 0.7230 - acc: 0.5178
1920/5677 [=========>....................] - ETA: 11:55 - loss: 0.7217 - acc: 0.5198
1984/5677 [=========>....................] - ETA: 11:39 - loss: 0.7210 - acc: 0.5202
2048/5677 [=========>....................] - ETA: 11:23 - loss: 0.7204 - acc: 0.5215
2112/5677 [==========>...................] - ETA: 11:08 - loss: 0.7209 - acc: 0.5208
2176/5677 [==========>...................] - ETA: 10:52 - loss: 0.7196 - acc: 0.5221
2240/5677 [==========>...................] - ETA: 10:37 - loss: 0.7196 - acc: 0.5201
2304/5677 [===========>..................] - ETA: 10:21 - loss: 0.7207 - acc: 0.5182
2368/5677 [===========>..................] - ETA: 10:07 - loss: 0.7213 - acc: 0.5156
2432/5677 [===========>..................] - ETA: 9:54 - loss: 0.7212 - acc: 0.5144 
2496/5677 [============>.................] - ETA: 9:40 - loss: 0.7212 - acc: 0.5128
2560/5677 [============>.................] - ETA: 9:27 - loss: 0.7213 - acc: 0.5117
2624/5677 [============>.................] - ETA: 9:14 - loss: 0.7212 - acc: 0.5114
2688/5677 [=============>................] - ETA: 9:01 - loss: 0.7216 - acc: 0.5119
2752/5677 [=============>................] - ETA: 8:48 - loss: 0.7218 - acc: 0.5131
2816/5677 [=============>................] - ETA: 8:36 - loss: 0.7205 - acc: 0.5146
2880/5677 [==============>...............] - ETA: 8:23 - loss: 0.7205 - acc: 0.5156
2944/5677 [==============>...............] - ETA: 8:10 - loss: 0.7196 - acc: 0.5166
3008/5677 [==============>...............] - ETA: 7:57 - loss: 0.7184 - acc: 0.5180
3072/5677 [===============>..............] - ETA: 7:45 - loss: 0.7180 - acc: 0.5176
3136/5677 [===============>..............] - ETA: 7:33 - loss: 0.7183 - acc: 0.5166
3200/5677 [===============>..............] - ETA: 7:20 - loss: 0.7188 - acc: 0.5153
3264/5677 [================>.............] - ETA: 7:08 - loss: 0.7186 - acc: 0.5156
3328/5677 [================>.............] - ETA: 6:56 - loss: 0.7173 - acc: 0.5189
3392/5677 [================>.............] - ETA: 6:44 - loss: 0.7172 - acc: 0.5183
3456/5677 [=================>............] - ETA: 6:31 - loss: 0.7180 - acc: 0.5156
3520/5677 [=================>............] - ETA: 6:18 - loss: 0.7181 - acc: 0.5142
3584/5677 [=================>............] - ETA: 6:06 - loss: 0.7177 - acc: 0.5148
3648/5677 [==================>...........] - ETA: 5:55 - loss: 0.7174 - acc: 0.5148
3712/5677 [==================>...........] - ETA: 5:43 - loss: 0.7169 - acc: 0.5159
3776/5677 [==================>...........] - ETA: 5:31 - loss: 0.7170 - acc: 0.5156
3840/5677 [===================>..........] - ETA: 5:19 - loss: 0.7165 - acc: 0.5174
3904/5677 [===================>..........] - ETA: 5:08 - loss: 0.7166 - acc: 0.5172
3968/5677 [===================>..........] - ETA: 4:56 - loss: 0.7164 - acc: 0.5171
4032/5677 [====================>.........] - ETA: 4:45 - loss: 0.7165 - acc: 0.5169
4096/5677 [====================>.........] - ETA: 4:33 - loss: 0.7162 - acc: 0.5176
4160/5677 [====================>.........] - ETA: 4:22 - loss: 0.7163 - acc: 0.5171
4224/5677 [=====================>........] - ETA: 4:10 - loss: 0.7169 - acc: 0.5159
4288/5677 [=====================>........] - ETA: 3:59 - loss: 0.7168 - acc: 0.5161
4352/5677 [=====================>........] - ETA: 3:48 - loss: 0.7167 - acc: 0.5156
4416/5677 [======================>.......] - ETA: 3:36 - loss: 0.7159 - acc: 0.5165
4480/5677 [======================>.......] - ETA: 3:25 - loss: 0.7157 - acc: 0.5163
4544/5677 [=======================>......] - ETA: 3:14 - loss: 0.7155 - acc: 0.5163
4608/5677 [=======================>......] - ETA: 3:03 - loss: 0.7153 - acc: 0.5154
4672/5677 [=======================>......] - ETA: 2:52 - loss: 0.7149 - acc: 0.5158
4736/5677 [========================>.....] - ETA: 2:41 - loss: 0.7148 - acc: 0.5150
4800/5677 [========================>.....] - ETA: 2:30 - loss: 0.7141 - acc: 0.5162
4864/5677 [========================>.....] - ETA: 2:18 - loss: 0.7140 - acc: 0.5150
4928/5677 [=========================>....] - ETA: 2:07 - loss: 0.7137 - acc: 0.5154
4992/5677 [=========================>....] - ETA: 1:56 - loss: 0.7134 - acc: 0.5158
5056/5677 [=========================>....] - ETA: 1:45 - loss: 0.7132 - acc: 0.5160
5120/5677 [==========================>...] - ETA: 1:34 - loss: 0.7136 - acc: 0.5156
5184/5677 [==========================>...] - ETA: 1:23 - loss: 0.7134 - acc: 0.5152
5248/5677 [==========================>...] - ETA: 1:12 - loss: 0.7132 - acc: 0.5152
5312/5677 [===========================>..] - ETA: 1:01 - loss: 0.7127 - acc: 0.5156
5376/5677 [===========================>..] - ETA: 50s - loss: 0.7127 - acc: 0.5158 
5440/5677 [===========================>..] - ETA: 40s - loss: 0.7123 - acc: 0.5156
5504/5677 [============================>.] - ETA: 29s - loss: 0.7121 - acc: 0.5156
5568/5677 [============================>.] - ETA: 18s - loss: 0.7116 - acc: 0.5169
5632/5677 [============================>.] - ETA: 7s - loss: 0.7117 - acc: 0.5167 
5677/5677 [==============================] - 1000s 176ms/step - loss: 0.7115 - acc: 0.5166 - val_loss: 0.6823 - val_acc: 0.5436

Epoch 00001: val_acc improved from -inf to 0.54358, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window04/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 16:09 - loss: 0.6946 - acc: 0.5312
 128/5677 [..............................] - ETA: 15:16 - loss: 0.6811 - acc: 0.5781
 192/5677 [>.............................] - ETA: 15:15 - loss: 0.6854 - acc: 0.5625
 256/5677 [>.............................] - ETA: 15:05 - loss: 0.6922 - acc: 0.5312
 320/5677 [>.............................] - ETA: 14:47 - loss: 0.6935 - acc: 0.5219
 384/5677 [=>............................] - ETA: 14:23 - loss: 0.6936 - acc: 0.5182
 448/5677 [=>............................] - ETA: 14:13 - loss: 0.6964 - acc: 0.5112
 512/5677 [=>............................] - ETA: 13:59 - loss: 0.7011 - acc: 0.5020
 576/5677 [==>...........................] - ETA: 13:36 - loss: 0.7021 - acc: 0.4948
 640/5677 [==>...........................] - ETA: 13:18 - loss: 0.6999 - acc: 0.5047
 704/5677 [==>...........................] - ETA: 13:03 - loss: 0.7091 - acc: 0.4929
 768/5677 [===>..........................] - ETA: 12:54 - loss: 0.7083 - acc: 0.4974
 832/5677 [===>..........................] - ETA: 12:38 - loss: 0.7042 - acc: 0.5036
 896/5677 [===>..........................] - ETA: 12:32 - loss: 0.7066 - acc: 0.4978
 960/5677 [====>.........................] - ETA: 12:22 - loss: 0.7054 - acc: 0.5010
1024/5677 [====>.........................] - ETA: 12:16 - loss: 0.7055 - acc: 0.5010
1088/5677 [====>.........................] - ETA: 12:07 - loss: 0.7052 - acc: 0.4982
1152/5677 [=====>........................] - ETA: 11:54 - loss: 0.7059 - acc: 0.4931
1216/5677 [=====>........................] - ETA: 11:45 - loss: 0.7064 - acc: 0.4959
1280/5677 [=====>........................] - ETA: 11:34 - loss: 0.7058 - acc: 0.4977
1344/5677 [======>.......................] - ETA: 11:23 - loss: 0.7062 - acc: 0.4985
1408/5677 [======>.......................] - ETA: 11:12 - loss: 0.7045 - acc: 0.5036
1472/5677 [======>.......................] - ETA: 11:01 - loss: 0.7037 - acc: 0.5041
1536/5677 [=======>......................] - ETA: 10:50 - loss: 0.7033 - acc: 0.5072
1600/5677 [=======>......................] - ETA: 10:40 - loss: 0.7031 - acc: 0.5069
1664/5677 [=======>......................] - ETA: 10:28 - loss: 0.7031 - acc: 0.5072
1728/5677 [========>.....................] - ETA: 10:17 - loss: 0.7025 - acc: 0.5098
1792/5677 [========>.....................] - ETA: 10:06 - loss: 0.7025 - acc: 0.5117
1856/5677 [========>.....................] - ETA: 9:56 - loss: 0.7025 - acc: 0.5097 
1920/5677 [=========>....................] - ETA: 9:46 - loss: 0.7033 - acc: 0.5073
1984/5677 [=========>....................] - ETA: 9:35 - loss: 0.7035 - acc: 0.5050
2048/5677 [=========>....................] - ETA: 9:24 - loss: 0.7035 - acc: 0.5034
2112/5677 [==========>...................] - ETA: 9:11 - loss: 0.7034 - acc: 0.5033
2176/5677 [==========>...................] - ETA: 9:01 - loss: 0.7028 - acc: 0.5046
2240/5677 [==========>...................] - ETA: 8:48 - loss: 0.7029 - acc: 0.5049
2304/5677 [===========>..................] - ETA: 8:38 - loss: 0.7031 - acc: 0.5048
2368/5677 [===========>..................] - ETA: 8:28 - loss: 0.7022 - acc: 0.5055
2432/5677 [===========>..................] - ETA: 8:15 - loss: 0.7017 - acc: 0.5058
2496/5677 [============>.................] - ETA: 8:04 - loss: 0.7027 - acc: 0.5032
2560/5677 [============>.................] - ETA: 7:53 - loss: 0.7022 - acc: 0.5039
2624/5677 [============>.................] - ETA: 7:41 - loss: 0.7017 - acc: 0.5050
2688/5677 [=============>................] - ETA: 7:30 - loss: 0.7011 - acc: 0.5067
2752/5677 [=============>................] - ETA: 7:19 - loss: 0.7011 - acc: 0.5062
2816/5677 [=============>................] - ETA: 7:08 - loss: 0.7015 - acc: 0.5057
2880/5677 [==============>...............] - ETA: 6:57 - loss: 0.7018 - acc: 0.5052
2944/5677 [==============>...............] - ETA: 6:47 - loss: 0.7010 - acc: 0.5078
3008/5677 [==============>...............] - ETA: 6:37 - loss: 0.7013 - acc: 0.5076
3072/5677 [===============>..............] - ETA: 6:26 - loss: 0.7012 - acc: 0.5081
3136/5677 [===============>..............] - ETA: 6:18 - loss: 0.7006 - acc: 0.5096
3200/5677 [===============>..............] - ETA: 6:08 - loss: 0.7001 - acc: 0.5119
3264/5677 [================>.............] - ETA: 5:58 - loss: 0.7003 - acc: 0.5123
3328/5677 [================>.............] - ETA: 5:48 - loss: 0.7008 - acc: 0.5120
3392/5677 [================>.............] - ETA: 5:38 - loss: 0.7001 - acc: 0.5136
3456/5677 [=================>............] - ETA: 5:28 - loss: 0.7002 - acc: 0.5133
3520/5677 [=================>............] - ETA: 5:18 - loss: 0.7004 - acc: 0.5128
3584/5677 [=================>............] - ETA: 5:08 - loss: 0.7006 - acc: 0.5128
3648/5677 [==================>...........] - ETA: 4:59 - loss: 0.7007 - acc: 0.5129
3712/5677 [==================>...........] - ETA: 4:49 - loss: 0.6997 - acc: 0.5156
3776/5677 [==================>...........] - ETA: 4:40 - loss: 0.7002 - acc: 0.5146
3840/5677 [===================>..........] - ETA: 4:30 - loss: 0.7007 - acc: 0.5141
3904/5677 [===================>..........] - ETA: 4:20 - loss: 0.7002 - acc: 0.5143
3968/5677 [===================>..........] - ETA: 4:10 - loss: 0.7000 - acc: 0.5151
4032/5677 [====================>.........] - ETA: 4:01 - loss: 0.7003 - acc: 0.5139
4096/5677 [====================>.........] - ETA: 3:51 - loss: 0.6999 - acc: 0.5144
4160/5677 [====================>.........] - ETA: 3:42 - loss: 0.6997 - acc: 0.5159
4224/5677 [=====================>........] - ETA: 3:32 - loss: 0.7000 - acc: 0.5149
4288/5677 [=====================>........] - ETA: 3:23 - loss: 0.6997 - acc: 0.5147
4352/5677 [=====================>........] - ETA: 3:13 - loss: 0.6999 - acc: 0.5142
4416/5677 [======================>.......] - ETA: 3:04 - loss: 0.7002 - acc: 0.5136
4480/5677 [======================>.......] - ETA: 2:54 - loss: 0.7002 - acc: 0.5138
4544/5677 [=======================>......] - ETA: 2:45 - loss: 0.7002 - acc: 0.5136
4608/5677 [=======================>......] - ETA: 2:35 - loss: 0.7005 - acc: 0.5132
4672/5677 [=======================>......] - ETA: 2:26 - loss: 0.7004 - acc: 0.5139
4736/5677 [========================>.....] - ETA: 2:17 - loss: 0.7004 - acc: 0.5133
4800/5677 [========================>.....] - ETA: 2:07 - loss: 0.7005 - acc: 0.5142
4864/5677 [========================>.....] - ETA: 1:58 - loss: 0.7007 - acc: 0.5134
4928/5677 [=========================>....] - ETA: 1:49 - loss: 0.7010 - acc: 0.5118
4992/5677 [=========================>....] - ETA: 1:39 - loss: 0.7006 - acc: 0.5124
5056/5677 [=========================>....] - ETA: 1:30 - loss: 0.7006 - acc: 0.5117
5120/5677 [==========================>...] - ETA: 1:21 - loss: 0.7005 - acc: 0.5123
5184/5677 [==========================>...] - ETA: 1:11 - loss: 0.7007 - acc: 0.5118
5248/5677 [==========================>...] - ETA: 1:02 - loss: 0.7006 - acc: 0.5126
5312/5677 [===========================>..] - ETA: 53s - loss: 0.7008 - acc: 0.5132 
5376/5677 [===========================>..] - ETA: 43s - loss: 0.7004 - acc: 0.5141
5440/5677 [===========================>..] - ETA: 34s - loss: 0.7007 - acc: 0.5131
5504/5677 [============================>.] - ETA: 25s - loss: 0.7006 - acc: 0.5140
5568/5677 [============================>.] - ETA: 15s - loss: 0.7004 - acc: 0.5153
5632/5677 [============================>.] - ETA: 6s - loss: 0.7004 - acc: 0.5149 
5677/5677 [==============================] - 856s 151ms/step - loss: 0.7003 - acc: 0.5151 - val_loss: 0.6843 - val_acc: 0.5468

Epoch 00002: val_acc improved from 0.54358 to 0.54675, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window04/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 3/10

  64/5677 [..............................] - ETA: 12:48 - loss: 0.7414 - acc: 0.4375
 128/5677 [..............................] - ETA: 12:37 - loss: 0.7183 - acc: 0.5000
 192/5677 [>.............................] - ETA: 12:33 - loss: 0.7187 - acc: 0.5052
 256/5677 [>.............................] - ETA: 12:35 - loss: 0.7174 - acc: 0.5156
 320/5677 [>.............................] - ETA: 12:31 - loss: 0.7128 - acc: 0.5125
 384/5677 [=>............................] - ETA: 12:12 - loss: 0.7101 - acc: 0.5234
 448/5677 [=>............................] - ETA: 12:11 - loss: 0.7070 - acc: 0.5179
 512/5677 [=>............................] - ETA: 12:03 - loss: 0.7028 - acc: 0.5273
 576/5677 [==>...........................] - ETA: 11:53 - loss: 0.7019 - acc: 0.5312
 640/5677 [==>...........................] - ETA: 11:45 - loss: 0.6973 - acc: 0.5359
 704/5677 [==>...........................] - ETA: 11:37 - loss: 0.6971 - acc: 0.5312
 768/5677 [===>..........................] - ETA: 11:24 - loss: 0.6957 - acc: 0.5339
 832/5677 [===>..........................] - ETA: 11:13 - loss: 0.6968 - acc: 0.5312
 896/5677 [===>..........................] - ETA: 11:02 - loss: 0.6976 - acc: 0.5324
 960/5677 [====>.........................] - ETA: 10:55 - loss: 0.6940 - acc: 0.5385
1024/5677 [====>.........................] - ETA: 10:47 - loss: 0.6950 - acc: 0.5381
1088/5677 [====>.........................] - ETA: 10:38 - loss: 0.6940 - acc: 0.5377
1152/5677 [=====>........................] - ETA: 10:28 - loss: 0.6942 - acc: 0.5373
1216/5677 [=====>........................] - ETA: 10:20 - loss: 0.6949 - acc: 0.5354
1280/5677 [=====>........................] - ETA: 10:13 - loss: 0.6955 - acc: 0.5352
1344/5677 [======>.......................] - ETA: 10:03 - loss: 0.6962 - acc: 0.5342
1408/5677 [======>.......................] - ETA: 9:57 - loss: 0.6970 - acc: 0.5348 
1472/5677 [======>.......................] - ETA: 9:50 - loss: 0.6980 - acc: 0.5333
1536/5677 [=======>......................] - ETA: 9:43 - loss: 0.6999 - acc: 0.5280
1600/5677 [=======>......................] - ETA: 9:36 - loss: 0.6986 - acc: 0.5294
1664/5677 [=======>......................] - ETA: 9:29 - loss: 0.6991 - acc: 0.5276
1728/5677 [========>.....................] - ETA: 9:20 - loss: 0.6972 - acc: 0.5312
1792/5677 [========>.....................] - ETA: 9:10 - loss: 0.6967 - acc: 0.5312
1856/5677 [========>.....................] - ETA: 9:01 - loss: 0.6956 - acc: 0.5334
1920/5677 [=========>....................] - ETA: 8:52 - loss: 0.6946 - acc: 0.5349
1984/5677 [=========>....................] - ETA: 8:44 - loss: 0.6954 - acc: 0.5323
2048/5677 [=========>....................] - ETA: 8:36 - loss: 0.6954 - acc: 0.5332
2112/5677 [==========>...................] - ETA: 8:27 - loss: 0.6956 - acc: 0.5341
2176/5677 [==========>...................] - ETA: 8:18 - loss: 0.6955 - acc: 0.5335
2240/5677 [==========>...................] - ETA: 8:07 - loss: 0.6951 - acc: 0.5326
2304/5677 [===========>..................] - ETA: 7:57 - loss: 0.6954 - acc: 0.5321
2368/5677 [===========>..................] - ETA: 7:48 - loss: 0.6951 - acc: 0.5325
2432/5677 [===========>..................] - ETA: 7:39 - loss: 0.6954 - acc: 0.5312
2496/5677 [============>.................] - ETA: 7:29 - loss: 0.6961 - acc: 0.5304
2560/5677 [============>.................] - ETA: 7:21 - loss: 0.6957 - acc: 0.5309
2624/5677 [============>.................] - ETA: 7:11 - loss: 0.6970 - acc: 0.5274
2688/5677 [=============>................] - ETA: 7:03 - loss: 0.6967 - acc: 0.5286
2752/5677 [=============>................] - ETA: 6:56 - loss: 0.6979 - acc: 0.5251
2816/5677 [=============>................] - ETA: 6:46 - loss: 0.6979 - acc: 0.5256
2880/5677 [==============>...............] - ETA: 6:36 - loss: 0.6980 - acc: 0.5247
2944/5677 [==============>...............] - ETA: 6:27 - loss: 0.6978 - acc: 0.5238
3008/5677 [==============>...............] - ETA: 6:17 - loss: 0.6975 - acc: 0.5229
3072/5677 [===============>..............] - ETA: 6:08 - loss: 0.6972 - acc: 0.5244
3136/5677 [===============>..............] - ETA: 5:59 - loss: 0.6968 - acc: 0.5249
3200/5677 [===============>..............] - ETA: 5:50 - loss: 0.6974 - acc: 0.5231
3264/5677 [================>.............] - ETA: 5:41 - loss: 0.6969 - acc: 0.5242
3328/5677 [================>.............] - ETA: 5:32 - loss: 0.6970 - acc: 0.5237
3392/5677 [================>.............] - ETA: 5:23 - loss: 0.6970 - acc: 0.5245
3456/5677 [=================>............] - ETA: 5:13 - loss: 0.6971 - acc: 0.5229
3520/5677 [=================>............] - ETA: 5:04 - loss: 0.6971 - acc: 0.5219
3584/5677 [=================>............] - ETA: 4:55 - loss: 0.6970 - acc: 0.5220
3648/5677 [==================>...........] - ETA: 4:47 - loss: 0.6966 - acc: 0.5222
3712/5677 [==================>...........] - ETA: 4:38 - loss: 0.6962 - acc: 0.5226
3776/5677 [==================>...........] - ETA: 4:29 - loss: 0.6962 - acc: 0.5238
3840/5677 [===================>..........] - ETA: 4:20 - loss: 0.6965 - acc: 0.5221
3904/5677 [===================>..........] - ETA: 4:11 - loss: 0.6965 - acc: 0.5220
3968/5677 [===================>..........] - ETA: 4:02 - loss: 0.6964 - acc: 0.5227
4032/5677 [====================>.........] - ETA: 3:53 - loss: 0.6961 - acc: 0.5226
4096/5677 [====================>.........] - ETA: 3:43 - loss: 0.6957 - acc: 0.5225
4160/5677 [====================>.........] - ETA: 3:34 - loss: 0.6958 - acc: 0.5224
4224/5677 [=====================>........] - ETA: 3:25 - loss: 0.6966 - acc: 0.5204
4288/5677 [=====================>........] - ETA: 3:16 - loss: 0.6961 - acc: 0.5215
4352/5677 [=====================>........] - ETA: 3:07 - loss: 0.6965 - acc: 0.5209
4416/5677 [======================>.......] - ETA: 2:58 - loss: 0.6964 - acc: 0.5213
4480/5677 [======================>.......] - ETA: 2:49 - loss: 0.6962 - acc: 0.5214
4544/5677 [=======================>......] - ETA: 2:40 - loss: 0.6965 - acc: 0.5207
4608/5677 [=======================>......] - ETA: 2:30 - loss: 0.6961 - acc: 0.5213
4672/5677 [=======================>......] - ETA: 2:21 - loss: 0.6958 - acc: 0.5225
4736/5677 [========================>.....] - ETA: 2:12 - loss: 0.6958 - acc: 0.5224
4800/5677 [========================>.....] - ETA: 2:03 - loss: 0.6957 - acc: 0.5223
4864/5677 [========================>.....] - ETA: 1:54 - loss: 0.6959 - acc: 0.5208
4928/5677 [=========================>....] - ETA: 1:45 - loss: 0.6956 - acc: 0.5217
4992/5677 [=========================>....] - ETA: 1:36 - loss: 0.6958 - acc: 0.5216
5056/5677 [=========================>....] - ETA: 1:27 - loss: 0.6957 - acc: 0.5218
5120/5677 [==========================>...] - ETA: 1:18 - loss: 0.6957 - acc: 0.5217
5184/5677 [==========================>...] - ETA: 1:09 - loss: 0.6957 - acc: 0.5206
5248/5677 [==========================>...] - ETA: 1:00 - loss: 0.6956 - acc: 0.5215
5312/5677 [===========================>..] - ETA: 51s - loss: 0.6956 - acc: 0.5218 
5376/5677 [===========================>..] - ETA: 42s - loss: 0.6956 - acc: 0.5223
5440/5677 [===========================>..] - ETA: 33s - loss: 0.6957 - acc: 0.5217
5504/5677 [============================>.] - ETA: 24s - loss: 0.6958 - acc: 0.5211
5568/5677 [============================>.] - ETA: 15s - loss: 0.6957 - acc: 0.5221
5632/5677 [============================>.] - ETA: 6s - loss: 0.6957 - acc: 0.5225 
5677/5677 [==============================] - 840s 148ms/step - loss: 0.6958 - acc: 0.5226 - val_loss: 0.6787 - val_acc: 0.5832

Epoch 00003: val_acc improved from 0.54675 to 0.58320, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window04/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 4/10

  64/5677 [..............................] - ETA: 12:59 - loss: 0.6887 - acc: 0.5156
 128/5677 [..............................] - ETA: 12:24 - loss: 0.6859 - acc: 0.5312
 192/5677 [>.............................] - ETA: 12:21 - loss: 0.6744 - acc: 0.5729
 256/5677 [>.............................] - ETA: 12:19 - loss: 0.6753 - acc: 0.5586
 320/5677 [>.............................] - ETA: 12:08 - loss: 0.6791 - acc: 0.5531
 384/5677 [=>............................] - ETA: 12:05 - loss: 0.6799 - acc: 0.5495
 448/5677 [=>............................] - ETA: 11:56 - loss: 0.6796 - acc: 0.5580
 512/5677 [=>............................] - ETA: 11:45 - loss: 0.6895 - acc: 0.5410
 576/5677 [==>...........................] - ETA: 11:36 - loss: 0.6939 - acc: 0.5243
 640/5677 [==>...........................] - ETA: 11:28 - loss: 0.6960 - acc: 0.5172
 704/5677 [==>...........................] - ETA: 11:17 - loss: 0.6942 - acc: 0.5256
 768/5677 [===>..........................] - ETA: 11:11 - loss: 0.6958 - acc: 0.5195
 832/5677 [===>..........................] - ETA: 11:03 - loss: 0.6957 - acc: 0.5240
 896/5677 [===>..........................] - ETA: 10:53 - loss: 0.6957 - acc: 0.5246
 960/5677 [====>.........................] - ETA: 10:43 - loss: 0.6951 - acc: 0.5260
1024/5677 [====>.........................] - ETA: 10:30 - loss: 0.6930 - acc: 0.5322
1088/5677 [====>.........................] - ETA: 10:22 - loss: 0.6918 - acc: 0.5368
1152/5677 [=====>........................] - ETA: 10:11 - loss: 0.6918 - acc: 0.5373
1216/5677 [=====>........................] - ETA: 10:03 - loss: 0.6936 - acc: 0.5329
1280/5677 [=====>........................] - ETA: 9:53 - loss: 0.6937 - acc: 0.5297 
1344/5677 [======>.......................] - ETA: 9:49 - loss: 0.6916 - acc: 0.5327
1408/5677 [======>.......................] - ETA: 9:47 - loss: 0.6894 - acc: 0.5369
1472/5677 [======>.......................] - ETA: 9:39 - loss: 0.6896 - acc: 0.5353
1536/5677 [=======>......................] - ETA: 9:30 - loss: 0.6888 - acc: 0.5371
1600/5677 [=======>......................] - ETA: 9:20 - loss: 0.6888 - acc: 0.5400
1664/5677 [=======>......................] - ETA: 9:13 - loss: 0.6879 - acc: 0.5433
1728/5677 [========>.....................] - ETA: 9:06 - loss: 0.6882 - acc: 0.5422
1792/5677 [========>.....................] - ETA: 8:57 - loss: 0.6892 - acc: 0.5396
1856/5677 [========>.....................] - ETA: 8:50 - loss: 0.6899 - acc: 0.5415
1920/5677 [=========>....................] - ETA: 8:42 - loss: 0.6901 - acc: 0.5422
1984/5677 [=========>....................] - ETA: 8:36 - loss: 0.6883 - acc: 0.5459
2048/5677 [=========>....................] - ETA: 8:29 - loss: 0.6873 - acc: 0.5488
2112/5677 [==========>...................] - ETA: 8:23 - loss: 0.6874 - acc: 0.5483
2176/5677 [==========>...................] - ETA: 8:16 - loss: 0.6887 - acc: 0.5460
2240/5677 [==========>...................] - ETA: 8:08 - loss: 0.6893 - acc: 0.5451
2304/5677 [===========>..................] - ETA: 8:00 - loss: 0.6889 - acc: 0.5447
2368/5677 [===========>..................] - ETA: 7:51 - loss: 0.6898 - acc: 0.5435
2432/5677 [===========>..................] - ETA: 7:42 - loss: 0.6897 - acc: 0.5424
2496/5677 [============>.................] - ETA: 7:33 - loss: 0.6901 - acc: 0.5405
2560/5677 [============>.................] - ETA: 7:23 - loss: 0.6916 - acc: 0.5352
2624/5677 [============>.................] - ETA: 7:16 - loss: 0.6915 - acc: 0.5351
2688/5677 [=============>................] - ETA: 7:05 - loss: 0.6916 - acc: 0.5346
2752/5677 [=============>................] - ETA: 6:56 - loss: 0.6912 - acc: 0.5352
2816/5677 [=============>................] - ETA: 6:46 - loss: 0.6913 - acc: 0.5352
2880/5677 [==============>...............] - ETA: 6:37 - loss: 0.6910 - acc: 0.5361
2944/5677 [==============>...............] - ETA: 6:29 - loss: 0.6904 - acc: 0.5380
3008/5677 [==============>...............] - ETA: 6:20 - loss: 0.6902 - acc: 0.5396
3072/5677 [===============>..............] - ETA: 6:11 - loss: 0.6900 - acc: 0.5391
3136/5677 [===============>..............] - ETA: 6:02 - loss: 0.6903 - acc: 0.5392
3200/5677 [===============>..............] - ETA: 5:53 - loss: 0.6904 - acc: 0.5381
3264/5677 [================>.............] - ETA: 5:44 - loss: 0.6908 - acc: 0.5365
3328/5677 [================>.............] - ETA: 5:36 - loss: 0.6907 - acc: 0.5367
3392/5677 [================>.............] - ETA: 5:27 - loss: 0.6907 - acc: 0.5377
3456/5677 [=================>............] - ETA: 5:18 - loss: 0.6905 - acc: 0.5376
3520/5677 [=================>............] - ETA: 5:09 - loss: 0.6905 - acc: 0.5378
3584/5677 [=================>............] - ETA: 5:01 - loss: 0.6901 - acc: 0.5393
3648/5677 [==================>...........] - ETA: 4:52 - loss: 0.6907 - acc: 0.5381
3712/5677 [==================>...........] - ETA: 4:43 - loss: 0.6910 - acc: 0.5366
3776/5677 [==================>...........] - ETA: 4:34 - loss: 0.6906 - acc: 0.5379
3840/5677 [===================>..........] - ETA: 4:25 - loss: 0.6910 - acc: 0.5378
3904/5677 [===================>..........] - ETA: 4:15 - loss: 0.6911 - acc: 0.5371
3968/5677 [===================>..........] - ETA: 4:06 - loss: 0.6910 - acc: 0.5376
4032/5677 [====================>.........] - ETA: 3:57 - loss: 0.6905 - acc: 0.5397
4096/5677 [====================>.........] - ETA: 3:47 - loss: 0.6905 - acc: 0.5400
4160/5677 [====================>.........] - ETA: 3:38 - loss: 0.6901 - acc: 0.5411
4224/5677 [=====================>........] - ETA: 3:29 - loss: 0.6905 - acc: 0.5407
4288/5677 [=====================>........] - ETA: 3:20 - loss: 0.6909 - acc: 0.5396
4352/5677 [=====================>........] - ETA: 3:11 - loss: 0.6899 - acc: 0.5420
4416/5677 [======================>.......] - ETA: 3:01 - loss: 0.6898 - acc: 0.5426
4480/5677 [======================>.......] - ETA: 2:52 - loss: 0.6896 - acc: 0.5433
4544/5677 [=======================>......] - ETA: 2:43 - loss: 0.6896 - acc: 0.5431
4608/5677 [=======================>......] - ETA: 2:34 - loss: 0.6896 - acc: 0.5432
4672/5677 [=======================>......] - ETA: 2:24 - loss: 0.6897 - acc: 0.5437
4736/5677 [========================>.....] - ETA: 2:15 - loss: 0.6896 - acc: 0.5450
4800/5677 [========================>.....] - ETA: 2:06 - loss: 0.6897 - acc: 0.5444
4864/5677 [========================>.....] - ETA: 1:57 - loss: 0.6894 - acc: 0.5452
4928/5677 [=========================>....] - ETA: 1:47 - loss: 0.6895 - acc: 0.5444
4992/5677 [=========================>....] - ETA: 1:38 - loss: 0.6895 - acc: 0.5447
5056/5677 [=========================>....] - ETA: 1:29 - loss: 0.6890 - acc: 0.5459
5120/5677 [==========================>...] - ETA: 1:20 - loss: 0.6889 - acc: 0.5461
5184/5677 [==========================>...] - ETA: 1:10 - loss: 0.6889 - acc: 0.5459
5248/5677 [==========================>...] - ETA: 1:01 - loss: 0.6891 - acc: 0.5446
5312/5677 [===========================>..] - ETA: 52s - loss: 0.6889 - acc: 0.5457 
5376/5677 [===========================>..] - ETA: 43s - loss: 0.6889 - acc: 0.5454
5440/5677 [===========================>..] - ETA: 34s - loss: 0.6890 - acc: 0.5456
5504/5677 [============================>.] - ETA: 24s - loss: 0.6889 - acc: 0.5454
5568/5677 [============================>.] - ETA: 15s - loss: 0.6895 - acc: 0.5451
5632/5677 [============================>.] - ETA: 6s - loss: 0.6896 - acc: 0.5444 
5677/5677 [==============================] - 849s 150ms/step - loss: 0.6897 - acc: 0.5443 - val_loss: 0.6816 - val_acc: 0.5499

Epoch 00004: val_acc did not improve from 0.58320
Epoch 5/10

  64/5677 [..............................] - ETA: 13:04 - loss: 0.6945 - acc: 0.5469
 128/5677 [..............................] - ETA: 13:31 - loss: 0.6841 - acc: 0.5703
 192/5677 [>.............................] - ETA: 13:20 - loss: 0.6794 - acc: 0.5833
 256/5677 [>.............................] - ETA: 13:13 - loss: 0.6892 - acc: 0.5586
 320/5677 [>.............................] - ETA: 13:03 - loss: 0.6836 - acc: 0.5625
 384/5677 [=>............................] - ETA: 12:45 - loss: 0.6806 - acc: 0.5625
 448/5677 [=>............................] - ETA: 12:30 - loss: 0.6838 - acc: 0.5536
 512/5677 [=>............................] - ETA: 12:13 - loss: 0.6851 - acc: 0.5410
 576/5677 [==>...........................] - ETA: 12:02 - loss: 0.6822 - acc: 0.5469
 640/5677 [==>...........................] - ETA: 11:53 - loss: 0.6828 - acc: 0.5484
 704/5677 [==>...........................] - ETA: 11:42 - loss: 0.6839 - acc: 0.5483
 768/5677 [===>..........................] - ETA: 11:33 - loss: 0.6842 - acc: 0.5508
 832/5677 [===>..........................] - ETA: 11:28 - loss: 0.6841 - acc: 0.5469
 896/5677 [===>..........................] - ETA: 11:21 - loss: 0.6853 - acc: 0.5469
 960/5677 [====>.........................] - ETA: 11:09 - loss: 0.6830 - acc: 0.5542
1024/5677 [====>.........................] - ETA: 10:58 - loss: 0.6827 - acc: 0.5557
1088/5677 [====>.........................] - ETA: 10:47 - loss: 0.6849 - acc: 0.5533
1152/5677 [=====>........................] - ETA: 10:39 - loss: 0.6818 - acc: 0.5582
1216/5677 [=====>........................] - ETA: 10:35 - loss: 0.6822 - acc: 0.5559
1280/5677 [=====>........................] - ETA: 10:24 - loss: 0.6827 - acc: 0.5531
1344/5677 [======>.......................] - ETA: 10:14 - loss: 0.6834 - acc: 0.5484
1408/5677 [======>.......................] - ETA: 10:03 - loss: 0.6842 - acc: 0.5490
1472/5677 [======>.......................] - ETA: 9:51 - loss: 0.6848 - acc: 0.5489 
1536/5677 [=======>......................] - ETA: 9:39 - loss: 0.6854 - acc: 0.5469
1600/5677 [=======>......................] - ETA: 9:30 - loss: 0.6854 - acc: 0.5494
1664/5677 [=======>......................] - ETA: 9:21 - loss: 0.6852 - acc: 0.5529
1728/5677 [========>.....................] - ETA: 9:12 - loss: 0.6861 - acc: 0.5509
1792/5677 [========>.....................] - ETA: 9:06 - loss: 0.6861 - acc: 0.5508
1856/5677 [========>.....................] - ETA: 8:56 - loss: 0.6875 - acc: 0.5496
1920/5677 [=========>....................] - ETA: 8:46 - loss: 0.6871 - acc: 0.5495
1984/5677 [=========>....................] - ETA: 8:37 - loss: 0.6872 - acc: 0.5514
2048/5677 [=========>....................] - ETA: 8:27 - loss: 0.6867 - acc: 0.5527
2112/5677 [==========>...................] - ETA: 8:19 - loss: 0.6875 - acc: 0.5502
2176/5677 [==========>...................] - ETA: 8:09 - loss: 0.6885 - acc: 0.5496
2240/5677 [==========>...................] - ETA: 8:00 - loss: 0.6885 - acc: 0.5509
2304/5677 [===========>..................] - ETA: 7:50 - loss: 0.6893 - acc: 0.5486
2368/5677 [===========>..................] - ETA: 7:40 - loss: 0.6893 - acc: 0.5481
2432/5677 [===========>..................] - ETA: 7:30 - loss: 0.6904 - acc: 0.5456
2496/5677 [============>.................] - ETA: 7:22 - loss: 0.6909 - acc: 0.5437
2560/5677 [============>.................] - ETA: 7:12 - loss: 0.6905 - acc: 0.5437
2624/5677 [============>.................] - ETA: 7:05 - loss: 0.6906 - acc: 0.5438
2688/5677 [=============>................] - ETA: 6:56 - loss: 0.6910 - acc: 0.5420
2752/5677 [=============>................] - ETA: 6:47 - loss: 0.6906 - acc: 0.5436
2816/5677 [=============>................] - ETA: 6:39 - loss: 0.6907 - acc: 0.5430
2880/5677 [==============>...............] - ETA: 6:31 - loss: 0.6905 - acc: 0.5431
2944/5677 [==============>...............] - ETA: 6:22 - loss: 0.6903 - acc: 0.5452
3008/5677 [==============>...............] - ETA: 6:12 - loss: 0.6897 - acc: 0.5469
3072/5677 [===============>..............] - ETA: 6:02 - loss: 0.6897 - acc: 0.5482
3136/5677 [===============>..............] - ETA: 5:53 - loss: 0.6901 - acc: 0.5475
3200/5677 [===============>..............] - ETA: 5:44 - loss: 0.6901 - acc: 0.5469
3264/5677 [================>.............] - ETA: 5:34 - loss: 0.6899 - acc: 0.5475
3328/5677 [================>.............] - ETA: 5:25 - loss: 0.6897 - acc: 0.5469
3392/5677 [================>.............] - ETA: 5:16 - loss: 0.6897 - acc: 0.5457
3456/5677 [=================>............] - ETA: 5:07 - loss: 0.6893 - acc: 0.5469
3520/5677 [=================>............] - ETA: 4:58 - loss: 0.6892 - acc: 0.5474
3584/5677 [=================>............] - ETA: 4:49 - loss: 0.6893 - acc: 0.5477
3648/5677 [==================>...........] - ETA: 4:41 - loss: 0.6884 - acc: 0.5513
3712/5677 [==================>...........] - ETA: 4:32 - loss: 0.6886 - acc: 0.5504
3776/5677 [==================>...........] - ETA: 4:23 - loss: 0.6884 - acc: 0.5501
3840/5677 [===================>..........] - ETA: 4:14 - loss: 0.6885 - acc: 0.5492
3904/5677 [===================>..........] - ETA: 4:06 - loss: 0.6880 - acc: 0.5510
3968/5677 [===================>..........] - ETA: 3:57 - loss: 0.6878 - acc: 0.5519
4032/5677 [====================>.........] - ETA: 3:48 - loss: 0.6875 - acc: 0.5528
4096/5677 [====================>.........] - ETA: 3:39 - loss: 0.6875 - acc: 0.5542
4160/5677 [====================>.........] - ETA: 3:30 - loss: 0.6881 - acc: 0.5529
4224/5677 [=====================>........] - ETA: 3:21 - loss: 0.6885 - acc: 0.5523
4288/5677 [=====================>........] - ETA: 3:12 - loss: 0.6889 - acc: 0.5515
4352/5677 [=====================>........] - ETA: 3:04 - loss: 0.6888 - acc: 0.5524
4416/5677 [======================>.......] - ETA: 2:55 - loss: 0.6884 - acc: 0.5530
4480/5677 [======================>.......] - ETA: 2:46 - loss: 0.6882 - acc: 0.5531
4544/5677 [=======================>......] - ETA: 2:37 - loss: 0.6878 - acc: 0.5535
4608/5677 [=======================>......] - ETA: 2:28 - loss: 0.6878 - acc: 0.5527
4672/5677 [=======================>......] - ETA: 2:19 - loss: 0.6881 - acc: 0.5529
4736/5677 [========================>.....] - ETA: 2:11 - loss: 0.6879 - acc: 0.5532
4800/5677 [========================>.....] - ETA: 2:02 - loss: 0.6878 - acc: 0.5537
4864/5677 [========================>.....] - ETA: 1:53 - loss: 0.6874 - acc: 0.5551
4928/5677 [=========================>....] - ETA: 1:44 - loss: 0.6874 - acc: 0.5550
4992/5677 [=========================>....] - ETA: 1:35 - loss: 0.6871 - acc: 0.5555
5056/5677 [=========================>....] - ETA: 1:26 - loss: 0.6871 - acc: 0.5552
5120/5677 [==========================>...] - ETA: 1:17 - loss: 0.6873 - acc: 0.5551
5184/5677 [==========================>...] - ETA: 1:08 - loss: 0.6870 - acc: 0.5559
5248/5677 [==========================>...] - ETA: 59s - loss: 0.6870 - acc: 0.5560 
5312/5677 [===========================>..] - ETA: 50s - loss: 0.6871 - acc: 0.5561
5376/5677 [===========================>..] - ETA: 41s - loss: 0.6871 - acc: 0.5556
5440/5677 [===========================>..] - ETA: 32s - loss: 0.6872 - acc: 0.5553
5504/5677 [============================>.] - ETA: 23s - loss: 0.6875 - acc: 0.5547
5568/5677 [============================>.] - ETA: 15s - loss: 0.6874 - acc: 0.5548
5632/5677 [============================>.] - ETA: 6s - loss: 0.6875 - acc: 0.5542 
5677/5677 [==============================] - 815s 144ms/step - loss: 0.6872 - acc: 0.5547 - val_loss: 0.6838 - val_acc: 0.5642

Epoch 00005: val_acc did not improve from 0.58320
Epoch 6/10

  64/5677 [..............................] - ETA: 14:00 - loss: 0.6669 - acc: 0.6094
 128/5677 [..............................] - ETA: 12:54 - loss: 0.6723 - acc: 0.6172
 192/5677 [>.............................] - ETA: 12:54 - loss: 0.6666 - acc: 0.6094
 256/5677 [>.............................] - ETA: 12:40 - loss: 0.6795 - acc: 0.5703
 320/5677 [>.............................] - ETA: 12:16 - loss: 0.6814 - acc: 0.5625
 384/5677 [=>............................] - ETA: 12:15 - loss: 0.6827 - acc: 0.5677
 448/5677 [=>............................] - ETA: 12:18 - loss: 0.6902 - acc: 0.5513
 512/5677 [=>............................] - ETA: 11:58 - loss: 0.6888 - acc: 0.5566
 576/5677 [==>...........................] - ETA: 11:43 - loss: 0.6916 - acc: 0.5469
 640/5677 [==>...........................] - ETA: 11:33 - loss: 0.6922 - acc: 0.5453
 704/5677 [==>...........................] - ETA: 11:23 - loss: 0.6919 - acc: 0.5455
 768/5677 [===>..........................] - ETA: 11:16 - loss: 0.6917 - acc: 0.5469
 832/5677 [===>..........................] - ETA: 11:07 - loss: 0.6925 - acc: 0.5469
 896/5677 [===>..........................] - ETA: 10:58 - loss: 0.6913 - acc: 0.5513
 960/5677 [====>.........................] - ETA: 10:46 - loss: 0.6890 - acc: 0.5583
1024/5677 [====>.........................] - ETA: 10:35 - loss: 0.6875 - acc: 0.5596
1088/5677 [====>.........................] - ETA: 10:21 - loss: 0.6875 - acc: 0.5570
1152/5677 [=====>........................] - ETA: 10:12 - loss: 0.6888 - acc: 0.5538
1216/5677 [=====>........................] - ETA: 10:06 - loss: 0.6882 - acc: 0.5559
1280/5677 [=====>........................] - ETA: 9:56 - loss: 0.6874 - acc: 0.5594 
1344/5677 [======>.......................] - ETA: 9:45 - loss: 0.6892 - acc: 0.5543
1408/5677 [======>.......................] - ETA: 9:35 - loss: 0.6886 - acc: 0.5554
1472/5677 [======>.......................] - ETA: 9:27 - loss: 0.6879 - acc: 0.5537
1536/5677 [=======>......................] - ETA: 9:19 - loss: 0.6881 - acc: 0.5527
1600/5677 [=======>......................] - ETA: 9:13 - loss: 0.6862 - acc: 0.5575
1664/5677 [=======>......................] - ETA: 9:05 - loss: 0.6845 - acc: 0.5619
1728/5677 [========>.....................] - ETA: 8:57 - loss: 0.6840 - acc: 0.5637
1792/5677 [========>.....................] - ETA: 8:47 - loss: 0.6839 - acc: 0.5625
1856/5677 [========>.....................] - ETA: 8:36 - loss: 0.6828 - acc: 0.5641
1920/5677 [=========>....................] - ETA: 8:26 - loss: 0.6827 - acc: 0.5667
1984/5677 [=========>....................] - ETA: 8:18 - loss: 0.6828 - acc: 0.5670
2048/5677 [=========>....................] - ETA: 8:11 - loss: 0.6839 - acc: 0.5635
2112/5677 [==========>...................] - ETA: 8:04 - loss: 0.6840 - acc: 0.5630
2176/5677 [==========>...................] - ETA: 7:57 - loss: 0.6834 - acc: 0.5634
2240/5677 [==========>...................] - ETA: 7:48 - loss: 0.6836 - acc: 0.5629
2304/5677 [===========>..................] - ETA: 7:40 - loss: 0.6838 - acc: 0.5621
2368/5677 [===========>..................] - ETA: 7:32 - loss: 0.6831 - acc: 0.5625
2432/5677 [===========>..................] - ETA: 7:21 - loss: 0.6838 - acc: 0.5613
2496/5677 [============>.................] - ETA: 7:12 - loss: 0.6842 - acc: 0.5617
2560/5677 [============>.................] - ETA: 7:03 - loss: 0.6838 - acc: 0.5645
2624/5677 [============>.................] - ETA: 6:55 - loss: 0.6845 - acc: 0.5621
2688/5677 [=============>................] - ETA: 6:47 - loss: 0.6844 - acc: 0.5629
2752/5677 [=============>................] - ETA: 6:40 - loss: 0.6833 - acc: 0.5661
2816/5677 [=============>................] - ETA: 6:31 - loss: 0.6827 - acc: 0.5664
2880/5677 [==============>...............] - ETA: 6:22 - loss: 0.6823 - acc: 0.5667
2944/5677 [==============>...............] - ETA: 6:13 - loss: 0.6824 - acc: 0.5666
3008/5677 [==============>...............] - ETA: 6:05 - loss: 0.6816 - acc: 0.5688
3072/5677 [===============>..............] - ETA: 5:56 - loss: 0.6806 - acc: 0.5703
3136/5677 [===============>..............] - ETA: 5:48 - loss: 0.6809 - acc: 0.5702
3200/5677 [===============>..............] - ETA: 5:41 - loss: 0.6801 - acc: 0.5713
3264/5677 [================>.............] - ETA: 5:32 - loss: 0.6797 - acc: 0.5726
3328/5677 [================>.............] - ETA: 5:24 - loss: 0.6802 - acc: 0.5721
3392/5677 [================>.............] - ETA: 5:16 - loss: 0.6801 - acc: 0.5725
3456/5677 [=================>............] - ETA: 5:07 - loss: 0.6802 - acc: 0.5726
3520/5677 [=================>............] - ETA: 4:59 - loss: 0.6801 - acc: 0.5733
3584/5677 [=================>............] - ETA: 4:51 - loss: 0.6803 - acc: 0.5739
3648/5677 [==================>...........] - ETA: 4:43 - loss: 0.6810 - acc: 0.5729
3712/5677 [==================>...........] - ETA: 4:35 - loss: 0.6816 - acc: 0.5727
3776/5677 [==================>...........] - ETA: 4:27 - loss: 0.6818 - acc: 0.5720
3840/5677 [===================>..........] - ETA: 4:18 - loss: 0.6825 - acc: 0.5706
3904/5677 [===================>..........] - ETA: 4:10 - loss: 0.6826 - acc: 0.5694
3968/5677 [===================>..........] - ETA: 4:01 - loss: 0.6823 - acc: 0.5701
4032/5677 [====================>.........] - ETA: 3:52 - loss: 0.6824 - acc: 0.5694
4096/5677 [====================>.........] - ETA: 3:43 - loss: 0.6827 - acc: 0.5686
4160/5677 [====================>.........] - ETA: 3:35 - loss: 0.6823 - acc: 0.5700
4224/5677 [=====================>........] - ETA: 3:26 - loss: 0.6824 - acc: 0.5701
4288/5677 [=====================>........] - ETA: 3:17 - loss: 0.6825 - acc: 0.5695
4352/5677 [=====================>........] - ETA: 3:09 - loss: 0.6814 - acc: 0.5710
4416/5677 [======================>.......] - ETA: 3:00 - loss: 0.6815 - acc: 0.5704
4480/5677 [======================>.......] - ETA: 2:51 - loss: 0.6815 - acc: 0.5703
4544/5677 [=======================>......] - ETA: 2:42 - loss: 0.6815 - acc: 0.5695
4608/5677 [=======================>......] - ETA: 2:33 - loss: 0.6818 - acc: 0.5686
4672/5677 [=======================>......] - ETA: 2:24 - loss: 0.6818 - acc: 0.5685
4736/5677 [========================>.....] - ETA: 2:15 - loss: 0.6818 - acc: 0.5678
4800/5677 [========================>.....] - ETA: 2:06 - loss: 0.6821 - acc: 0.5679
4864/5677 [========================>.....] - ETA: 1:57 - loss: 0.6822 - acc: 0.5678
4928/5677 [=========================>....] - ETA: 1:48 - loss: 0.6823 - acc: 0.5682
4992/5677 [=========================>....] - ETA: 1:39 - loss: 0.6826 - acc: 0.5675
5056/5677 [=========================>....] - ETA: 1:30 - loss: 0.6825 - acc: 0.5680
5120/5677 [==========================>...] - ETA: 1:20 - loss: 0.6825 - acc: 0.5676
5184/5677 [==========================>...] - ETA: 1:11 - loss: 0.6823 - acc: 0.5683
5248/5677 [==========================>...] - ETA: 1:02 - loss: 0.6825 - acc: 0.5675
5312/5677 [===========================>..] - ETA: 53s - loss: 0.6824 - acc: 0.5680 
5376/5677 [===========================>..] - ETA: 43s - loss: 0.6820 - acc: 0.5694
5440/5677 [===========================>..] - ETA: 34s - loss: 0.6821 - acc: 0.5693
5504/5677 [============================>.] - ETA: 25s - loss: 0.6818 - acc: 0.5698
5568/5677 [============================>.] - ETA: 15s - loss: 0.6815 - acc: 0.5711
5632/5677 [============================>.] - ETA: 6s - loss: 0.6814 - acc: 0.5710 
5677/5677 [==============================] - 858s 151ms/step - loss: 0.6814 - acc: 0.5705 - val_loss: 0.6964 - val_acc: 0.5404

Epoch 00006: val_acc did not improve from 0.58320
Epoch 7/10

  64/5677 [..............................] - ETA: 14:43 - loss: 0.6631 - acc: 0.5781
 128/5677 [..............................] - ETA: 14:13 - loss: 0.6736 - acc: 0.5781
 192/5677 [>.............................] - ETA: 13:18 - loss: 0.6777 - acc: 0.5625
 256/5677 [>.............................] - ETA: 12:51 - loss: 0.6809 - acc: 0.5547
 320/5677 [>.............................] - ETA: 12:28 - loss: 0.6824 - acc: 0.5437
 384/5677 [=>............................] - ETA: 12:04 - loss: 0.6857 - acc: 0.5365
 448/5677 [=>............................] - ETA: 11:55 - loss: 0.6821 - acc: 0.5558
 512/5677 [=>............................] - ETA: 11:42 - loss: 0.6819 - acc: 0.5586
 576/5677 [==>...........................] - ETA: 11:35 - loss: 0.6790 - acc: 0.5625
 640/5677 [==>...........................] - ETA: 11:18 - loss: 0.6807 - acc: 0.5578
 704/5677 [==>...........................] - ETA: 10:59 - loss: 0.6821 - acc: 0.5611
 768/5677 [===>..........................] - ETA: 10:47 - loss: 0.6794 - acc: 0.5664
 832/5677 [===>..........................] - ETA: 10:32 - loss: 0.6815 - acc: 0.5613
 896/5677 [===>..........................] - ETA: 10:22 - loss: 0.6841 - acc: 0.5603
 960/5677 [====>.........................] - ETA: 10:10 - loss: 0.6789 - acc: 0.5729
1024/5677 [====>.........................] - ETA: 10:00 - loss: 0.6783 - acc: 0.5771
1088/5677 [====>.........................] - ETA: 9:50 - loss: 0.6784 - acc: 0.5763 
1152/5677 [=====>........................] - ETA: 9:44 - loss: 0.6794 - acc: 0.5764
1216/5677 [=====>........................] - ETA: 9:36 - loss: 0.6806 - acc: 0.5691
1280/5677 [=====>........................] - ETA: 9:29 - loss: 0.6793 - acc: 0.5703
1344/5677 [======>.......................] - ETA: 9:20 - loss: 0.6810 - acc: 0.5685
1408/5677 [======>.......................] - ETA: 9:10 - loss: 0.6817 - acc: 0.5668
1472/5677 [======>.......................] - ETA: 9:00 - loss: 0.6821 - acc: 0.5652
1536/5677 [=======>......................] - ETA: 8:53 - loss: 0.6820 - acc: 0.5677
1600/5677 [=======>......................] - ETA: 8:43 - loss: 0.6813 - acc: 0.5694
1664/5677 [=======>......................] - ETA: 8:35 - loss: 0.6814 - acc: 0.5715
1728/5677 [========>.....................] - ETA: 8:25 - loss: 0.6826 - acc: 0.5694
1792/5677 [========>.....................] - ETA: 8:18 - loss: 0.6839 - acc: 0.5636
1856/5677 [========>.....................] - ETA: 8:10 - loss: 0.6829 - acc: 0.5668
1920/5677 [=========>....................] - ETA: 8:02 - loss: 0.6823 - acc: 0.5661
1984/5677 [=========>....................] - ETA: 7:54 - loss: 0.6822 - acc: 0.5680
2048/5677 [=========>....................] - ETA: 7:45 - loss: 0.6816 - acc: 0.5698
2112/5677 [==========>...................] - ETA: 7:38 - loss: 0.6819 - acc: 0.5701
2176/5677 [==========>...................] - ETA: 7:30 - loss: 0.6828 - acc: 0.5676
2240/5677 [==========>...................] - ETA: 7:21 - loss: 0.6821 - acc: 0.5692
2304/5677 [===========>..................] - ETA: 7:12 - loss: 0.6818 - acc: 0.5707
2368/5677 [===========>..................] - ETA: 7:04 - loss: 0.6811 - acc: 0.5714
2432/5677 [===========>..................] - ETA: 6:55 - loss: 0.6809 - acc: 0.5715
2496/5677 [============>.................] - ETA: 6:46 - loss: 0.6804 - acc: 0.5729
2560/5677 [============>.................] - ETA: 6:38 - loss: 0.6806 - acc: 0.5719
2624/5677 [============>.................] - ETA: 6:31 - loss: 0.6802 - acc: 0.5739
2688/5677 [=============>................] - ETA: 6:23 - loss: 0.6806 - acc: 0.5718
2752/5677 [=============>................] - ETA: 6:15 - loss: 0.6796 - acc: 0.5741
2816/5677 [=============>................] - ETA: 6:06 - loss: 0.6799 - acc: 0.5732
2880/5677 [==============>...............] - ETA: 5:59 - loss: 0.6798 - acc: 0.5753
2944/5677 [==============>...............] - ETA: 5:51 - loss: 0.6802 - acc: 0.5744
3008/5677 [==============>...............] - ETA: 5:43 - loss: 0.6795 - acc: 0.5761
3072/5677 [===============>..............] - ETA: 5:34 - loss: 0.6791 - acc: 0.5765
3136/5677 [===============>..............] - ETA: 5:26 - loss: 0.6790 - acc: 0.5765
3200/5677 [===============>..............] - ETA: 5:18 - loss: 0.6786 - acc: 0.5766
3264/5677 [================>.............] - ETA: 5:09 - loss: 0.6795 - acc: 0.5748
3328/5677 [================>.............] - ETA: 5:02 - loss: 0.6797 - acc: 0.5751
3392/5677 [================>.............] - ETA: 4:53 - loss: 0.6796 - acc: 0.5749
3456/5677 [=================>............] - ETA: 4:45 - loss: 0.6795 - acc: 0.5752
3520/5677 [=================>............] - ETA: 4:37 - loss: 0.6791 - acc: 0.5756
3584/5677 [=================>............] - ETA: 4:28 - loss: 0.6792 - acc: 0.5759
3648/5677 [==================>...........] - ETA: 4:20 - loss: 0.6785 - acc: 0.5770
3712/5677 [==================>...........] - ETA: 4:12 - loss: 0.6792 - acc: 0.5757
3776/5677 [==================>...........] - ETA: 4:04 - loss: 0.6795 - acc: 0.5752
3840/5677 [===================>..........] - ETA: 3:55 - loss: 0.6803 - acc: 0.5740
3904/5677 [===================>..........] - ETA: 3:47 - loss: 0.6807 - acc: 0.5730
3968/5677 [===================>..........] - ETA: 3:38 - loss: 0.6804 - acc: 0.5738
4032/5677 [====================>.........] - ETA: 3:30 - loss: 0.6806 - acc: 0.5742
4096/5677 [====================>.........] - ETA: 3:21 - loss: 0.6804 - acc: 0.5742
4160/5677 [====================>.........] - ETA: 3:13 - loss: 0.6806 - acc: 0.5743
4224/5677 [=====================>........] - ETA: 3:04 - loss: 0.6807 - acc: 0.5741
4288/5677 [=====================>........] - ETA: 2:56 - loss: 0.6802 - acc: 0.5744
4352/5677 [=====================>........] - ETA: 2:48 - loss: 0.6802 - acc: 0.5740
4416/5677 [======================>.......] - ETA: 2:40 - loss: 0.6806 - acc: 0.5736
4480/5677 [======================>.......] - ETA: 2:32 - loss: 0.6807 - acc: 0.5730
4544/5677 [=======================>......] - ETA: 2:24 - loss: 0.6806 - acc: 0.5733
4608/5677 [=======================>......] - ETA: 2:16 - loss: 0.6810 - acc: 0.5729
4672/5677 [=======================>......] - ETA: 2:08 - loss: 0.6811 - acc: 0.5726
4736/5677 [========================>.....] - ETA: 1:59 - loss: 0.6812 - acc: 0.5724
4800/5677 [========================>.....] - ETA: 1:51 - loss: 0.6811 - acc: 0.5723
4864/5677 [========================>.....] - ETA: 1:43 - loss: 0.6811 - acc: 0.5728
4928/5677 [=========================>....] - ETA: 1:35 - loss: 0.6809 - acc: 0.5737
4992/5677 [=========================>....] - ETA: 1:27 - loss: 0.6809 - acc: 0.5741
5056/5677 [=========================>....] - ETA: 1:18 - loss: 0.6814 - acc: 0.5730
5120/5677 [==========================>...] - ETA: 1:10 - loss: 0.6812 - acc: 0.5736
5184/5677 [==========================>...] - ETA: 1:02 - loss: 0.6811 - acc: 0.5741
5248/5677 [==========================>...] - ETA: 54s - loss: 0.6812 - acc: 0.5745 
5312/5677 [===========================>..] - ETA: 46s - loss: 0.6815 - acc: 0.5738
5376/5677 [===========================>..] - ETA: 38s - loss: 0.6816 - acc: 0.5737
5440/5677 [===========================>..] - ETA: 30s - loss: 0.6817 - acc: 0.5735
5504/5677 [============================>.] - ETA: 22s - loss: 0.6818 - acc: 0.5732
5568/5677 [============================>.] - ETA: 13s - loss: 0.6820 - acc: 0.5729
5632/5677 [============================>.] - ETA: 5s - loss: 0.6822 - acc: 0.5721 
5677/5677 [==============================] - 762s 134ms/step - loss: 0.6819 - acc: 0.5730 - val_loss: 0.7374 - val_acc: 0.4723

Epoch 00007: val_acc did not improve from 0.58320
Epoch 8/10

  64/5677 [..............................] - ETA: 13:15 - loss: 0.7018 - acc: 0.5312
 128/5677 [..............................] - ETA: 13:49 - loss: 0.6988 - acc: 0.5156
 192/5677 [>.............................] - ETA: 13:34 - loss: 0.6931 - acc: 0.5260
 256/5677 [>.............................] - ETA: 13:15 - loss: 0.6845 - acc: 0.5508
 320/5677 [>.............................] - ETA: 13:24 - loss: 0.6806 - acc: 0.5563
 384/5677 [=>............................] - ETA: 13:07 - loss: 0.6771 - acc: 0.5703
 448/5677 [=>............................] - ETA: 12:42 - loss: 0.6817 - acc: 0.5603
 512/5677 [=>............................] - ETA: 12:37 - loss: 0.6782 - acc: 0.5723
 576/5677 [==>...........................] - ETA: 12:32 - loss: 0.6780 - acc: 0.5747
 640/5677 [==>...........................] - ETA: 12:25 - loss: 0.6769 - acc: 0.5734
 704/5677 [==>...........................] - ETA: 12:15 - loss: 0.6764 - acc: 0.5753
 768/5677 [===>..........................] - ETA: 12:05 - loss: 0.6755 - acc: 0.5807
 832/5677 [===>..........................] - ETA: 11:53 - loss: 0.6772 - acc: 0.5805
 896/5677 [===>..........................] - ETA: 11:43 - loss: 0.6781 - acc: 0.5792
 960/5677 [====>.........................] - ETA: 11:36 - loss: 0.6768 - acc: 0.5813
1024/5677 [====>.........................] - ETA: 11:24 - loss: 0.6768 - acc: 0.5791
1088/5677 [====>.........................] - ETA: 11:12 - loss: 0.6753 - acc: 0.5800
1152/5677 [=====>........................] - ETA: 11:02 - loss: 0.6761 - acc: 0.5799
1216/5677 [=====>........................] - ETA: 10:51 - loss: 0.6750 - acc: 0.5822
1280/5677 [=====>........................] - ETA: 10:39 - loss: 0.6768 - acc: 0.5797
1344/5677 [======>.......................] - ETA: 10:34 - loss: 0.6751 - acc: 0.5841
1408/5677 [======>.......................] - ETA: 10:25 - loss: 0.6744 - acc: 0.5824
1472/5677 [======>.......................] - ETA: 10:15 - loss: 0.6730 - acc: 0.5863
1536/5677 [=======>......................] - ETA: 10:05 - loss: 0.6733 - acc: 0.5859
1600/5677 [=======>......................] - ETA: 9:57 - loss: 0.6751 - acc: 0.5831 
1664/5677 [=======>......................] - ETA: 9:47 - loss: 0.6762 - acc: 0.5835
1728/5677 [========>.....................] - ETA: 9:39 - loss: 0.6754 - acc: 0.5833
1792/5677 [========>.....................] - ETA: 9:30 - loss: 0.6765 - acc: 0.5776
1856/5677 [========>.....................] - ETA: 9:19 - loss: 0.6753 - acc: 0.5814
1920/5677 [=========>....................] - ETA: 9:10 - loss: 0.6760 - acc: 0.5797
1984/5677 [=========>....................] - ETA: 8:59 - loss: 0.6771 - acc: 0.5801
2048/5677 [=========>....................] - ETA: 8:48 - loss: 0.6777 - acc: 0.5791
2112/5677 [==========>...................] - ETA: 8:38 - loss: 0.6773 - acc: 0.5791
2176/5677 [==========>...................] - ETA: 8:27 - loss: 0.6777 - acc: 0.5777
2240/5677 [==========>...................] - ETA: 8:18 - loss: 0.6771 - acc: 0.5804
2304/5677 [===========>..................] - ETA: 8:08 - loss: 0.6766 - acc: 0.5807
2368/5677 [===========>..................] - ETA: 8:01 - loss: 0.6757 - acc: 0.5828
2432/5677 [===========>..................] - ETA: 7:52 - loss: 0.6769 - acc: 0.5814
2496/5677 [============>.................] - ETA: 7:42 - loss: 0.6770 - acc: 0.5813
2560/5677 [============>.................] - ETA: 7:32 - loss: 0.6769 - acc: 0.5824
2624/5677 [============>.................] - ETA: 7:23 - loss: 0.6770 - acc: 0.5812
2688/5677 [=============>................] - ETA: 7:13 - loss: 0.6755 - acc: 0.5837
2752/5677 [=============>................] - ETA: 7:04 - loss: 0.6752 - acc: 0.5843
2816/5677 [=============>................] - ETA: 6:55 - loss: 0.6754 - acc: 0.5849
2880/5677 [==============>...............] - ETA: 6:47 - loss: 0.6758 - acc: 0.5844
2944/5677 [==============>...............] - ETA: 6:38 - loss: 0.6758 - acc: 0.5839
3008/5677 [==============>...............] - ETA: 6:29 - loss: 0.6748 - acc: 0.5858
3072/5677 [===============>..............] - ETA: 6:20 - loss: 0.6744 - acc: 0.5856
3136/5677 [===============>..............] - ETA: 6:11 - loss: 0.6746 - acc: 0.5851
3200/5677 [===============>..............] - ETA: 6:01 - loss: 0.6764 - acc: 0.5809
3264/5677 [================>.............] - ETA: 5:52 - loss: 0.6764 - acc: 0.5806
3328/5677 [================>.............] - ETA: 5:43 - loss: 0.6768 - acc: 0.5793
3392/5677 [================>.............] - ETA: 5:34 - loss: 0.6769 - acc: 0.5799
3456/5677 [=================>............] - ETA: 5:25 - loss: 0.6767 - acc: 0.5807
3520/5677 [=================>............] - ETA: 5:16 - loss: 0.6767 - acc: 0.5804
3584/5677 [=================>............] - ETA: 5:06 - loss: 0.6773 - acc: 0.5798
3648/5677 [==================>...........] - ETA: 4:57 - loss: 0.6773 - acc: 0.5792
3712/5677 [==================>...........] - ETA: 4:47 - loss: 0.6772 - acc: 0.5792
3776/5677 [==================>...........] - ETA: 4:38 - loss: 0.6772 - acc: 0.5789
3840/5677 [===================>..........] - ETA: 4:28 - loss: 0.6772 - acc: 0.5794
3904/5677 [===================>..........] - ETA: 4:18 - loss: 0.6771 - acc: 0.5791
3968/5677 [===================>..........] - ETA: 4:09 - loss: 0.6771 - acc: 0.5791
4032/5677 [====================>.........] - ETA: 3:59 - loss: 0.6771 - acc: 0.5786
4096/5677 [====================>.........] - ETA: 3:50 - loss: 0.6775 - acc: 0.5774
4160/5677 [====================>.........] - ETA: 3:40 - loss: 0.6778 - acc: 0.5769
4224/5677 [=====================>........] - ETA: 3:31 - loss: 0.6782 - acc: 0.5741
4288/5677 [=====================>........] - ETA: 3:21 - loss: 0.6779 - acc: 0.5739
4352/5677 [=====================>........] - ETA: 3:12 - loss: 0.6776 - acc: 0.5735
4416/5677 [======================>.......] - ETA: 3:02 - loss: 0.6780 - acc: 0.5727
4480/5677 [======================>.......] - ETA: 2:53 - loss: 0.6780 - acc: 0.5728
4544/5677 [=======================>......] - ETA: 2:44 - loss: 0.6782 - acc: 0.5720
4608/5677 [=======================>......] - ETA: 2:34 - loss: 0.6783 - acc: 0.5718
4672/5677 [=======================>......] - ETA: 2:25 - loss: 0.6782 - acc: 0.5726
4736/5677 [========================>.....] - ETA: 2:16 - loss: 0.6782 - acc: 0.5720
4800/5677 [========================>.....] - ETA: 2:06 - loss: 0.6779 - acc: 0.5735
4864/5677 [========================>.....] - ETA: 1:57 - loss: 0.6779 - acc: 0.5736
4928/5677 [=========================>....] - ETA: 1:48 - loss: 0.6782 - acc: 0.5733
4992/5677 [=========================>....] - ETA: 1:38 - loss: 0.6781 - acc: 0.5735
5056/5677 [=========================>....] - ETA: 1:29 - loss: 0.6782 - acc: 0.5740
5120/5677 [==========================>...] - ETA: 1:19 - loss: 0.6784 - acc: 0.5740
5184/5677 [==========================>...] - ETA: 1:10 - loss: 0.6786 - acc: 0.5737
5248/5677 [==========================>...] - ETA: 1:01 - loss: 0.6788 - acc: 0.5734
5312/5677 [===========================>..] - ETA: 52s - loss: 0.6788 - acc: 0.5732 
5376/5677 [===========================>..] - ETA: 43s - loss: 0.6790 - acc: 0.5727
5440/5677 [===========================>..] - ETA: 33s - loss: 0.6791 - acc: 0.5722
5504/5677 [============================>.] - ETA: 24s - loss: 0.6789 - acc: 0.5727
5568/5677 [============================>.] - ETA: 15s - loss: 0.6787 - acc: 0.5729
5632/5677 [============================>.] - ETA: 6s - loss: 0.6786 - acc: 0.5742 
5677/5677 [==============================] - 845s 149ms/step - loss: 0.6787 - acc: 0.5739 - val_loss: 0.6799 - val_acc: 0.5626

Epoch 00008: val_acc did not improve from 0.58320
Epoch 9/10

  64/5677 [..............................] - ETA: 13:34 - loss: 0.6506 - acc: 0.7031
 128/5677 [..............................] - ETA: 12:57 - loss: 0.6596 - acc: 0.6328
 192/5677 [>.............................] - ETA: 12:44 - loss: 0.6703 - acc: 0.5885
 256/5677 [>.............................] - ETA: 12:34 - loss: 0.6579 - acc: 0.6328
 320/5677 [>.............................] - ETA: 12:18 - loss: 0.6609 - acc: 0.6156
 384/5677 [=>............................] - ETA: 11:59 - loss: 0.6612 - acc: 0.6146
 448/5677 [=>............................] - ETA: 11:41 - loss: 0.6651 - acc: 0.6049
 512/5677 [=>............................] - ETA: 11:36 - loss: 0.6663 - acc: 0.6055
 576/5677 [==>...........................] - ETA: 11:20 - loss: 0.6704 - acc: 0.5955
 640/5677 [==>...........................] - ETA: 11:04 - loss: 0.6756 - acc: 0.5875
 704/5677 [==>...........................] - ETA: 11:00 - loss: 0.6775 - acc: 0.5810
 768/5677 [===>..........................] - ETA: 10:54 - loss: 0.6745 - acc: 0.5885
 832/5677 [===>..........................] - ETA: 10:46 - loss: 0.6737 - acc: 0.5889
 896/5677 [===>..........................] - ETA: 10:39 - loss: 0.6759 - acc: 0.5848
 960/5677 [====>.........................] - ETA: 10:34 - loss: 0.6774 - acc: 0.5813
1024/5677 [====>.........................] - ETA: 10:30 - loss: 0.6796 - acc: 0.5801
1088/5677 [====>.........................] - ETA: 10:21 - loss: 0.6779 - acc: 0.5827
1152/5677 [=====>........................] - ETA: 10:12 - loss: 0.6774 - acc: 0.5807
1216/5677 [=====>........................] - ETA: 10:03 - loss: 0.6778 - acc: 0.5798
1280/5677 [=====>........................] - ETA: 9:52 - loss: 0.6777 - acc: 0.5820 
1344/5677 [======>.......................] - ETA: 9:43 - loss: 0.6769 - acc: 0.5833
1408/5677 [======>.......................] - ETA: 9:33 - loss: 0.6772 - acc: 0.5803
1472/5677 [======>.......................] - ETA: 9:27 - loss: 0.6752 - acc: 0.5829
1536/5677 [=======>......................] - ETA: 9:20 - loss: 0.6745 - acc: 0.5827
1600/5677 [=======>......................] - ETA: 9:13 - loss: 0.6736 - acc: 0.5844
1664/5677 [=======>......................] - ETA: 9:09 - loss: 0.6749 - acc: 0.5811
1728/5677 [========>.....................] - ETA: 9:02 - loss: 0.6749 - acc: 0.5833
1792/5677 [========>.....................] - ETA: 8:54 - loss: 0.6746 - acc: 0.5831
1856/5677 [========>.....................] - ETA: 8:47 - loss: 0.6740 - acc: 0.5841
1920/5677 [=========>....................] - ETA: 8:39 - loss: 0.6759 - acc: 0.5828
1984/5677 [=========>....................] - ETA: 8:30 - loss: 0.6762 - acc: 0.5852
2048/5677 [=========>....................] - ETA: 8:22 - loss: 0.6760 - acc: 0.5874
2112/5677 [==========>...................] - ETA: 8:13 - loss: 0.6760 - acc: 0.5876
2176/5677 [==========>...................] - ETA: 8:04 - loss: 0.6755 - acc: 0.5882
2240/5677 [==========>...................] - ETA: 7:57 - loss: 0.6764 - acc: 0.5871
2304/5677 [===========>..................] - ETA: 7:50 - loss: 0.6772 - acc: 0.5859
2368/5677 [===========>..................] - ETA: 7:40 - loss: 0.6766 - acc: 0.5866
2432/5677 [===========>..................] - ETA: 7:33 - loss: 0.6757 - acc: 0.5896
2496/5677 [============>.................] - ETA: 7:23 - loss: 0.6759 - acc: 0.5881
2560/5677 [============>.................] - ETA: 7:13 - loss: 0.6760 - acc: 0.5895
2624/5677 [============>.................] - ETA: 7:05 - loss: 0.6762 - acc: 0.5896
2688/5677 [=============>................] - ETA: 6:57 - loss: 0.6761 - acc: 0.5897
2752/5677 [=============>................] - ETA: 6:48 - loss: 0.6760 - acc: 0.5894
2816/5677 [=============>................] - ETA: 6:38 - loss: 0.6756 - acc: 0.5888
2880/5677 [==============>...............] - ETA: 6:30 - loss: 0.6760 - acc: 0.5878
2944/5677 [==============>...............] - ETA: 6:20 - loss: 0.6750 - acc: 0.5890
3008/5677 [==============>...............] - ETA: 6:10 - loss: 0.6752 - acc: 0.5891
3072/5677 [===============>..............] - ETA: 6:01 - loss: 0.6753 - acc: 0.5892
3136/5677 [===============>..............] - ETA: 5:52 - loss: 0.6761 - acc: 0.5871
3200/5677 [===============>..............] - ETA: 5:44 - loss: 0.6754 - acc: 0.5891
3264/5677 [================>.............] - ETA: 5:35 - loss: 0.6753 - acc: 0.5895
3328/5677 [================>.............] - ETA: 5:27 - loss: 0.6749 - acc: 0.5907
3392/5677 [================>.............] - ETA: 5:17 - loss: 0.6753 - acc: 0.5899
3456/5677 [=================>............] - ETA: 5:09 - loss: 0.6754 - acc: 0.5903
3520/5677 [=================>............] - ETA: 5:00 - loss: 0.6745 - acc: 0.5918
3584/5677 [=================>............] - ETA: 4:51 - loss: 0.6745 - acc: 0.5915
3648/5677 [==================>...........] - ETA: 4:42 - loss: 0.6740 - acc: 0.5921
3712/5677 [==================>...........] - ETA: 4:34 - loss: 0.6747 - acc: 0.5900
3776/5677 [==================>...........] - ETA: 4:25 - loss: 0.6756 - acc: 0.5869
3840/5677 [===================>..........] - ETA: 4:16 - loss: 0.6760 - acc: 0.5867
3904/5677 [===================>..........] - ETA: 4:07 - loss: 0.6763 - acc: 0.5863
3968/5677 [===================>..........] - ETA: 3:59 - loss: 0.6766 - acc: 0.5852
4032/5677 [====================>.........] - ETA: 3:50 - loss: 0.6760 - acc: 0.5863
4096/5677 [====================>.........] - ETA: 3:41 - loss: 0.6760 - acc: 0.5857
4160/5677 [====================>.........] - ETA: 3:32 - loss: 0.6760 - acc: 0.5861
4224/5677 [=====================>........] - ETA: 3:23 - loss: 0.6766 - acc: 0.5850
4288/5677 [=====================>........] - ETA: 3:14 - loss: 0.6769 - acc: 0.5844
4352/5677 [=====================>........] - ETA: 3:05 - loss: 0.6767 - acc: 0.5839
4416/5677 [======================>.......] - ETA: 2:56 - loss: 0.6766 - acc: 0.5845
4480/5677 [======================>.......] - ETA: 2:47 - loss: 0.6769 - acc: 0.5844
4544/5677 [=======================>......] - ETA: 2:38 - loss: 0.6770 - acc: 0.5838
4608/5677 [=======================>......] - ETA: 2:29 - loss: 0.6768 - acc: 0.5844
4672/5677 [=======================>......] - ETA: 2:20 - loss: 0.6769 - acc: 0.5835
4736/5677 [========================>.....] - ETA: 2:11 - loss: 0.6773 - acc: 0.5817
4800/5677 [========================>.....] - ETA: 2:03 - loss: 0.6775 - acc: 0.5810
4864/5677 [========================>.....] - ETA: 1:54 - loss: 0.6776 - acc: 0.5810
4928/5677 [=========================>....] - ETA: 1:45 - loss: 0.6776 - acc: 0.5810
4992/5677 [=========================>....] - ETA: 1:36 - loss: 0.6772 - acc: 0.5819
5056/5677 [=========================>....] - ETA: 1:27 - loss: 0.6773 - acc: 0.5811
5120/5677 [==========================>...] - ETA: 1:18 - loss: 0.6770 - acc: 0.5822
5184/5677 [==========================>...] - ETA: 1:09 - loss: 0.6773 - acc: 0.5816
5248/5677 [==========================>...] - ETA: 1:00 - loss: 0.6772 - acc: 0.5819
5312/5677 [===========================>..] - ETA: 51s - loss: 0.6773 - acc: 0.5821 
5376/5677 [===========================>..] - ETA: 42s - loss: 0.6773 - acc: 0.5820
5440/5677 [===========================>..] - ETA: 33s - loss: 0.6772 - acc: 0.5825
5504/5677 [============================>.] - ETA: 24s - loss: 0.6776 - acc: 0.5818
5568/5677 [============================>.] - ETA: 15s - loss: 0.6773 - acc: 0.5824
5632/5677 [============================>.] - ETA: 6s - loss: 0.6776 - acc: 0.5815 
5677/5677 [==============================] - 834s 147ms/step - loss: 0.6774 - acc: 0.5822 - val_loss: 0.6814 - val_acc: 0.5658

Epoch 00009: val_acc did not improve from 0.58320
Epoch 10/10

  64/5677 [..............................] - ETA: 14:25 - loss: 0.6977 - acc: 0.6094
 128/5677 [..............................] - ETA: 13:46 - loss: 0.6867 - acc: 0.5703
 192/5677 [>.............................] - ETA: 13:37 - loss: 0.6850 - acc: 0.5781
 256/5677 [>.............................] - ETA: 13:31 - loss: 0.6824 - acc: 0.5938
 320/5677 [>.............................] - ETA: 13:32 - loss: 0.6725 - acc: 0.6031
 384/5677 [=>............................] - ETA: 13:14 - loss: 0.6742 - acc: 0.5911
 448/5677 [=>............................] - ETA: 12:59 - loss: 0.6768 - acc: 0.5848
 512/5677 [=>............................] - ETA: 12:48 - loss: 0.6757 - acc: 0.5820
 576/5677 [==>...........................] - ETA: 12:39 - loss: 0.6754 - acc: 0.5833
 640/5677 [==>...........................] - ETA: 12:26 - loss: 0.6774 - acc: 0.5766
 704/5677 [==>...........................] - ETA: 12:16 - loss: 0.6810 - acc: 0.5682
 768/5677 [===>..........................] - ETA: 12:06 - loss: 0.6774 - acc: 0.5768
 832/5677 [===>..........................] - ETA: 11:54 - loss: 0.6789 - acc: 0.5733
 896/5677 [===>..........................] - ETA: 11:46 - loss: 0.6798 - acc: 0.5725
 960/5677 [====>.........................] - ETA: 11:38 - loss: 0.6804 - acc: 0.5687
1024/5677 [====>.........................] - ETA: 11:30 - loss: 0.6798 - acc: 0.5674
1088/5677 [====>.........................] - ETA: 11:22 - loss: 0.6816 - acc: 0.5653
1152/5677 [=====>........................] - ETA: 11:14 - loss: 0.6806 - acc: 0.5703
1216/5677 [=====>........................] - ETA: 11:06 - loss: 0.6800 - acc: 0.5707
1280/5677 [=====>........................] - ETA: 10:54 - loss: 0.6803 - acc: 0.5695
1344/5677 [======>.......................] - ETA: 10:44 - loss: 0.6828 - acc: 0.5647
1408/5677 [======>.......................] - ETA: 10:34 - loss: 0.6829 - acc: 0.5653
1472/5677 [======>.......................] - ETA: 10:24 - loss: 0.6818 - acc: 0.5679
1536/5677 [=======>......................] - ETA: 10:15 - loss: 0.6825 - acc: 0.5658
1600/5677 [=======>......................] - ETA: 10:04 - loss: 0.6823 - acc: 0.5637
1664/5677 [=======>......................] - ETA: 9:55 - loss: 0.6825 - acc: 0.5613 
1728/5677 [========>.....................] - ETA: 9:45 - loss: 0.6828 - acc: 0.5596
1792/5677 [========>.....................] - ETA: 9:34 - loss: 0.6830 - acc: 0.5569
1856/5677 [========>.....................] - ETA: 9:23 - loss: 0.6824 - acc: 0.5566
1920/5677 [=========>....................] - ETA: 9:13 - loss: 0.6816 - acc: 0.5594
1984/5677 [=========>....................] - ETA: 9:05 - loss: 0.6809 - acc: 0.5605
2048/5677 [=========>....................] - ETA: 8:55 - loss: 0.6803 - acc: 0.5635
2112/5677 [==========>...................] - ETA: 8:46 - loss: 0.6802 - acc: 0.5639
2176/5677 [==========>...................] - ETA: 8:37 - loss: 0.6787 - acc: 0.5662
2240/5677 [==========>...................] - ETA: 8:26 - loss: 0.6794 - acc: 0.5647
2304/5677 [===========>..................] - ETA: 8:17 - loss: 0.6794 - acc: 0.5664
2368/5677 [===========>..................] - ETA: 8:08 - loss: 0.6808 - acc: 0.5642
2432/5677 [===========>..................] - ETA: 7:59 - loss: 0.6805 - acc: 0.5654
2496/5677 [============>.................] - ETA: 7:49 - loss: 0.6805 - acc: 0.5653
2560/5677 [============>.................] - ETA: 7:40 - loss: 0.6804 - acc: 0.5641
2624/5677 [============>.................] - ETA: 7:30 - loss: 0.6797 - acc: 0.5663
2688/5677 [=============>................] - ETA: 7:20 - loss: 0.6790 - acc: 0.5696
2752/5677 [=============>................] - ETA: 7:10 - loss: 0.6791 - acc: 0.5687
2816/5677 [=============>................] - ETA: 7:00 - loss: 0.6797 - acc: 0.5661
2880/5677 [==============>...............] - ETA: 6:50 - loss: 0.6794 - acc: 0.5656
2944/5677 [==============>...............] - ETA: 6:41 - loss: 0.6794 - acc: 0.5656
3008/5677 [==============>...............] - ETA: 6:32 - loss: 0.6789 - acc: 0.5665
3072/5677 [===============>..............] - ETA: 6:23 - loss: 0.6797 - acc: 0.5638
3136/5677 [===============>..............] - ETA: 6:14 - loss: 0.6794 - acc: 0.5644
3200/5677 [===============>..............] - ETA: 6:05 - loss: 0.6792 - acc: 0.5628
3264/5677 [================>.............] - ETA: 5:56 - loss: 0.6789 - acc: 0.5628
3328/5677 [================>.............] - ETA: 5:46 - loss: 0.6786 - acc: 0.5640
3392/5677 [================>.............] - ETA: 5:37 - loss: 0.6778 - acc: 0.5649
3456/5677 [=================>............] - ETA: 5:28 - loss: 0.6781 - acc: 0.5648
3520/5677 [=================>............] - ETA: 5:19 - loss: 0.6776 - acc: 0.5662
3584/5677 [=================>............] - ETA: 5:09 - loss: 0.6776 - acc: 0.5664
3648/5677 [==================>...........] - ETA: 5:00 - loss: 0.6772 - acc: 0.5674
3712/5677 [==================>...........] - ETA: 4:51 - loss: 0.6768 - acc: 0.5679
3776/5677 [==================>...........] - ETA: 4:41 - loss: 0.6765 - acc: 0.5686
3840/5677 [===================>..........] - ETA: 4:31 - loss: 0.6765 - acc: 0.5687
3904/5677 [===================>..........] - ETA: 4:21 - loss: 0.6766 - acc: 0.5694
3968/5677 [===================>..........] - ETA: 4:12 - loss: 0.6763 - acc: 0.5701
4032/5677 [====================>.........] - ETA: 4:02 - loss: 0.6769 - acc: 0.5692
4096/5677 [====================>.........] - ETA: 3:53 - loss: 0.6768 - acc: 0.5698
4160/5677 [====================>.........] - ETA: 3:43 - loss: 0.6770 - acc: 0.5695
4224/5677 [=====================>........] - ETA: 3:33 - loss: 0.6770 - acc: 0.5698
4288/5677 [=====================>........] - ETA: 3:24 - loss: 0.6779 - acc: 0.5693
4352/5677 [=====================>........] - ETA: 3:14 - loss: 0.6778 - acc: 0.5687
4416/5677 [======================>.......] - ETA: 3:04 - loss: 0.6779 - acc: 0.5682
4480/5677 [======================>.......] - ETA: 2:55 - loss: 0.6772 - acc: 0.5694
4544/5677 [=======================>......] - ETA: 2:46 - loss: 0.6774 - acc: 0.5693
4608/5677 [=======================>......] - ETA: 2:36 - loss: 0.6777 - acc: 0.5688
4672/5677 [=======================>......] - ETA: 2:27 - loss: 0.6777 - acc: 0.5698
4736/5677 [========================>.....] - ETA: 2:17 - loss: 0.6776 - acc: 0.5701
4800/5677 [========================>.....] - ETA: 2:08 - loss: 0.6770 - acc: 0.5721
4864/5677 [========================>.....] - ETA: 1:59 - loss: 0.6766 - acc: 0.5728
4928/5677 [=========================>....] - ETA: 1:49 - loss: 0.6768 - acc: 0.5728
4992/5677 [=========================>....] - ETA: 1:39 - loss: 0.6765 - acc: 0.5733
5056/5677 [=========================>....] - ETA: 1:30 - loss: 0.6771 - acc: 0.5722
5120/5677 [==========================>...] - ETA: 1:21 - loss: 0.6775 - acc: 0.5715
5184/5677 [==========================>...] - ETA: 1:11 - loss: 0.6773 - acc: 0.5720
5248/5677 [==========================>...] - ETA: 1:02 - loss: 0.6772 - acc: 0.5728
5312/5677 [===========================>..] - ETA: 53s - loss: 0.6777 - acc: 0.5717 
5376/5677 [===========================>..] - ETA: 43s - loss: 0.6770 - acc: 0.5733
5440/5677 [===========================>..] - ETA: 34s - loss: 0.6767 - acc: 0.5735
5504/5677 [============================>.] - ETA: 25s - loss: 0.6763 - acc: 0.5747
5568/5677 [============================>.] - ETA: 15s - loss: 0.6764 - acc: 0.5745
5632/5677 [============================>.] - ETA: 6s - loss: 0.6761 - acc: 0.5755 
5677/5677 [==============================] - 859s 151ms/step - loss: 0.6757 - acc: 0.5758 - val_loss: 0.6728 - val_acc: 0.5911

Epoch 00010: val_acc improved from 0.58320 to 0.59113, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window04/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f1df035bcd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f1df035bcd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f1df0310d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f1df0310d10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1b303e3790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1b303e3790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1df026fc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1df026fc50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1de8614e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1de8614e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1de85ce610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1de85ce610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1df026f8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1df026f8d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1de837b250>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1de837b250>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1de8268a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1de8268a10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1de84c3110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1de84c3110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1de83fb3d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1de83fb3d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1de8268ad0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1de8268ad0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1de84835d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1de84835d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f19f420fed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f19f420fed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1de806af10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1de806af10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1de806d810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1de806d810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1de856d0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1de856d0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1de80ee9d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1de80ee9d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1db4561790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1db4561790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1de80d2e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1de80d2e10>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1db458a210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1db458a210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1de85cad90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1de85cad90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cb8554810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cb8554810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1cb8368e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1cb8368e90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1cb81c0950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1cb81c0950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cb82e96d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cb82e96d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1cb8273410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1cb8273410>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cb80f2990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cb80f2990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1cb07e7c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1cb07e7c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1cb80f2950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1cb80f2950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cb8281b50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cb8281b50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1cb06ce650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1cb06ce650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cb0585150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cb0585150>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1cb0494750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1cb0494750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1cb049c190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1cb049c190>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cb051c790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cb051c790>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1cb03f2750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1cb03f2750>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cb037b190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cb037b190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1cb019f610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1cb019f610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1cb012e610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1cb012e610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c706fba50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c706fba50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1cb037be50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1cb037be50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cb004cfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1cb004cfd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1c70614750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1c70614750>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1b30465f90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1b30465f90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e301a21d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1e301a21d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1c706f2710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1c706f2710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c70617650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c70617650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1c703489d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1c703489d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1c70311610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1c70311610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c70309590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c70309590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1c70348710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1c70348710>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c702e8d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c702e8d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1c70041110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1c70041110>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1c70256790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1c70256790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c505f0a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c505f0a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1c703c6fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1c703c6fd0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c70256110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c70256110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1c50487c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1c50487c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1c504847d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1c504847d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c502a2d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c502a2d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1c50737650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1c50737650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c503e8e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c503e8e90>>: AttributeError: module 'gast' has no attribute 'Str'
01window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 8:41
 128/1578 [=>............................] - ETA: 4:54
 192/1578 [==>...........................] - ETA: 3:37
 256/1578 [===>..........................] - ETA: 2:53
 320/1578 [=====>........................] - ETA: 2:26
 384/1578 [======>.......................] - ETA: 2:06
 448/1578 [=======>......................] - ETA: 1:53
 512/1578 [========>.....................] - ETA: 1:40
 576/1578 [=========>....................] - ETA: 1:30
 640/1578 [===========>..................] - ETA: 1:21
 704/1578 [============>.................] - ETA: 1:13
 768/1578 [=============>................] - ETA: 1:05
 832/1578 [==============>...............] - ETA: 59s 
 896/1578 [================>.............] - ETA: 53s
 960/1578 [=================>............] - ETA: 47s
1024/1578 [==================>...........] - ETA: 41s
1088/1578 [===================>..........] - ETA: 36s
1152/1578 [====================>.........] - ETA: 31s
1216/1578 [======================>.......] - ETA: 26s
1280/1578 [=======================>......] - ETA: 21s
1344/1578 [========================>.....] - ETA: 16s
1408/1578 [=========================>....] - ETA: 11s
1472/1578 [==========================>...] - ETA: 7s 
1536/1578 [============================>.] - ETA: 2s
1578/1578 [==============================] - 109s 69ms/step
loss: 0.6718070538778269
acc: 0.5823827628400389
样本个数 3154
样本个数 6308
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f19305f8610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f19305f8610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f1930582e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f1930582e50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1ee01d7a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1ee01d7a50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1c101e81d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1c101e81d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1bf8679950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1bf8679950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f19305b4a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f19305b4a90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1c101e8310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1c101e8310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c10108c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c10108c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1df0330210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1df0330210>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1df03303d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1df03303d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1df0271b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1df0271b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1c101f94d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1c101f94d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1df0275810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1df0275810>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1df01c1d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1df01c1d50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f19302dc290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f19302dc290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f193034af90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f193034af90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1df01c8e50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1df01c8e50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1df00d4550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1df00d4550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f193008c610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f193008c610>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f19106d2fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f19106d2fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f19107a27d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f19107a27d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1930161690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1930161690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f19105f16d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f19105f16d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f19104d92d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f19104d92d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f191059f490>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f191059f490>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f191049df50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f191049df50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f19103c4190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f19103c4190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f19104d9d10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f19104d9d10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f19101806d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f19101806d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1910452dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1910452dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c504cc190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c504cc190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1910180e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1910180e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f191008ded0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f191008ded0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f18f064af50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f18f064af50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f18f0613a50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f18f0613a50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f18f06522d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f18f06522d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1910176310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1910176310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f18f06b8c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f18f06b8c50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f18f035ea90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f18f035ea90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f18f0372810>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f18f0372810>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f18f01039d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f18f01039d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f18f053e510>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f18f053e510>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f18f0393210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f18f0393210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f18f00fc910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f18f00fc910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f18f00d9c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f18f00d9c90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f18cc734690>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f18cc734690>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f18f00fc2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f18f00fc2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f18f00d9f50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f18f00d9f50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f18cc5e9910>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f18cc5e9910>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f18cc38f350>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f18cc38f350>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f18cc4802d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f18cc4802d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f18cc5de2d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f18cc5de2d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f18f00a7650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f18f00a7650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f17f020cc90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f17f020cc90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f17f01785d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f17f01785d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f17e07ecf10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f17e07ecf10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f18cc45ee10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f18cc45ee10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f17f025b390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f17f025b390>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f17e07ec5d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f17e07ec5d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f17e05cc850>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f17e05cc850>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f17e06b5f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f17e06b5f10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f17f02b52d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f17f02b52d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f17e0488b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f17e0488b10>>: AttributeError: module 'gast' has no attribute 'Str'
Fitting first model...
Train on 5677 samples, validate on 631 samples
Epoch 1/10

  64/5677 [..............................] - ETA: 1:42:14 - loss: 0.6558 - acc: 0.6562
 128/5677 [..............................] - ETA: 59:20 - loss: 0.8770 - acc: 0.5391  
 192/5677 [>.............................] - ETA: 44:29 - loss: 0.8713 - acc: 0.5156
 256/5677 [>.............................] - ETA: 37:08 - loss: 0.8553 - acc: 0.5156
 320/5677 [>.............................] - ETA: 32:26 - loss: 0.8346 - acc: 0.5188
 384/5677 [=>............................] - ETA: 29:26 - loss: 0.8332 - acc: 0.5078
 448/5677 [=>............................] - ETA: 26:59 - loss: 0.8153 - acc: 0.5179
 512/5677 [=>............................] - ETA: 25:02 - loss: 0.7990 - acc: 0.5234
 576/5677 [==>...........................] - ETA: 23:36 - loss: 0.7901 - acc: 0.5260
 640/5677 [==>...........................] - ETA: 22:18 - loss: 0.7832 - acc: 0.5156
 704/5677 [==>...........................] - ETA: 21:19 - loss: 0.7819 - acc: 0.5099
 768/5677 [===>..........................] - ETA: 20:20 - loss: 0.7787 - acc: 0.5052
 832/5677 [===>..........................] - ETA: 19:44 - loss: 0.7750 - acc: 0.5012
 896/5677 [===>..........................] - ETA: 19:01 - loss: 0.7752 - acc: 0.4967
 960/5677 [====>.........................] - ETA: 18:27 - loss: 0.7705 - acc: 0.4990
1024/5677 [====>.........................] - ETA: 17:47 - loss: 0.7684 - acc: 0.4951
1088/5677 [====>.........................] - ETA: 17:09 - loss: 0.7642 - acc: 0.4963
1152/5677 [=====>........................] - ETA: 16:41 - loss: 0.7631 - acc: 0.4965
1216/5677 [=====>........................] - ETA: 16:16 - loss: 0.7595 - acc: 0.5000
1280/5677 [=====>........................] - ETA: 15:51 - loss: 0.7564 - acc: 0.5023
1344/5677 [======>.......................] - ETA: 15:26 - loss: 0.7536 - acc: 0.5060
1408/5677 [======>.......................] - ETA: 15:03 - loss: 0.7516 - acc: 0.5071
1472/5677 [======>.......................] - ETA: 14:42 - loss: 0.7501 - acc: 0.5075
1536/5677 [=======>......................] - ETA: 14:25 - loss: 0.7494 - acc: 0.5059
1600/5677 [=======>......................] - ETA: 14:03 - loss: 0.7493 - acc: 0.5038
1664/5677 [=======>......................] - ETA: 13:42 - loss: 0.7492 - acc: 0.5036
1728/5677 [========>.....................] - ETA: 13:23 - loss: 0.7489 - acc: 0.5017
1792/5677 [========>.....................] - ETA: 13:03 - loss: 0.7462 - acc: 0.5061
1856/5677 [========>.....................] - ETA: 12:45 - loss: 0.7449 - acc: 0.5065
1920/5677 [=========>....................] - ETA: 12:27 - loss: 0.7442 - acc: 0.5078
1984/5677 [=========>....................] - ETA: 12:10 - loss: 0.7438 - acc: 0.5086
2048/5677 [=========>....................] - ETA: 11:52 - loss: 0.7427 - acc: 0.5083
2112/5677 [==========>...................] - ETA: 11:36 - loss: 0.7415 - acc: 0.5095
2176/5677 [==========>...................] - ETA: 11:21 - loss: 0.7414 - acc: 0.5087
2240/5677 [==========>...................] - ETA: 11:06 - loss: 0.7405 - acc: 0.5094
2304/5677 [===========>..................] - ETA: 10:51 - loss: 0.7405 - acc: 0.5074
2368/5677 [===========>..................] - ETA: 10:36 - loss: 0.7392 - acc: 0.5076
2432/5677 [===========>..................] - ETA: 10:23 - loss: 0.7381 - acc: 0.5086
2496/5677 [============>.................] - ETA: 10:08 - loss: 0.7370 - acc: 0.5104
2560/5677 [============>.................] - ETA: 9:54 - loss: 0.7367 - acc: 0.5078 
2624/5677 [============>.................] - ETA: 9:41 - loss: 0.7367 - acc: 0.5072
2688/5677 [=============>................] - ETA: 9:27 - loss: 0.7376 - acc: 0.5060
2752/5677 [=============>................] - ETA: 9:14 - loss: 0.7368 - acc: 0.5062
2816/5677 [=============>................] - ETA: 9:01 - loss: 0.7364 - acc: 0.5075
2880/5677 [==============>...............] - ETA: 8:47 - loss: 0.7364 - acc: 0.5080
2944/5677 [==============>...............] - ETA: 8:33 - loss: 0.7359 - acc: 0.5085
3008/5677 [==============>...............] - ETA: 8:20 - loss: 0.7359 - acc: 0.5080
3072/5677 [===============>..............] - ETA: 8:07 - loss: 0.7355 - acc: 0.5072
3136/5677 [===============>..............] - ETA: 7:54 - loss: 0.7358 - acc: 0.5067
3200/5677 [===============>..............] - ETA: 7:40 - loss: 0.7360 - acc: 0.5034
3264/5677 [================>.............] - ETA: 7:27 - loss: 0.7349 - acc: 0.5043
3328/5677 [================>.............] - ETA: 7:15 - loss: 0.7344 - acc: 0.5039
3392/5677 [================>.............] - ETA: 7:02 - loss: 0.7333 - acc: 0.5050
3456/5677 [=================>............] - ETA: 6:49 - loss: 0.7338 - acc: 0.5032
3520/5677 [=================>............] - ETA: 6:36 - loss: 0.7326 - acc: 0.5045
3584/5677 [=================>............] - ETA: 6:23 - loss: 0.7318 - acc: 0.5053
3648/5677 [==================>...........] - ETA: 6:11 - loss: 0.7320 - acc: 0.5041
3712/5677 [==================>...........] - ETA: 5:59 - loss: 0.7314 - acc: 0.5048
3776/5677 [==================>...........] - ETA: 5:47 - loss: 0.7314 - acc: 0.5040
3840/5677 [===================>..........] - ETA: 5:35 - loss: 0.7308 - acc: 0.5049
3904/5677 [===================>..........] - ETA: 5:23 - loss: 0.7306 - acc: 0.5041
3968/5677 [===================>..........] - ETA: 5:11 - loss: 0.7300 - acc: 0.5055
4032/5677 [====================>.........] - ETA: 4:58 - loss: 0.7289 - acc: 0.5082
4096/5677 [====================>.........] - ETA: 4:46 - loss: 0.7285 - acc: 0.5078
4160/5677 [====================>.........] - ETA: 4:35 - loss: 0.7276 - acc: 0.5087
4224/5677 [=====================>........] - ETA: 4:23 - loss: 0.7275 - acc: 0.5085
4288/5677 [=====================>........] - ETA: 4:11 - loss: 0.7267 - acc: 0.5100
4352/5677 [=====================>........] - ETA: 4:00 - loss: 0.7265 - acc: 0.5103
4416/5677 [======================>.......] - ETA: 3:48 - loss: 0.7262 - acc: 0.5113
4480/5677 [======================>.......] - ETA: 3:36 - loss: 0.7259 - acc: 0.5109
4544/5677 [=======================>......] - ETA: 3:24 - loss: 0.7256 - acc: 0.5106
4608/5677 [=======================>......] - ETA: 3:12 - loss: 0.7254 - acc: 0.5106
4672/5677 [=======================>......] - ETA: 3:00 - loss: 0.7247 - acc: 0.5113
4736/5677 [========================>.....] - ETA: 2:49 - loss: 0.7248 - acc: 0.5114
4800/5677 [========================>.....] - ETA: 2:37 - loss: 0.7253 - acc: 0.5110
4864/5677 [========================>.....] - ETA: 2:25 - loss: 0.7253 - acc: 0.5109
4928/5677 [=========================>....] - ETA: 2:13 - loss: 0.7251 - acc: 0.5116
4992/5677 [=========================>....] - ETA: 2:02 - loss: 0.7245 - acc: 0.5116
5056/5677 [=========================>....] - ETA: 1:51 - loss: 0.7241 - acc: 0.5117
5120/5677 [==========================>...] - ETA: 1:39 - loss: 0.7236 - acc: 0.5117
5184/5677 [==========================>...] - ETA: 1:27 - loss: 0.7234 - acc: 0.5110
5248/5677 [==========================>...] - ETA: 1:16 - loss: 0.7234 - acc: 0.5097
5312/5677 [===========================>..] - ETA: 1:04 - loss: 0.7231 - acc: 0.5104
5376/5677 [===========================>..] - ETA: 53s - loss: 0.7226 - acc: 0.5108 
5440/5677 [===========================>..] - ETA: 42s - loss: 0.7223 - acc: 0.5112
5504/5677 [============================>.] - ETA: 30s - loss: 0.7218 - acc: 0.5127
5568/5677 [============================>.] - ETA: 19s - loss: 0.7217 - acc: 0.5119
5632/5677 [============================>.] - ETA: 7s - loss: 0.7215 - acc: 0.5119 
5677/5677 [==============================] - 1051s 185ms/step - loss: 0.7214 - acc: 0.5119 - val_loss: 0.7003 - val_acc: 0.4643

Epoch 00001: val_acc improved from -inf to 0.46434, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window05/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 2/10

  64/5677 [..............................] - ETA: 15:47 - loss: 0.6752 - acc: 0.5781
 128/5677 [..............................] - ETA: 15:42 - loss: 0.6882 - acc: 0.5391
 192/5677 [>.............................] - ETA: 15:47 - loss: 0.6910 - acc: 0.5260
 256/5677 [>.............................] - ETA: 15:10 - loss: 0.7005 - acc: 0.4805
 320/5677 [>.............................] - ETA: 15:12 - loss: 0.7001 - acc: 0.4875
 384/5677 [=>............................] - ETA: 14:54 - loss: 0.7066 - acc: 0.4792
 448/5677 [=>............................] - ETA: 14:40 - loss: 0.7071 - acc: 0.4754
 512/5677 [=>............................] - ETA: 14:22 - loss: 0.7058 - acc: 0.4766
 576/5677 [==>...........................] - ETA: 14:07 - loss: 0.7047 - acc: 0.4774
 640/5677 [==>...........................] - ETA: 13:42 - loss: 0.7034 - acc: 0.4859
 704/5677 [==>...........................] - ETA: 13:26 - loss: 0.7039 - acc: 0.4901
 768/5677 [===>..........................] - ETA: 13:11 - loss: 0.7026 - acc: 0.5000
 832/5677 [===>..........................] - ETA: 12:58 - loss: 0.7029 - acc: 0.4964
 896/5677 [===>..........................] - ETA: 12:42 - loss: 0.7022 - acc: 0.5045
 960/5677 [====>.........................] - ETA: 12:29 - loss: 0.7030 - acc: 0.5094
1024/5677 [====>.........................] - ETA: 12:16 - loss: 0.7022 - acc: 0.5117
1088/5677 [====>.........................] - ETA: 12:08 - loss: 0.7014 - acc: 0.5147
1152/5677 [=====>........................] - ETA: 11:56 - loss: 0.7027 - acc: 0.5061
1216/5677 [=====>........................] - ETA: 11:45 - loss: 0.7027 - acc: 0.5049
1280/5677 [=====>........................] - ETA: 11:36 - loss: 0.7046 - acc: 0.5023
1344/5677 [======>.......................] - ETA: 11:24 - loss: 0.7054 - acc: 0.5007
1408/5677 [======>.......................] - ETA: 11:12 - loss: 0.7049 - acc: 0.4993
1472/5677 [======>.......................] - ETA: 11:04 - loss: 0.7037 - acc: 0.5000
1536/5677 [=======>......................] - ETA: 10:56 - loss: 0.7053 - acc: 0.5007
1600/5677 [=======>......................] - ETA: 10:47 - loss: 0.7042 - acc: 0.5019
1664/5677 [=======>......................] - ETA: 10:38 - loss: 0.7044 - acc: 0.5006
1728/5677 [========>.....................] - ETA: 10:28 - loss: 0.7043 - acc: 0.5006
1792/5677 [========>.....................] - ETA: 10:18 - loss: 0.7052 - acc: 0.4983
1856/5677 [========>.....................] - ETA: 10:08 - loss: 0.7049 - acc: 0.5000
1920/5677 [=========>....................] - ETA: 9:59 - loss: 0.7050 - acc: 0.4979 
1984/5677 [=========>....................] - ETA: 9:49 - loss: 0.7038 - acc: 0.4995
2048/5677 [=========>....................] - ETA: 9:38 - loss: 0.7043 - acc: 0.4990
2112/5677 [==========>...................] - ETA: 9:28 - loss: 0.7029 - acc: 0.5033
2176/5677 [==========>...................] - ETA: 9:18 - loss: 0.7021 - acc: 0.5046
2240/5677 [==========>...................] - ETA: 9:09 - loss: 0.7022 - acc: 0.5045
2304/5677 [===========>..................] - ETA: 9:01 - loss: 0.7014 - acc: 0.5061
2368/5677 [===========>..................] - ETA: 8:55 - loss: 0.7005 - acc: 0.5084
2432/5677 [===========>..................] - ETA: 8:44 - loss: 0.7001 - acc: 0.5107
2496/5677 [============>.................] - ETA: 8:34 - loss: 0.7005 - acc: 0.5112
2560/5677 [============>.................] - ETA: 8:25 - loss: 0.7007 - acc: 0.5129
2624/5677 [============>.................] - ETA: 8:17 - loss: 0.7005 - acc: 0.5133
2688/5677 [=============>................] - ETA: 8:06 - loss: 0.7010 - acc: 0.5115
2752/5677 [=============>................] - ETA: 7:56 - loss: 0.7015 - acc: 0.5105
2816/5677 [=============>................] - ETA: 7:46 - loss: 0.7017 - acc: 0.5099
2880/5677 [==============>...............] - ETA: 7:36 - loss: 0.7016 - acc: 0.5090
2944/5677 [==============>...............] - ETA: 7:26 - loss: 0.7022 - acc: 0.5092
3008/5677 [==============>...............] - ETA: 7:15 - loss: 0.7035 - acc: 0.5070
3072/5677 [===============>..............] - ETA: 7:05 - loss: 0.7044 - acc: 0.5049
3136/5677 [===============>..............] - ETA: 6:55 - loss: 0.7042 - acc: 0.5041
3200/5677 [===============>..............] - ETA: 6:45 - loss: 0.7045 - acc: 0.5044
3264/5677 [================>.............] - ETA: 6:35 - loss: 0.7042 - acc: 0.5058
3328/5677 [================>.............] - ETA: 6:25 - loss: 0.7044 - acc: 0.5051
3392/5677 [================>.............] - ETA: 6:15 - loss: 0.7045 - acc: 0.5035
3456/5677 [=================>............] - ETA: 6:04 - loss: 0.7046 - acc: 0.5035
3520/5677 [=================>............] - ETA: 5:54 - loss: 0.7045 - acc: 0.5020
3584/5677 [=================>............] - ETA: 5:42 - loss: 0.7043 - acc: 0.5028
3648/5677 [==================>...........] - ETA: 5:32 - loss: 0.7042 - acc: 0.5022
3712/5677 [==================>...........] - ETA: 5:21 - loss: 0.7041 - acc: 0.5030
3776/5677 [==================>...........] - ETA: 5:11 - loss: 0.7042 - acc: 0.5016
3840/5677 [===================>..........] - ETA: 5:00 - loss: 0.7040 - acc: 0.5039
3904/5677 [===================>..........] - ETA: 4:50 - loss: 0.7042 - acc: 0.5026
3968/5677 [===================>..........] - ETA: 4:39 - loss: 0.7041 - acc: 0.5023
4032/5677 [====================>.........] - ETA: 4:29 - loss: 0.7037 - acc: 0.5035
4096/5677 [====================>.........] - ETA: 4:19 - loss: 0.7038 - acc: 0.5034
4160/5677 [====================>.........] - ETA: 4:08 - loss: 0.7037 - acc: 0.5046
4224/5677 [=====================>........] - ETA: 3:58 - loss: 0.7037 - acc: 0.5047
4288/5677 [=====================>........] - ETA: 3:47 - loss: 0.7037 - acc: 0.5047
4352/5677 [=====================>........] - ETA: 3:37 - loss: 0.7038 - acc: 0.5037
4416/5677 [======================>.......] - ETA: 3:26 - loss: 0.7040 - acc: 0.5029
4480/5677 [======================>.......] - ETA: 3:15 - loss: 0.7038 - acc: 0.5042
4544/5677 [=======================>......] - ETA: 3:05 - loss: 0.7033 - acc: 0.5059
4608/5677 [=======================>......] - ETA: 2:54 - loss: 0.7034 - acc: 0.5052
4672/5677 [=======================>......] - ETA: 2:43 - loss: 0.7032 - acc: 0.5060
4736/5677 [========================>.....] - ETA: 2:33 - loss: 0.7033 - acc: 0.5055
4800/5677 [========================>.....] - ETA: 2:22 - loss: 0.7034 - acc: 0.5046
4864/5677 [========================>.....] - ETA: 2:12 - loss: 0.7037 - acc: 0.5035
4928/5677 [=========================>....] - ETA: 2:01 - loss: 0.7036 - acc: 0.5034
4992/5677 [=========================>....] - ETA: 1:51 - loss: 0.7036 - acc: 0.5026
5056/5677 [=========================>....] - ETA: 1:40 - loss: 0.7035 - acc: 0.5032
5120/5677 [==========================>...] - ETA: 1:30 - loss: 0.7032 - acc: 0.5039
5184/5677 [==========================>...] - ETA: 1:19 - loss: 0.7031 - acc: 0.5044
5248/5677 [==========================>...] - ETA: 1:09 - loss: 0.7034 - acc: 0.5030
5312/5677 [===========================>..] - ETA: 58s - loss: 0.7033 - acc: 0.5028 
5376/5677 [===========================>..] - ETA: 48s - loss: 0.7034 - acc: 0.5022
5440/5677 [===========================>..] - ETA: 38s - loss: 0.7033 - acc: 0.5026
5504/5677 [============================>.] - ETA: 27s - loss: 0.7031 - acc: 0.5031
5568/5677 [============================>.] - ETA: 17s - loss: 0.7027 - acc: 0.5041
5632/5677 [============================>.] - ETA: 7s - loss: 0.7028 - acc: 0.5039 
5677/5677 [==============================] - 948s 167ms/step - loss: 0.7027 - acc: 0.5036 - val_loss: 0.6973 - val_acc: 0.5135

Epoch 00002: val_acc improved from 0.46434 to 0.51347, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window05/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 3/10

  64/5677 [..............................] - ETA: 15:21 - loss: 0.6691 - acc: 0.5625
 128/5677 [..............................] - ETA: 15:11 - loss: 0.6999 - acc: 0.4766
 192/5677 [>.............................] - ETA: 14:47 - loss: 0.6956 - acc: 0.5260
 256/5677 [>.............................] - ETA: 13:56 - loss: 0.6934 - acc: 0.5312
 320/5677 [>.............................] - ETA: 13:59 - loss: 0.6941 - acc: 0.5219
 384/5677 [=>............................] - ETA: 13:48 - loss: 0.6967 - acc: 0.5078
 448/5677 [=>............................] - ETA: 13:44 - loss: 0.6994 - acc: 0.4955
 512/5677 [=>............................] - ETA: 13:27 - loss: 0.7006 - acc: 0.4902
 576/5677 [==>...........................] - ETA: 13:24 - loss: 0.7002 - acc: 0.4878
 640/5677 [==>...........................] - ETA: 13:19 - loss: 0.6982 - acc: 0.4953
 704/5677 [==>...........................] - ETA: 13:14 - loss: 0.7007 - acc: 0.4901
 768/5677 [===>..........................] - ETA: 13:02 - loss: 0.6994 - acc: 0.4844
 832/5677 [===>..........................] - ETA: 12:58 - loss: 0.6987 - acc: 0.4844
 896/5677 [===>..........................] - ETA: 12:47 - loss: 0.6982 - acc: 0.4866
 960/5677 [====>.........................] - ETA: 12:35 - loss: 0.6986 - acc: 0.4885
1024/5677 [====>.........................] - ETA: 12:20 - loss: 0.6987 - acc: 0.4854
1088/5677 [====>.........................] - ETA: 12:11 - loss: 0.6988 - acc: 0.4871
1152/5677 [=====>........................] - ETA: 11:58 - loss: 0.6975 - acc: 0.4931
1216/5677 [=====>........................] - ETA: 11:45 - loss: 0.6973 - acc: 0.4959
1280/5677 [=====>........................] - ETA: 11:36 - loss: 0.6968 - acc: 0.4961
1344/5677 [======>.......................] - ETA: 11:24 - loss: 0.6961 - acc: 0.4985
1408/5677 [======>.......................] - ETA: 11:20 - loss: 0.6955 - acc: 0.5036
1472/5677 [======>.......................] - ETA: 11:09 - loss: 0.6963 - acc: 0.5007
1536/5677 [=======>......................] - ETA: 11:02 - loss: 0.6963 - acc: 0.4993
1600/5677 [=======>......................] - ETA: 10:53 - loss: 0.6979 - acc: 0.4969
1664/5677 [=======>......................] - ETA: 10:42 - loss: 0.6971 - acc: 0.5006
1728/5677 [========>.....................] - ETA: 10:32 - loss: 0.6970 - acc: 0.5023
1792/5677 [========>.....................] - ETA: 10:22 - loss: 0.6977 - acc: 0.4994
1856/5677 [========>.....................] - ETA: 10:13 - loss: 0.6978 - acc: 0.5005
1920/5677 [=========>....................] - ETA: 10:03 - loss: 0.6972 - acc: 0.5021
1984/5677 [=========>....................] - ETA: 9:52 - loss: 0.6967 - acc: 0.5040 
2048/5677 [=========>....................] - ETA: 9:40 - loss: 0.6962 - acc: 0.5068
2112/5677 [==========>...................] - ETA: 9:31 - loss: 0.6961 - acc: 0.5085
2176/5677 [==========>...................] - ETA: 9:20 - loss: 0.6963 - acc: 0.5087
2240/5677 [==========>...................] - ETA: 9:11 - loss: 0.6965 - acc: 0.5076
2304/5677 [===========>..................] - ETA: 9:01 - loss: 0.6973 - acc: 0.5048
2368/5677 [===========>..................] - ETA: 8:49 - loss: 0.6962 - acc: 0.5068
2432/5677 [===========>..................] - ETA: 8:40 - loss: 0.6960 - acc: 0.5066
2496/5677 [============>.................] - ETA: 8:31 - loss: 0.6966 - acc: 0.5064
2560/5677 [============>.................] - ETA: 8:19 - loss: 0.6963 - acc: 0.5090
2624/5677 [============>.................] - ETA: 8:08 - loss: 0.6960 - acc: 0.5088
2688/5677 [=============>................] - ETA: 7:57 - loss: 0.6958 - acc: 0.5082
2752/5677 [=============>................] - ETA: 7:47 - loss: 0.6960 - acc: 0.5073
2816/5677 [=============>................] - ETA: 7:37 - loss: 0.6962 - acc: 0.5075
2880/5677 [==============>...............] - ETA: 7:28 - loss: 0.6964 - acc: 0.5073
2944/5677 [==============>...............] - ETA: 7:18 - loss: 0.6964 - acc: 0.5068
3008/5677 [==============>...............] - ETA: 7:07 - loss: 0.6960 - acc: 0.5093
3072/5677 [===============>..............] - ETA: 6:57 - loss: 0.6961 - acc: 0.5091
3136/5677 [===============>..............] - ETA: 6:46 - loss: 0.6958 - acc: 0.5102
3200/5677 [===============>..............] - ETA: 6:36 - loss: 0.6953 - acc: 0.5122
3264/5677 [================>.............] - ETA: 6:25 - loss: 0.6947 - acc: 0.5144
3328/5677 [================>.............] - ETA: 6:14 - loss: 0.6946 - acc: 0.5153
3392/5677 [================>.............] - ETA: 6:04 - loss: 0.6942 - acc: 0.5165
3456/5677 [=================>............] - ETA: 5:55 - loss: 0.6940 - acc: 0.5182
3520/5677 [=================>............] - ETA: 5:45 - loss: 0.6937 - acc: 0.5196
3584/5677 [=================>............] - ETA: 5:35 - loss: 0.6941 - acc: 0.5187
3648/5677 [==================>...........] - ETA: 5:24 - loss: 0.6939 - acc: 0.5181
3712/5677 [==================>...........] - ETA: 5:13 - loss: 0.6939 - acc: 0.5180
3776/5677 [==================>...........] - ETA: 5:03 - loss: 0.6937 - acc: 0.5185
3840/5677 [===================>..........] - ETA: 4:53 - loss: 0.6940 - acc: 0.5177
3904/5677 [===================>..........] - ETA: 4:42 - loss: 0.6942 - acc: 0.5174
3968/5677 [===================>..........] - ETA: 4:32 - loss: 0.6938 - acc: 0.5181
4032/5677 [====================>.........] - ETA: 4:22 - loss: 0.6940 - acc: 0.5184
4096/5677 [====================>.........] - ETA: 4:11 - loss: 0.6939 - acc: 0.5178
4160/5677 [====================>.........] - ETA: 4:02 - loss: 0.6937 - acc: 0.5200
4224/5677 [=====================>........] - ETA: 3:51 - loss: 0.6938 - acc: 0.5201
4288/5677 [=====================>........] - ETA: 3:41 - loss: 0.6940 - acc: 0.5201
4352/5677 [=====================>........] - ETA: 3:31 - loss: 0.6937 - acc: 0.5209
4416/5677 [======================>.......] - ETA: 3:21 - loss: 0.6936 - acc: 0.5211
4480/5677 [======================>.......] - ETA: 3:10 - loss: 0.6932 - acc: 0.5219
4544/5677 [=======================>......] - ETA: 3:00 - loss: 0.6931 - acc: 0.5224
4608/5677 [=======================>......] - ETA: 2:50 - loss: 0.6932 - acc: 0.5230
4672/5677 [=======================>......] - ETA: 2:40 - loss: 0.6934 - acc: 0.5218
4736/5677 [========================>.....] - ETA: 2:30 - loss: 0.6936 - acc: 0.5209
4800/5677 [========================>.....] - ETA: 2:19 - loss: 0.6936 - acc: 0.5212
4864/5677 [========================>.....] - ETA: 2:09 - loss: 0.6938 - acc: 0.5218
4928/5677 [=========================>....] - ETA: 1:59 - loss: 0.6935 - acc: 0.5223
4992/5677 [=========================>....] - ETA: 1:49 - loss: 0.6935 - acc: 0.5220
5056/5677 [=========================>....] - ETA: 1:39 - loss: 0.6934 - acc: 0.5233
5120/5677 [==========================>...] - ETA: 1:28 - loss: 0.6934 - acc: 0.5236
5184/5677 [==========================>...] - ETA: 1:18 - loss: 0.6935 - acc: 0.5243
5248/5677 [==========================>...] - ETA: 1:08 - loss: 0.6936 - acc: 0.5244
5312/5677 [===========================>..] - ETA: 58s - loss: 0.6937 - acc: 0.5239 
5376/5677 [===========================>..] - ETA: 47s - loss: 0.6939 - acc: 0.5234
5440/5677 [===========================>..] - ETA: 37s - loss: 0.6940 - acc: 0.5226
5504/5677 [============================>.] - ETA: 27s - loss: 0.6938 - acc: 0.5234
5568/5677 [============================>.] - ETA: 17s - loss: 0.6936 - acc: 0.5242
5632/5677 [============================>.] - ETA: 7s - loss: 0.6933 - acc: 0.5245 
5677/5677 [==============================] - 933s 164ms/step - loss: 0.6931 - acc: 0.5251 - val_loss: 0.6877 - val_acc: 0.5563

Epoch 00003: val_acc improved from 0.51347 to 0.55626, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window05/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 4/10

  64/5677 [..............................] - ETA: 13:53 - loss: 0.7240 - acc: 0.4844
 128/5677 [..............................] - ETA: 14:08 - loss: 0.6948 - acc: 0.5312
 192/5677 [>.............................] - ETA: 13:35 - loss: 0.7026 - acc: 0.5260
 256/5677 [>.............................] - ETA: 13:39 - loss: 0.6989 - acc: 0.5312
 320/5677 [>.............................] - ETA: 13:31 - loss: 0.6950 - acc: 0.5375
 384/5677 [=>............................] - ETA: 13:37 - loss: 0.6927 - acc: 0.5365
 448/5677 [=>............................] - ETA: 13:20 - loss: 0.6925 - acc: 0.5357
 512/5677 [=>............................] - ETA: 13:09 - loss: 0.6907 - acc: 0.5410
 576/5677 [==>...........................] - ETA: 12:57 - loss: 0.6910 - acc: 0.5347
 640/5677 [==>...........................] - ETA: 12:39 - loss: 0.6930 - acc: 0.5297
 704/5677 [==>...........................] - ETA: 12:38 - loss: 0.6910 - acc: 0.5327
 768/5677 [===>..........................] - ETA: 12:27 - loss: 0.6923 - acc: 0.5260
 832/5677 [===>..........................] - ETA: 12:15 - loss: 0.6933 - acc: 0.5228
 896/5677 [===>..........................] - ETA: 12:08 - loss: 0.6937 - acc: 0.5201
 960/5677 [====>.........................] - ETA: 11:59 - loss: 0.6928 - acc: 0.5240
1024/5677 [====>.........................] - ETA: 11:48 - loss: 0.6937 - acc: 0.5225
1088/5677 [====>.........................] - ETA: 11:45 - loss: 0.6947 - acc: 0.5193
1152/5677 [=====>........................] - ETA: 11:36 - loss: 0.6938 - acc: 0.5234
1216/5677 [=====>........................] - ETA: 11:29 - loss: 0.6927 - acc: 0.5255
1280/5677 [=====>........................] - ETA: 11:22 - loss: 0.6936 - acc: 0.5234
1344/5677 [======>.......................] - ETA: 11:14 - loss: 0.6938 - acc: 0.5216
1408/5677 [======>.......................] - ETA: 11:00 - loss: 0.6958 - acc: 0.5163
1472/5677 [======>.......................] - ETA: 10:53 - loss: 0.6966 - acc: 0.5149
1536/5677 [=======>......................] - ETA: 10:45 - loss: 0.6956 - acc: 0.5202
1600/5677 [=======>......................] - ETA: 10:35 - loss: 0.6958 - acc: 0.5206
1664/5677 [=======>......................] - ETA: 10:26 - loss: 0.6954 - acc: 0.5192
1728/5677 [========>.....................] - ETA: 10:15 - loss: 0.6954 - acc: 0.5208
1792/5677 [========>.....................] - ETA: 10:05 - loss: 0.6937 - acc: 0.5262
1856/5677 [========>.....................] - ETA: 9:56 - loss: 0.6942 - acc: 0.5242 
1920/5677 [=========>....................] - ETA: 9:45 - loss: 0.6944 - acc: 0.5255
1984/5677 [=========>....................] - ETA: 9:35 - loss: 0.6936 - acc: 0.5267
2048/5677 [=========>....................] - ETA: 9:27 - loss: 0.6934 - acc: 0.5278
2112/5677 [==========>...................] - ETA: 9:18 - loss: 0.6935 - acc: 0.5270
2176/5677 [==========>...................] - ETA: 9:08 - loss: 0.6930 - acc: 0.5262
2240/5677 [==========>...................] - ETA: 8:59 - loss: 0.6935 - acc: 0.5250
2304/5677 [===========>..................] - ETA: 8:48 - loss: 0.6924 - acc: 0.5273
2368/5677 [===========>..................] - ETA: 8:40 - loss: 0.6916 - acc: 0.5304
2432/5677 [===========>..................] - ETA: 8:29 - loss: 0.6920 - acc: 0.5300
2496/5677 [============>.................] - ETA: 8:18 - loss: 0.6923 - acc: 0.5296
2560/5677 [============>.................] - ETA: 8:09 - loss: 0.6920 - acc: 0.5312
2624/5677 [============>.................] - ETA: 7:59 - loss: 0.6922 - acc: 0.5309
2688/5677 [=============>................] - ETA: 7:49 - loss: 0.6922 - acc: 0.5294
2752/5677 [=============>................] - ETA: 7:38 - loss: 0.6919 - acc: 0.5287
2816/5677 [=============>................] - ETA: 7:28 - loss: 0.6920 - acc: 0.5284
2880/5677 [==============>...............] - ETA: 7:18 - loss: 0.6919 - acc: 0.5285
2944/5677 [==============>...............] - ETA: 7:09 - loss: 0.6923 - acc: 0.5285
3008/5677 [==============>...............] - ETA: 6:59 - loss: 0.6920 - acc: 0.5289
3072/5677 [===============>..............] - ETA: 6:49 - loss: 0.6918 - acc: 0.5303
3136/5677 [===============>..............] - ETA: 6:39 - loss: 0.6912 - acc: 0.5328
3200/5677 [===============>..............] - ETA: 6:30 - loss: 0.6921 - acc: 0.5316
3264/5677 [================>.............] - ETA: 6:20 - loss: 0.6920 - acc: 0.5319
3328/5677 [================>.............] - ETA: 6:10 - loss: 0.6921 - acc: 0.5325
3392/5677 [================>.............] - ETA: 6:00 - loss: 0.6922 - acc: 0.5336
3456/5677 [=================>............] - ETA: 5:49 - loss: 0.6921 - acc: 0.5333
3520/5677 [=================>............] - ETA: 5:39 - loss: 0.6918 - acc: 0.5349
3584/5677 [=================>............] - ETA: 5:29 - loss: 0.6918 - acc: 0.5349
3648/5677 [==================>...........] - ETA: 5:18 - loss: 0.6915 - acc: 0.5345
3712/5677 [==================>...........] - ETA: 5:09 - loss: 0.6914 - acc: 0.5348
3776/5677 [==================>...........] - ETA: 4:59 - loss: 0.6915 - acc: 0.5339
3840/5677 [===================>..........] - ETA: 4:48 - loss: 0.6917 - acc: 0.5331
3904/5677 [===================>..........] - ETA: 4:39 - loss: 0.6916 - acc: 0.5336
3968/5677 [===================>..........] - ETA: 4:29 - loss: 0.6913 - acc: 0.5345
4032/5677 [====================>.........] - ETA: 4:19 - loss: 0.6912 - acc: 0.5350
4096/5677 [====================>.........] - ETA: 4:10 - loss: 0.6913 - acc: 0.5342
4160/5677 [====================>.........] - ETA: 3:59 - loss: 0.6913 - acc: 0.5349
4224/5677 [=====================>........] - ETA: 3:49 - loss: 0.6911 - acc: 0.5355
4288/5677 [=====================>........] - ETA: 3:39 - loss: 0.6904 - acc: 0.5375
4352/5677 [=====================>........] - ETA: 3:29 - loss: 0.6906 - acc: 0.5368
4416/5677 [======================>.......] - ETA: 3:19 - loss: 0.6914 - acc: 0.5344
4480/5677 [======================>.......] - ETA: 3:09 - loss: 0.6912 - acc: 0.5348
4544/5677 [=======================>......] - ETA: 3:00 - loss: 0.6914 - acc: 0.5346
4608/5677 [=======================>......] - ETA: 2:50 - loss: 0.6912 - acc: 0.5343
4672/5677 [=======================>......] - ETA: 2:40 - loss: 0.6911 - acc: 0.5340
4736/5677 [========================>.....] - ETA: 2:29 - loss: 0.6907 - acc: 0.5351
4800/5677 [========================>.....] - ETA: 2:19 - loss: 0.6907 - acc: 0.5354
4864/5677 [========================>.....] - ETA: 2:09 - loss: 0.6906 - acc: 0.5358
4928/5677 [=========================>....] - ETA: 1:59 - loss: 0.6905 - acc: 0.5365
4992/5677 [=========================>....] - ETA: 1:49 - loss: 0.6904 - acc: 0.5377
5056/5677 [=========================>....] - ETA: 1:38 - loss: 0.6903 - acc: 0.5378
5120/5677 [==========================>...] - ETA: 1:28 - loss: 0.6902 - acc: 0.5377
5184/5677 [==========================>...] - ETA: 1:18 - loss: 0.6903 - acc: 0.5386
5248/5677 [==========================>...] - ETA: 1:08 - loss: 0.6901 - acc: 0.5393
5312/5677 [===========================>..] - ETA: 58s - loss: 0.6900 - acc: 0.5395 
5376/5677 [===========================>..] - ETA: 47s - loss: 0.6902 - acc: 0.5387
5440/5677 [===========================>..] - ETA: 37s - loss: 0.6896 - acc: 0.5406
5504/5677 [============================>.] - ETA: 27s - loss: 0.6896 - acc: 0.5403
5568/5677 [============================>.] - ETA: 17s - loss: 0.6895 - acc: 0.5408
5632/5677 [============================>.] - ETA: 7s - loss: 0.6894 - acc: 0.5412 
5677/5677 [==============================] - 934s 164ms/step - loss: 0.6893 - acc: 0.5417 - val_loss: 0.6871 - val_acc: 0.5420

Epoch 00004: val_acc did not improve from 0.55626
Epoch 5/10

  64/5677 [..............................] - ETA: 14:38 - loss: 0.7016 - acc: 0.4844
 128/5677 [..............................] - ETA: 14:03 - loss: 0.6884 - acc: 0.5312
 192/5677 [>.............................] - ETA: 14:14 - loss: 0.6875 - acc: 0.5365
 256/5677 [>.............................] - ETA: 13:56 - loss: 0.6835 - acc: 0.5352
 320/5677 [>.............................] - ETA: 13:32 - loss: 0.6904 - acc: 0.5156
 384/5677 [=>............................] - ETA: 13:29 - loss: 0.6834 - acc: 0.5443
 448/5677 [=>............................] - ETA: 13:24 - loss: 0.6882 - acc: 0.5357
 512/5677 [=>............................] - ETA: 13:22 - loss: 0.6927 - acc: 0.5156
 576/5677 [==>...........................] - ETA: 13:07 - loss: 0.6939 - acc: 0.5156
 640/5677 [==>...........................] - ETA: 12:47 - loss: 0.6923 - acc: 0.5219
 704/5677 [==>...........................] - ETA: 12:35 - loss: 0.6950 - acc: 0.5185
 768/5677 [===>..........................] - ETA: 12:24 - loss: 0.6948 - acc: 0.5234
 832/5677 [===>..........................] - ETA: 12:14 - loss: 0.6941 - acc: 0.5240
 896/5677 [===>..........................] - ETA: 12:05 - loss: 0.6942 - acc: 0.5301
 960/5677 [====>.........................] - ETA: 11:57 - loss: 0.6940 - acc: 0.5292
1024/5677 [====>.........................] - ETA: 11:41 - loss: 0.6929 - acc: 0.5312
1088/5677 [====>.........................] - ETA: 11:27 - loss: 0.6940 - acc: 0.5303
1152/5677 [=====>........................] - ETA: 11:18 - loss: 0.6917 - acc: 0.5373
1216/5677 [=====>........................] - ETA: 11:09 - loss: 0.6928 - acc: 0.5345
1280/5677 [=====>........................] - ETA: 10:57 - loss: 0.6924 - acc: 0.5328
1344/5677 [======>.......................] - ETA: 10:47 - loss: 0.6925 - acc: 0.5335
1408/5677 [======>.......................] - ETA: 10:34 - loss: 0.6897 - acc: 0.5405
1472/5677 [======>.......................] - ETA: 10:26 - loss: 0.6886 - acc: 0.5462
1536/5677 [=======>......................] - ETA: 10:15 - loss: 0.6875 - acc: 0.5508
1600/5677 [=======>......................] - ETA: 10:05 - loss: 0.6882 - acc: 0.5487
1664/5677 [=======>......................] - ETA: 9:54 - loss: 0.6880 - acc: 0.5511 
1728/5677 [========>.....................] - ETA: 9:47 - loss: 0.6877 - acc: 0.5492
1792/5677 [========>.....................] - ETA: 9:37 - loss: 0.6882 - acc: 0.5485
1856/5677 [========>.....................] - ETA: 9:30 - loss: 0.6875 - acc: 0.5517
1920/5677 [=========>....................] - ETA: 9:19 - loss: 0.6868 - acc: 0.5552
1984/5677 [=========>....................] - ETA: 9:11 - loss: 0.6864 - acc: 0.5554
2048/5677 [=========>....................] - ETA: 9:02 - loss: 0.6860 - acc: 0.5562
2112/5677 [==========>...................] - ETA: 8:53 - loss: 0.6849 - acc: 0.5573
2176/5677 [==========>...................] - ETA: 8:44 - loss: 0.6844 - acc: 0.5584
2240/5677 [==========>...................] - ETA: 8:35 - loss: 0.6852 - acc: 0.5563
2304/5677 [===========>..................] - ETA: 8:26 - loss: 0.6861 - acc: 0.5521
2368/5677 [===========>..................] - ETA: 8:17 - loss: 0.6867 - acc: 0.5511
2432/5677 [===========>..................] - ETA: 8:06 - loss: 0.6853 - acc: 0.5543
2496/5677 [============>.................] - ETA: 7:57 - loss: 0.6861 - acc: 0.5525
2560/5677 [============>.................] - ETA: 7:47 - loss: 0.6864 - acc: 0.5508
2624/5677 [============>.................] - ETA: 7:36 - loss: 0.6863 - acc: 0.5507
2688/5677 [=============>................] - ETA: 7:26 - loss: 0.6865 - acc: 0.5495
2752/5677 [=============>................] - ETA: 7:16 - loss: 0.6864 - acc: 0.5494
2816/5677 [=============>................] - ETA: 7:07 - loss: 0.6867 - acc: 0.5494
2880/5677 [==============>...............] - ETA: 6:58 - loss: 0.6870 - acc: 0.5493
2944/5677 [==============>...............] - ETA: 6:49 - loss: 0.6873 - acc: 0.5499
3008/5677 [==============>...............] - ETA: 6:39 - loss: 0.6871 - acc: 0.5512
3072/5677 [===============>..............] - ETA: 6:29 - loss: 0.6864 - acc: 0.5540
3136/5677 [===============>..............] - ETA: 6:19 - loss: 0.6866 - acc: 0.5529
3200/5677 [===============>..............] - ETA: 6:09 - loss: 0.6871 - acc: 0.5503
3264/5677 [================>.............] - ETA: 5:59 - loss: 0.6870 - acc: 0.5502
3328/5677 [================>.............] - ETA: 5:49 - loss: 0.6873 - acc: 0.5505
3392/5677 [================>.............] - ETA: 5:40 - loss: 0.6877 - acc: 0.5492
3456/5677 [=================>............] - ETA: 5:30 - loss: 0.6877 - acc: 0.5486
3520/5677 [=================>............] - ETA: 5:20 - loss: 0.6879 - acc: 0.5497
3584/5677 [=================>............] - ETA: 5:10 - loss: 0.6877 - acc: 0.5499
3648/5677 [==================>...........] - ETA: 5:01 - loss: 0.6874 - acc: 0.5513
3712/5677 [==================>...........] - ETA: 4:51 - loss: 0.6874 - acc: 0.5506
3776/5677 [==================>...........] - ETA: 4:42 - loss: 0.6874 - acc: 0.5506
3840/5677 [===================>..........] - ETA: 4:33 - loss: 0.6876 - acc: 0.5500
3904/5677 [===================>..........] - ETA: 4:23 - loss: 0.6873 - acc: 0.5505
3968/5677 [===================>..........] - ETA: 4:13 - loss: 0.6875 - acc: 0.5494
4032/5677 [====================>.........] - ETA: 4:04 - loss: 0.6876 - acc: 0.5491
4096/5677 [====================>.........] - ETA: 3:54 - loss: 0.6874 - acc: 0.5498
4160/5677 [====================>.........] - ETA: 3:45 - loss: 0.6874 - acc: 0.5488
4224/5677 [=====================>........] - ETA: 3:35 - loss: 0.6873 - acc: 0.5485
4288/5677 [=====================>........] - ETA: 3:26 - loss: 0.6874 - acc: 0.5490
4352/5677 [=====================>........] - ETA: 3:16 - loss: 0.6880 - acc: 0.5476
4416/5677 [======================>.......] - ETA: 3:07 - loss: 0.6882 - acc: 0.5473
4480/5677 [======================>.......] - ETA: 2:57 - loss: 0.6883 - acc: 0.5484
4544/5677 [=======================>......] - ETA: 2:47 - loss: 0.6883 - acc: 0.5475
4608/5677 [=======================>......] - ETA: 2:38 - loss: 0.6886 - acc: 0.5467
4672/5677 [=======================>......] - ETA: 2:28 - loss: 0.6884 - acc: 0.5482
4736/5677 [========================>.....] - ETA: 2:18 - loss: 0.6887 - acc: 0.5467
4800/5677 [========================>.....] - ETA: 2:09 - loss: 0.6890 - acc: 0.5460
4864/5677 [========================>.....] - ETA: 1:59 - loss: 0.6891 - acc: 0.5465
4928/5677 [=========================>....] - ETA: 1:50 - loss: 0.6891 - acc: 0.5463
4992/5677 [=========================>....] - ETA: 1:40 - loss: 0.6891 - acc: 0.5465
5056/5677 [=========================>....] - ETA: 1:31 - loss: 0.6891 - acc: 0.5457
5120/5677 [==========================>...] - ETA: 1:21 - loss: 0.6890 - acc: 0.5455
5184/5677 [==========================>...] - ETA: 1:12 - loss: 0.6889 - acc: 0.5461
5248/5677 [==========================>...] - ETA: 1:02 - loss: 0.6889 - acc: 0.5463
5312/5677 [===========================>..] - ETA: 53s - loss: 0.6889 - acc: 0.5463 
5376/5677 [===========================>..] - ETA: 44s - loss: 0.6888 - acc: 0.5458
5440/5677 [===========================>..] - ETA: 34s - loss: 0.6887 - acc: 0.5452
5504/5677 [============================>.] - ETA: 25s - loss: 0.6884 - acc: 0.5452
5568/5677 [============================>.] - ETA: 15s - loss: 0.6886 - acc: 0.5440
5632/5677 [============================>.] - ETA: 6s - loss: 0.6892 - acc: 0.5424 
5677/5677 [==============================] - 859s 151ms/step - loss: 0.6892 - acc: 0.5427 - val_loss: 0.6835 - val_acc: 0.5658

Epoch 00005: val_acc improved from 0.55626 to 0.56577, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window05/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 6/10

  64/5677 [..............................] - ETA: 14:30 - loss: 0.6995 - acc: 0.5781
 128/5677 [..............................] - ETA: 13:40 - loss: 0.6731 - acc: 0.5938
 192/5677 [>.............................] - ETA: 13:08 - loss: 0.6732 - acc: 0.5781
 256/5677 [>.............................] - ETA: 12:46 - loss: 0.6763 - acc: 0.5742
 320/5677 [>.............................] - ETA: 12:33 - loss: 0.6756 - acc: 0.5750
 384/5677 [=>............................] - ETA: 12:29 - loss: 0.6752 - acc: 0.5833
 448/5677 [=>............................] - ETA: 12:21 - loss: 0.6776 - acc: 0.5714
 512/5677 [=>............................] - ETA: 12:08 - loss: 0.6763 - acc: 0.5762
 576/5677 [==>...........................] - ETA: 12:10 - loss: 0.6778 - acc: 0.5781
 640/5677 [==>...........................] - ETA: 11:59 - loss: 0.6817 - acc: 0.5734
 704/5677 [==>...........................] - ETA: 11:45 - loss: 0.6824 - acc: 0.5724
 768/5677 [===>..........................] - ETA: 11:33 - loss: 0.6811 - acc: 0.5716
 832/5677 [===>..........................] - ETA: 11:25 - loss: 0.6813 - acc: 0.5721
 896/5677 [===>..........................] - ETA: 11:16 - loss: 0.6826 - acc: 0.5692
 960/5677 [====>.........................] - ETA: 11:07 - loss: 0.6815 - acc: 0.5740
1024/5677 [====>.........................] - ETA: 10:57 - loss: 0.6808 - acc: 0.5742
1088/5677 [====>.........................] - ETA: 10:45 - loss: 0.6801 - acc: 0.5763
1152/5677 [=====>........................] - ETA: 10:37 - loss: 0.6801 - acc: 0.5755
1216/5677 [=====>........................] - ETA: 10:31 - loss: 0.6788 - acc: 0.5748
1280/5677 [=====>........................] - ETA: 10:20 - loss: 0.6786 - acc: 0.5766
1344/5677 [======>.......................] - ETA: 10:12 - loss: 0.6799 - acc: 0.5722
1408/5677 [======>.......................] - ETA: 10:02 - loss: 0.6800 - acc: 0.5703
1472/5677 [======>.......................] - ETA: 9:56 - loss: 0.6795 - acc: 0.5700 
1536/5677 [=======>......................] - ETA: 9:43 - loss: 0.6799 - acc: 0.5677
1600/5677 [=======>......................] - ETA: 9:32 - loss: 0.6805 - acc: 0.5663
1664/5677 [=======>......................] - ETA: 9:21 - loss: 0.6805 - acc: 0.5643
1728/5677 [========>.....................] - ETA: 9:12 - loss: 0.6811 - acc: 0.5619
1792/5677 [========>.....................] - ETA: 9:03 - loss: 0.6818 - acc: 0.5592
1856/5677 [========>.....................] - ETA: 8:55 - loss: 0.6819 - acc: 0.5566
1920/5677 [=========>....................] - ETA: 8:47 - loss: 0.6821 - acc: 0.5573
1984/5677 [=========>....................] - ETA: 8:38 - loss: 0.6826 - acc: 0.5554
2048/5677 [=========>....................] - ETA: 8:29 - loss: 0.6830 - acc: 0.5542
2112/5677 [==========>...................] - ETA: 8:20 - loss: 0.6833 - acc: 0.5530
2176/5677 [==========>...................] - ETA: 8:12 - loss: 0.6835 - acc: 0.5524
2240/5677 [==========>...................] - ETA: 8:04 - loss: 0.6818 - acc: 0.5558
2304/5677 [===========>..................] - ETA: 7:56 - loss: 0.6825 - acc: 0.5534
2368/5677 [===========>..................] - ETA: 7:47 - loss: 0.6831 - acc: 0.5524
2432/5677 [===========>..................] - ETA: 7:38 - loss: 0.6836 - acc: 0.5535
2496/5677 [============>.................] - ETA: 7:27 - loss: 0.6843 - acc: 0.5533
2560/5677 [============>.................] - ETA: 7:19 - loss: 0.6836 - acc: 0.5547
2624/5677 [============>.................] - ETA: 7:09 - loss: 0.6834 - acc: 0.5553
2688/5677 [=============>................] - ETA: 7:00 - loss: 0.6836 - acc: 0.5547
2752/5677 [=============>................] - ETA: 6:52 - loss: 0.6840 - acc: 0.5516
2816/5677 [=============>................] - ETA: 6:42 - loss: 0.6835 - acc: 0.5526
2880/5677 [==============>...............] - ETA: 6:33 - loss: 0.6832 - acc: 0.5531
2944/5677 [==============>...............] - ETA: 6:24 - loss: 0.6836 - acc: 0.5510
3008/5677 [==============>...............] - ETA: 6:15 - loss: 0.6837 - acc: 0.5515
3072/5677 [===============>..............] - ETA: 6:06 - loss: 0.6838 - acc: 0.5508
3136/5677 [===============>..............] - ETA: 5:57 - loss: 0.6837 - acc: 0.5504
3200/5677 [===============>..............] - ETA: 5:48 - loss: 0.6835 - acc: 0.5513
3264/5677 [================>.............] - ETA: 5:40 - loss: 0.6835 - acc: 0.5509
3328/5677 [================>.............] - ETA: 5:31 - loss: 0.6844 - acc: 0.5490
3392/5677 [================>.............] - ETA: 5:23 - loss: 0.6842 - acc: 0.5481
3456/5677 [=================>............] - ETA: 5:13 - loss: 0.6845 - acc: 0.5483
3520/5677 [=================>............] - ETA: 5:04 - loss: 0.6850 - acc: 0.5466
3584/5677 [=================>............] - ETA: 4:54 - loss: 0.6852 - acc: 0.5460
3648/5677 [==================>...........] - ETA: 4:45 - loss: 0.6851 - acc: 0.5466
3712/5677 [==================>...........] - ETA: 4:36 - loss: 0.6851 - acc: 0.5471
3776/5677 [==================>...........] - ETA: 4:26 - loss: 0.6858 - acc: 0.5463
3840/5677 [===================>..........] - ETA: 4:18 - loss: 0.6860 - acc: 0.5458
3904/5677 [===================>..........] - ETA: 4:09 - loss: 0.6867 - acc: 0.5438
3968/5677 [===================>..........] - ETA: 4:00 - loss: 0.6865 - acc: 0.5441
4032/5677 [====================>.........] - ETA: 3:51 - loss: 0.6873 - acc: 0.5419
4096/5677 [====================>.........] - ETA: 3:41 - loss: 0.6872 - acc: 0.5425
4160/5677 [====================>.........] - ETA: 3:33 - loss: 0.6872 - acc: 0.5435
4224/5677 [=====================>........] - ETA: 3:24 - loss: 0.6875 - acc: 0.5421
4288/5677 [=====================>........] - ETA: 3:15 - loss: 0.6875 - acc: 0.5434
4352/5677 [=====================>........] - ETA: 3:06 - loss: 0.6869 - acc: 0.5453
4416/5677 [======================>.......] - ETA: 2:57 - loss: 0.6872 - acc: 0.5453
4480/5677 [======================>.......] - ETA: 2:47 - loss: 0.6875 - acc: 0.5444
4544/5677 [=======================>......] - ETA: 2:38 - loss: 0.6877 - acc: 0.5445
4608/5677 [=======================>......] - ETA: 2:29 - loss: 0.6879 - acc: 0.5434
4672/5677 [=======================>......] - ETA: 2:20 - loss: 0.6876 - acc: 0.5447
4736/5677 [========================>.....] - ETA: 2:11 - loss: 0.6879 - acc: 0.5446
4800/5677 [========================>.....] - ETA: 2:02 - loss: 0.6885 - acc: 0.5425
4864/5677 [========================>.....] - ETA: 1:53 - loss: 0.6884 - acc: 0.5430
4928/5677 [=========================>....] - ETA: 1:44 - loss: 0.6883 - acc: 0.5430
4992/5677 [=========================>....] - ETA: 1:36 - loss: 0.6884 - acc: 0.5423
5056/5677 [=========================>....] - ETA: 1:27 - loss: 0.6886 - acc: 0.5421
5120/5677 [==========================>...] - ETA: 1:18 - loss: 0.6886 - acc: 0.5422
5184/5677 [==========================>...] - ETA: 1:09 - loss: 0.6888 - acc: 0.5421
5248/5677 [==========================>...] - ETA: 1:00 - loss: 0.6892 - acc: 0.5406
5312/5677 [===========================>..] - ETA: 51s - loss: 0.6893 - acc: 0.5397 
5376/5677 [===========================>..] - ETA: 42s - loss: 0.6893 - acc: 0.5394
5440/5677 [===========================>..] - ETA: 33s - loss: 0.6897 - acc: 0.5386
5504/5677 [============================>.] - ETA: 24s - loss: 0.6896 - acc: 0.5389
5568/5677 [============================>.] - ETA: 15s - loss: 0.6899 - acc: 0.5379
5632/5677 [============================>.] - ETA: 6s - loss: 0.6900 - acc: 0.5376 
5677/5677 [==============================] - 837s 147ms/step - loss: 0.6899 - acc: 0.5380 - val_loss: 0.6875 - val_acc: 0.5420

Epoch 00006: val_acc did not improve from 0.56577
Epoch 7/10

  64/5677 [..............................] - ETA: 13:59 - loss: 0.6846 - acc: 0.6562
 128/5677 [..............................] - ETA: 14:28 - loss: 0.6820 - acc: 0.6016
 192/5677 [>.............................] - ETA: 14:15 - loss: 0.6897 - acc: 0.5677
 256/5677 [>.............................] - ETA: 14:07 - loss: 0.6875 - acc: 0.5625
 320/5677 [>.............................] - ETA: 13:59 - loss: 0.6887 - acc: 0.5531
 384/5677 [=>............................] - ETA: 13:35 - loss: 0.6888 - acc: 0.5417
 448/5677 [=>............................] - ETA: 13:25 - loss: 0.6867 - acc: 0.5513
 512/5677 [=>............................] - ETA: 13:15 - loss: 0.6863 - acc: 0.5547
 576/5677 [==>...........................] - ETA: 13:00 - loss: 0.6862 - acc: 0.5590
 640/5677 [==>...........................] - ETA: 12:48 - loss: 0.6862 - acc: 0.5578
 704/5677 [==>...........................] - ETA: 12:42 - loss: 0.6869 - acc: 0.5526
 768/5677 [===>..........................] - ETA: 12:23 - loss: 0.6861 - acc: 0.5560
 832/5677 [===>..........................] - ETA: 12:13 - loss: 0.6851 - acc: 0.5601
 896/5677 [===>..........................] - ETA: 12:03 - loss: 0.6860 - acc: 0.5592
 960/5677 [====>.........................] - ETA: 11:54 - loss: 0.6855 - acc: 0.5635
1024/5677 [====>.........................] - ETA: 11:42 - loss: 0.6833 - acc: 0.5713
1088/5677 [====>.........................] - ETA: 11:30 - loss: 0.6822 - acc: 0.5689
1152/5677 [=====>........................] - ETA: 11:19 - loss: 0.6843 - acc: 0.5651
1216/5677 [=====>........................] - ETA: 11:09 - loss: 0.6838 - acc: 0.5641
1280/5677 [=====>........................] - ETA: 11:00 - loss: 0.6853 - acc: 0.5602
1344/5677 [======>.......................] - ETA: 10:49 - loss: 0.6860 - acc: 0.5580
1408/5677 [======>.......................] - ETA: 10:39 - loss: 0.6849 - acc: 0.5575
1472/5677 [======>.......................] - ETA: 10:31 - loss: 0.6861 - acc: 0.5577
1536/5677 [=======>......................] - ETA: 10:20 - loss: 0.6853 - acc: 0.5573
1600/5677 [=======>......................] - ETA: 10:08 - loss: 0.6870 - acc: 0.5525
1664/5677 [=======>......................] - ETA: 10:00 - loss: 0.6883 - acc: 0.5511
1728/5677 [========>.....................] - ETA: 9:48 - loss: 0.6879 - acc: 0.5503 
1792/5677 [========>.....................] - ETA: 9:38 - loss: 0.6875 - acc: 0.5541
1856/5677 [========>.....................] - ETA: 9:31 - loss: 0.6875 - acc: 0.5533
1920/5677 [=========>....................] - ETA: 9:21 - loss: 0.6884 - acc: 0.5516
1984/5677 [=========>....................] - ETA: 9:12 - loss: 0.6882 - acc: 0.5509
2048/5677 [=========>....................] - ETA: 9:00 - loss: 0.6881 - acc: 0.5493
2112/5677 [==========>...................] - ETA: 8:52 - loss: 0.6876 - acc: 0.5502
2176/5677 [==========>...................] - ETA: 8:41 - loss: 0.6877 - acc: 0.5501
2240/5677 [==========>...................] - ETA: 8:31 - loss: 0.6874 - acc: 0.5522
2304/5677 [===========>..................] - ETA: 8:21 - loss: 0.6875 - acc: 0.5512
2368/5677 [===========>..................] - ETA: 8:12 - loss: 0.6877 - acc: 0.5507
2432/5677 [===========>..................] - ETA: 8:01 - loss: 0.6872 - acc: 0.5530
2496/5677 [============>.................] - ETA: 7:51 - loss: 0.6868 - acc: 0.5545
2560/5677 [============>.................] - ETA: 7:41 - loss: 0.6863 - acc: 0.5551
2624/5677 [============>.................] - ETA: 7:33 - loss: 0.6869 - acc: 0.5534
2688/5677 [=============>................] - ETA: 7:23 - loss: 0.6866 - acc: 0.5536
2752/5677 [=============>................] - ETA: 7:13 - loss: 0.6859 - acc: 0.5556
2816/5677 [=============>................] - ETA: 7:04 - loss: 0.6862 - acc: 0.5536
2880/5677 [==============>...............] - ETA: 6:56 - loss: 0.6861 - acc: 0.5528
2944/5677 [==============>...............] - ETA: 6:45 - loss: 0.6867 - acc: 0.5520
3008/5677 [==============>...............] - ETA: 6:35 - loss: 0.6870 - acc: 0.5499
3072/5677 [===============>..............] - ETA: 6:25 - loss: 0.6858 - acc: 0.5518
3136/5677 [===============>..............] - ETA: 6:15 - loss: 0.6864 - acc: 0.5507
3200/5677 [===============>..............] - ETA: 6:06 - loss: 0.6868 - acc: 0.5500
3264/5677 [================>.............] - ETA: 5:56 - loss: 0.6874 - acc: 0.5487
3328/5677 [================>.............] - ETA: 5:46 - loss: 0.6885 - acc: 0.5454
3392/5677 [================>.............] - ETA: 5:37 - loss: 0.6887 - acc: 0.5466
3456/5677 [=================>............] - ETA: 5:27 - loss: 0.6877 - acc: 0.5489
3520/5677 [=================>............] - ETA: 5:17 - loss: 0.6874 - acc: 0.5500
3584/5677 [=================>............] - ETA: 5:08 - loss: 0.6876 - acc: 0.5497
3648/5677 [==================>...........] - ETA: 4:58 - loss: 0.6876 - acc: 0.5496
3712/5677 [==================>...........] - ETA: 4:49 - loss: 0.6876 - acc: 0.5496
3776/5677 [==================>...........] - ETA: 4:39 - loss: 0.6877 - acc: 0.5485
3840/5677 [===================>..........] - ETA: 4:30 - loss: 0.6879 - acc: 0.5479
3904/5677 [===================>..........] - ETA: 4:21 - loss: 0.6880 - acc: 0.5466
3968/5677 [===================>..........] - ETA: 4:11 - loss: 0.6876 - acc: 0.5474
4032/5677 [====================>.........] - ETA: 4:02 - loss: 0.6872 - acc: 0.5486
4096/5677 [====================>.........] - ETA: 3:52 - loss: 0.6869 - acc: 0.5493
4160/5677 [====================>.........] - ETA: 3:42 - loss: 0.6873 - acc: 0.5490
4224/5677 [=====================>........] - ETA: 3:33 - loss: 0.6871 - acc: 0.5490
4288/5677 [=====================>........] - ETA: 3:24 - loss: 0.6872 - acc: 0.5494
4352/5677 [=====================>........] - ETA: 3:14 - loss: 0.6875 - acc: 0.5496
4416/5677 [======================>.......] - ETA: 3:05 - loss: 0.6875 - acc: 0.5494
4480/5677 [======================>.......] - ETA: 2:55 - loss: 0.6875 - acc: 0.5493
4544/5677 [=======================>......] - ETA: 2:46 - loss: 0.6870 - acc: 0.5506
4608/5677 [=======================>......] - ETA: 2:36 - loss: 0.6870 - acc: 0.5506
4672/5677 [=======================>......] - ETA: 2:27 - loss: 0.6872 - acc: 0.5514
4736/5677 [========================>.....] - ETA: 2:18 - loss: 0.6871 - acc: 0.5513
4800/5677 [========================>.....] - ETA: 2:09 - loss: 0.6873 - acc: 0.5506
4864/5677 [========================>.....] - ETA: 1:59 - loss: 0.6872 - acc: 0.5504
4928/5677 [=========================>....] - ETA: 1:50 - loss: 0.6871 - acc: 0.5507
4992/5677 [=========================>....] - ETA: 1:40 - loss: 0.6870 - acc: 0.5513
5056/5677 [=========================>....] - ETA: 1:31 - loss: 0.6867 - acc: 0.5522
5120/5677 [==========================>...] - ETA: 1:21 - loss: 0.6865 - acc: 0.5523
5184/5677 [==========================>...] - ETA: 1:12 - loss: 0.6869 - acc: 0.5515
5248/5677 [==========================>...] - ETA: 1:03 - loss: 0.6870 - acc: 0.5520
5312/5677 [===========================>..] - ETA: 53s - loss: 0.6874 - acc: 0.5521 
5376/5677 [===========================>..] - ETA: 44s - loss: 0.6873 - acc: 0.5528
5440/5677 [===========================>..] - ETA: 34s - loss: 0.6872 - acc: 0.5529
5504/5677 [============================>.] - ETA: 25s - loss: 0.6872 - acc: 0.5536
5568/5677 [============================>.] - ETA: 16s - loss: 0.6873 - acc: 0.5537
5632/5677 [============================>.] - ETA: 6s - loss: 0.6871 - acc: 0.5540 
5677/5677 [==============================] - 868s 153ms/step - loss: 0.6876 - acc: 0.5517 - val_loss: 0.6871 - val_acc: 0.5531

Epoch 00007: val_acc did not improve from 0.56577
Epoch 8/10

  64/5677 [..............................] - ETA: 15:51 - loss: 0.6850 - acc: 0.5625
 128/5677 [..............................] - ETA: 16:10 - loss: 0.6767 - acc: 0.5703
 192/5677 [>.............................] - ETA: 14:35 - loss: 0.6819 - acc: 0.5677
 256/5677 [>.............................] - ETA: 14:21 - loss: 0.6795 - acc: 0.5703
 320/5677 [>.............................] - ETA: 13:58 - loss: 0.6770 - acc: 0.5813
 384/5677 [=>............................] - ETA: 13:41 - loss: 0.6752 - acc: 0.5833
 448/5677 [=>............................] - ETA: 13:34 - loss: 0.6762 - acc: 0.5826
 512/5677 [=>............................] - ETA: 13:10 - loss: 0.6770 - acc: 0.5820
 576/5677 [==>...........................] - ETA: 12:54 - loss: 0.6807 - acc: 0.5712
 640/5677 [==>...........................] - ETA: 12:49 - loss: 0.6784 - acc: 0.5719
 704/5677 [==>...........................] - ETA: 12:28 - loss: 0.6787 - acc: 0.5682
 768/5677 [===>..........................] - ETA: 12:20 - loss: 0.6801 - acc: 0.5651
 832/5677 [===>..........................] - ETA: 12:05 - loss: 0.6819 - acc: 0.5649
 896/5677 [===>..........................] - ETA: 11:53 - loss: 0.6826 - acc: 0.5625
 960/5677 [====>.........................] - ETA: 11:45 - loss: 0.6827 - acc: 0.5604
1024/5677 [====>.........................] - ETA: 11:30 - loss: 0.6834 - acc: 0.5605
1088/5677 [====>.........................] - ETA: 11:19 - loss: 0.6856 - acc: 0.5533
1152/5677 [=====>........................] - ETA: 11:14 - loss: 0.6838 - acc: 0.5599
1216/5677 [=====>........................] - ETA: 11:03 - loss: 0.6812 - acc: 0.5674
1280/5677 [=====>........................] - ETA: 10:54 - loss: 0.6819 - acc: 0.5633
1344/5677 [======>.......................] - ETA: 10:45 - loss: 0.6804 - acc: 0.5699
1408/5677 [======>.......................] - ETA: 10:35 - loss: 0.6809 - acc: 0.5682
1472/5677 [======>.......................] - ETA: 10:31 - loss: 0.6811 - acc: 0.5659
1536/5677 [=======>......................] - ETA: 10:20 - loss: 0.6820 - acc: 0.5632
1600/5677 [=======>......................] - ETA: 10:11 - loss: 0.6824 - acc: 0.5656
1664/5677 [=======>......................] - ETA: 9:59 - loss: 0.6829 - acc: 0.5661 
1728/5677 [========>.....................] - ETA: 9:48 - loss: 0.6837 - acc: 0.5648
1792/5677 [========>.....................] - ETA: 9:41 - loss: 0.6828 - acc: 0.5658
1856/5677 [========>.....................] - ETA: 9:31 - loss: 0.6819 - acc: 0.5690
1920/5677 [=========>....................] - ETA: 9:20 - loss: 0.6824 - acc: 0.5661
1984/5677 [=========>....................] - ETA: 9:09 - loss: 0.6818 - acc: 0.5680
2048/5677 [=========>....................] - ETA: 9:00 - loss: 0.6820 - acc: 0.5664
2112/5677 [==========>...................] - ETA: 8:51 - loss: 0.6829 - acc: 0.5653
2176/5677 [==========>...................] - ETA: 8:40 - loss: 0.6839 - acc: 0.5634
2240/5677 [==========>...................] - ETA: 8:31 - loss: 0.6836 - acc: 0.5629
2304/5677 [===========>..................] - ETA: 8:21 - loss: 0.6835 - acc: 0.5629
2368/5677 [===========>..................] - ETA: 8:11 - loss: 0.6831 - acc: 0.5638
2432/5677 [===========>..................] - ETA: 8:02 - loss: 0.6826 - acc: 0.5650
2496/5677 [============>.................] - ETA: 7:55 - loss: 0.6821 - acc: 0.5665
2560/5677 [============>.................] - ETA: 7:45 - loss: 0.6823 - acc: 0.5656
2624/5677 [============>.................] - ETA: 7:34 - loss: 0.6820 - acc: 0.5659
2688/5677 [=============>................] - ETA: 7:25 - loss: 0.6821 - acc: 0.5658
2752/5677 [=============>................] - ETA: 7:15 - loss: 0.6827 - acc: 0.5654
2816/5677 [=============>................] - ETA: 7:05 - loss: 0.6836 - acc: 0.5646
2880/5677 [==============>...............] - ETA: 6:55 - loss: 0.6834 - acc: 0.5646
2944/5677 [==============>...............] - ETA: 6:46 - loss: 0.6826 - acc: 0.5656
3008/5677 [==============>...............] - ETA: 6:36 - loss: 0.6834 - acc: 0.5625
3072/5677 [===============>..............] - ETA: 6:26 - loss: 0.6840 - acc: 0.5618
3136/5677 [===============>..............] - ETA: 6:18 - loss: 0.6839 - acc: 0.5622
3200/5677 [===============>..............] - ETA: 6:09 - loss: 0.6839 - acc: 0.5622
3264/5677 [================>.............] - ETA: 6:00 - loss: 0.6839 - acc: 0.5634
3328/5677 [================>.............] - ETA: 5:50 - loss: 0.6844 - acc: 0.5628
3392/5677 [================>.............] - ETA: 5:40 - loss: 0.6838 - acc: 0.5628
3456/5677 [=================>............] - ETA: 5:31 - loss: 0.6843 - acc: 0.5616
3520/5677 [=================>............] - ETA: 5:21 - loss: 0.6845 - acc: 0.5622
3584/5677 [=================>............] - ETA: 5:12 - loss: 0.6848 - acc: 0.5619
3648/5677 [==================>...........] - ETA: 5:03 - loss: 0.6848 - acc: 0.5611
3712/5677 [==================>...........] - ETA: 4:54 - loss: 0.6843 - acc: 0.5614
3776/5677 [==================>...........] - ETA: 4:45 - loss: 0.6847 - acc: 0.5604
3840/5677 [===================>..........] - ETA: 4:35 - loss: 0.6843 - acc: 0.5604
3904/5677 [===================>..........] - ETA: 4:26 - loss: 0.6849 - acc: 0.5594
3968/5677 [===================>..........] - ETA: 4:17 - loss: 0.6845 - acc: 0.5592
4032/5677 [====================>.........] - ETA: 4:07 - loss: 0.6843 - acc: 0.5603
4096/5677 [====================>.........] - ETA: 3:57 - loss: 0.6840 - acc: 0.5608
4160/5677 [====================>.........] - ETA: 3:48 - loss: 0.6836 - acc: 0.5613
4224/5677 [=====================>........] - ETA: 3:38 - loss: 0.6837 - acc: 0.5608
4288/5677 [=====================>........] - ETA: 3:29 - loss: 0.6834 - acc: 0.5611
4352/5677 [=====================>........] - ETA: 3:19 - loss: 0.6835 - acc: 0.5607
4416/5677 [======================>.......] - ETA: 3:09 - loss: 0.6841 - acc: 0.5600
4480/5677 [======================>.......] - ETA: 3:00 - loss: 0.6847 - acc: 0.5587
4544/5677 [=======================>......] - ETA: 2:50 - loss: 0.6845 - acc: 0.5581
4608/5677 [=======================>......] - ETA: 2:41 - loss: 0.6846 - acc: 0.5571
4672/5677 [=======================>......] - ETA: 2:31 - loss: 0.6846 - acc: 0.5563
4736/5677 [========================>.....] - ETA: 2:21 - loss: 0.6843 - acc: 0.5572
4800/5677 [========================>.....] - ETA: 2:12 - loss: 0.6840 - acc: 0.5575
4864/5677 [========================>.....] - ETA: 2:02 - loss: 0.6836 - acc: 0.5578
4928/5677 [=========================>....] - ETA: 1:52 - loss: 0.6833 - acc: 0.5578
4992/5677 [=========================>....] - ETA: 1:43 - loss: 0.6835 - acc: 0.5571
5056/5677 [=========================>....] - ETA: 1:33 - loss: 0.6834 - acc: 0.5566
5120/5677 [==========================>...] - ETA: 1:23 - loss: 0.6834 - acc: 0.5572
5184/5677 [==========================>...] - ETA: 1:14 - loss: 0.6835 - acc: 0.5579
5248/5677 [==========================>...] - ETA: 1:04 - loss: 0.6838 - acc: 0.5570
5312/5677 [===========================>..] - ETA: 55s - loss: 0.6840 - acc: 0.5563 
5376/5677 [===========================>..] - ETA: 45s - loss: 0.6837 - acc: 0.5562
5440/5677 [===========================>..] - ETA: 35s - loss: 0.6836 - acc: 0.5572
5504/5677 [============================>.] - ETA: 26s - loss: 0.6835 - acc: 0.5576
5568/5677 [============================>.] - ETA: 16s - loss: 0.6840 - acc: 0.5569
5632/5677 [============================>.] - ETA: 6s - loss: 0.6841 - acc: 0.5566 
5677/5677 [==============================] - 898s 158ms/step - loss: 0.6844 - acc: 0.5561 - val_loss: 0.6804 - val_acc: 0.5705

Epoch 00008: val_acc improved from 0.56577 to 0.57052, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window05/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
Epoch 9/10

  64/5677 [..............................] - ETA: 15:55 - loss: 0.6485 - acc: 0.6406
 128/5677 [..............................] - ETA: 15:33 - loss: 0.6615 - acc: 0.6172
 192/5677 [>.............................] - ETA: 14:29 - loss: 0.6739 - acc: 0.6094
 256/5677 [>.............................] - ETA: 13:34 - loss: 0.6729 - acc: 0.6016
 320/5677 [>.............................] - ETA: 13:47 - loss: 0.6776 - acc: 0.5844
 384/5677 [=>............................] - ETA: 13:31 - loss: 0.6776 - acc: 0.5833
 448/5677 [=>............................] - ETA: 13:05 - loss: 0.6731 - acc: 0.5804
 512/5677 [=>............................] - ETA: 12:53 - loss: 0.6727 - acc: 0.5879
 576/5677 [==>...........................] - ETA: 12:46 - loss: 0.6773 - acc: 0.5764
 640/5677 [==>...........................] - ETA: 12:37 - loss: 0.6796 - acc: 0.5703
 704/5677 [==>...........................] - ETA: 12:35 - loss: 0.6767 - acc: 0.5795
 768/5677 [===>..........................] - ETA: 12:27 - loss: 0.6761 - acc: 0.5885
 832/5677 [===>..........................] - ETA: 12:08 - loss: 0.6783 - acc: 0.5841
 896/5677 [===>..........................] - ETA: 12:00 - loss: 0.6771 - acc: 0.5915
 960/5677 [====>.........................] - ETA: 11:51 - loss: 0.6774 - acc: 0.5875
1024/5677 [====>.........................] - ETA: 11:39 - loss: 0.6762 - acc: 0.5869
1088/5677 [====>.........................] - ETA: 11:28 - loss: 0.6748 - acc: 0.5919
1152/5677 [=====>........................] - ETA: 11:15 - loss: 0.6766 - acc: 0.5842
1216/5677 [=====>........................] - ETA: 11:07 - loss: 0.6781 - acc: 0.5839
1280/5677 [=====>........................] - ETA: 10:56 - loss: 0.6799 - acc: 0.5797
1344/5677 [======>.......................] - ETA: 10:49 - loss: 0.6794 - acc: 0.5811
1408/5677 [======>.......................] - ETA: 10:41 - loss: 0.6782 - acc: 0.5817
1472/5677 [======>.......................] - ETA: 10:32 - loss: 0.6795 - acc: 0.5788
1536/5677 [=======>......................] - ETA: 10:22 - loss: 0.6812 - acc: 0.5755
1600/5677 [=======>......................] - ETA: 10:10 - loss: 0.6825 - acc: 0.5719
1664/5677 [=======>......................] - ETA: 10:03 - loss: 0.6813 - acc: 0.5739
1728/5677 [========>.....................] - ETA: 9:54 - loss: 0.6828 - acc: 0.5729 
1792/5677 [========>.....................] - ETA: 9:46 - loss: 0.6825 - acc: 0.5742
1856/5677 [========>.....................] - ETA: 9:36 - loss: 0.6810 - acc: 0.5765
1920/5677 [=========>....................] - ETA: 9:26 - loss: 0.6811 - acc: 0.5750
1984/5677 [=========>....................] - ETA: 9:13 - loss: 0.6814 - acc: 0.5736
2048/5677 [=========>....................] - ETA: 9:04 - loss: 0.6805 - acc: 0.5771
2112/5677 [==========>...................] - ETA: 8:52 - loss: 0.6816 - acc: 0.5758
2176/5677 [==========>...................] - ETA: 8:43 - loss: 0.6807 - acc: 0.5781
2240/5677 [==========>...................] - ETA: 8:35 - loss: 0.6814 - acc: 0.5754
2304/5677 [===========>..................] - ETA: 8:25 - loss: 0.6812 - acc: 0.5764
2368/5677 [===========>..................] - ETA: 8:14 - loss: 0.6816 - acc: 0.5773
2432/5677 [===========>..................] - ETA: 8:04 - loss: 0.6813 - acc: 0.5777
2496/5677 [============>.................] - ETA: 7:54 - loss: 0.6811 - acc: 0.5765
2560/5677 [============>.................] - ETA: 7:44 - loss: 0.6810 - acc: 0.5777
2624/5677 [============>.................] - ETA: 7:33 - loss: 0.6810 - acc: 0.5777
2688/5677 [=============>................] - ETA: 7:22 - loss: 0.6811 - acc: 0.5770
2752/5677 [=============>................] - ETA: 7:13 - loss: 0.6812 - acc: 0.5756
2816/5677 [=============>................] - ETA: 7:05 - loss: 0.6799 - acc: 0.5781
2880/5677 [==============>...............] - ETA: 6:56 - loss: 0.6810 - acc: 0.5757
2944/5677 [==============>...............] - ETA: 6:47 - loss: 0.6811 - acc: 0.5740
3008/5677 [==============>...............] - ETA: 6:37 - loss: 0.6806 - acc: 0.5748
3072/5677 [===============>..............] - ETA: 6:28 - loss: 0.6801 - acc: 0.5758
3136/5677 [===============>..............] - ETA: 6:18 - loss: 0.6802 - acc: 0.5743
3200/5677 [===============>..............] - ETA: 6:09 - loss: 0.6796 - acc: 0.5750
3264/5677 [================>.............] - ETA: 5:59 - loss: 0.6799 - acc: 0.5735
3328/5677 [================>.............] - ETA: 5:48 - loss: 0.6809 - acc: 0.5721
3392/5677 [================>.............] - ETA: 5:40 - loss: 0.6806 - acc: 0.5719
3456/5677 [=================>............] - ETA: 5:31 - loss: 0.6811 - acc: 0.5709
3520/5677 [=================>............] - ETA: 5:22 - loss: 0.6808 - acc: 0.5713
3584/5677 [=================>............] - ETA: 5:13 - loss: 0.6809 - acc: 0.5703
3648/5677 [==================>...........] - ETA: 5:02 - loss: 0.6813 - acc: 0.5694
3712/5677 [==================>...........] - ETA: 4:53 - loss: 0.6819 - acc: 0.5687
3776/5677 [==================>...........] - ETA: 4:44 - loss: 0.6813 - acc: 0.5699
3840/5677 [===================>..........] - ETA: 4:34 - loss: 0.6818 - acc: 0.5687
3904/5677 [===================>..........] - ETA: 4:25 - loss: 0.6820 - acc: 0.5674
3968/5677 [===================>..........] - ETA: 4:15 - loss: 0.6819 - acc: 0.5673
4032/5677 [====================>.........] - ETA: 4:06 - loss: 0.6821 - acc: 0.5662
4096/5677 [====================>.........] - ETA: 3:57 - loss: 0.6817 - acc: 0.5679
4160/5677 [====================>.........] - ETA: 3:48 - loss: 0.6823 - acc: 0.5654
4224/5677 [=====================>........] - ETA: 3:38 - loss: 0.6825 - acc: 0.5649
4288/5677 [=====================>........] - ETA: 3:28 - loss: 0.6821 - acc: 0.5658
4352/5677 [=====================>........] - ETA: 3:19 - loss: 0.6823 - acc: 0.5648
4416/5677 [======================>.......] - ETA: 3:09 - loss: 0.6820 - acc: 0.5657
4480/5677 [======================>.......] - ETA: 3:00 - loss: 0.6818 - acc: 0.5667
4544/5677 [=======================>......] - ETA: 2:50 - loss: 0.6815 - acc: 0.5669
4608/5677 [=======================>......] - ETA: 2:41 - loss: 0.6822 - acc: 0.5647
4672/5677 [=======================>......] - ETA: 2:31 - loss: 0.6822 - acc: 0.5655
4736/5677 [========================>.....] - ETA: 2:22 - loss: 0.6825 - acc: 0.5648
4800/5677 [========================>.....] - ETA: 2:12 - loss: 0.6824 - acc: 0.5648
4864/5677 [========================>.....] - ETA: 2:02 - loss: 0.6820 - acc: 0.5660
4928/5677 [=========================>....] - ETA: 1:53 - loss: 0.6819 - acc: 0.5664
4992/5677 [=========================>....] - ETA: 1:43 - loss: 0.6819 - acc: 0.5655
5056/5677 [=========================>....] - ETA: 1:34 - loss: 0.6819 - acc: 0.5657
5120/5677 [==========================>...] - ETA: 1:24 - loss: 0.6820 - acc: 0.5650
5184/5677 [==========================>...] - ETA: 1:14 - loss: 0.6818 - acc: 0.5652
5248/5677 [==========================>...] - ETA: 1:05 - loss: 0.6819 - acc: 0.5650
5312/5677 [===========================>..] - ETA: 55s - loss: 0.6817 - acc: 0.5651 
5376/5677 [===========================>..] - ETA: 45s - loss: 0.6813 - acc: 0.5670
5440/5677 [===========================>..] - ETA: 35s - loss: 0.6814 - acc: 0.5667
5504/5677 [============================>.] - ETA: 26s - loss: 0.6813 - acc: 0.5672
5568/5677 [============================>.] - ETA: 16s - loss: 0.6814 - acc: 0.5675
5632/5677 [============================>.] - ETA: 6s - loss: 0.6810 - acc: 0.5682 
5677/5677 [==============================] - 892s 157ms/step - loss: 0.6812 - acc: 0.5676 - val_loss: 0.6820 - val_acc: 0.5547

Epoch 00009: val_acc did not improve from 0.57052
Epoch 10/10

  64/5677 [..............................] - ETA: 15:21 - loss: 0.7161 - acc: 0.4688
 128/5677 [..............................] - ETA: 15:21 - loss: 0.6848 - acc: 0.5547
 192/5677 [>.............................] - ETA: 14:41 - loss: 0.6732 - acc: 0.5677
 256/5677 [>.............................] - ETA: 14:25 - loss: 0.6721 - acc: 0.5859
 320/5677 [>.............................] - ETA: 14:09 - loss: 0.6767 - acc: 0.5781
 384/5677 [=>............................] - ETA: 13:43 - loss: 0.6774 - acc: 0.5703
 448/5677 [=>............................] - ETA: 13:19 - loss: 0.6816 - acc: 0.5580
 512/5677 [=>............................] - ETA: 13:01 - loss: 0.6823 - acc: 0.5547
 576/5677 [==>...........................] - ETA: 12:49 - loss: 0.6815 - acc: 0.5590
 640/5677 [==>...........................] - ETA: 12:31 - loss: 0.6834 - acc: 0.5531
 704/5677 [==>...........................] - ETA: 12:24 - loss: 0.6863 - acc: 0.5369
 768/5677 [===>..........................] - ETA: 12:19 - loss: 0.6878 - acc: 0.5339
 832/5677 [===>..........................] - ETA: 12:11 - loss: 0.6880 - acc: 0.5312
 896/5677 [===>..........................] - ETA: 12:01 - loss: 0.6863 - acc: 0.5402
 960/5677 [====>.........................] - ETA: 11:49 - loss: 0.6879 - acc: 0.5354
1024/5677 [====>.........................] - ETA: 11:44 - loss: 0.6862 - acc: 0.5410
1088/5677 [====>.........................] - ETA: 11:31 - loss: 0.6858 - acc: 0.5460
1152/5677 [=====>........................] - ETA: 11:21 - loss: 0.6860 - acc: 0.5451
1216/5677 [=====>........................] - ETA: 11:12 - loss: 0.6840 - acc: 0.5469
1280/5677 [=====>........................] - ETA: 11:06 - loss: 0.6831 - acc: 0.5492
1344/5677 [======>.......................] - ETA: 10:54 - loss: 0.6849 - acc: 0.5446
1408/5677 [======>.......................] - ETA: 10:45 - loss: 0.6854 - acc: 0.5447
1472/5677 [======>.......................] - ETA: 10:37 - loss: 0.6875 - acc: 0.5414
1536/5677 [=======>......................] - ETA: 10:26 - loss: 0.6878 - acc: 0.5410
1600/5677 [=======>......................] - ETA: 10:18 - loss: 0.6882 - acc: 0.5413
1664/5677 [=======>......................] - ETA: 10:07 - loss: 0.6882 - acc: 0.5403
1728/5677 [========>.....................] - ETA: 9:55 - loss: 0.6877 - acc: 0.5428 
1792/5677 [========>.....................] - ETA: 9:44 - loss: 0.6879 - acc: 0.5402
1856/5677 [========>.....................] - ETA: 9:35 - loss: 0.6862 - acc: 0.5453
1920/5677 [=========>....................] - ETA: 9:26 - loss: 0.6864 - acc: 0.5437
1984/5677 [=========>....................] - ETA: 9:18 - loss: 0.6861 - acc: 0.5444
2048/5677 [=========>....................] - ETA: 9:08 - loss: 0.6861 - acc: 0.5430
2112/5677 [==========>...................] - ETA: 9:00 - loss: 0.6877 - acc: 0.5374
2176/5677 [==========>...................] - ETA: 8:49 - loss: 0.6880 - acc: 0.5363
2240/5677 [==========>...................] - ETA: 8:39 - loss: 0.6884 - acc: 0.5371
2304/5677 [===========>..................] - ETA: 8:29 - loss: 0.6884 - acc: 0.5369
2368/5677 [===========>..................] - ETA: 8:20 - loss: 0.6883 - acc: 0.5376
2432/5677 [===========>..................] - ETA: 8:10 - loss: 0.6876 - acc: 0.5411
2496/5677 [============>.................] - ETA: 8:02 - loss: 0.6878 - acc: 0.5409
2560/5677 [============>.................] - ETA: 7:53 - loss: 0.6875 - acc: 0.5414
2624/5677 [============>.................] - ETA: 7:45 - loss: 0.6876 - acc: 0.5412
2688/5677 [=============>................] - ETA: 7:36 - loss: 0.6870 - acc: 0.5424
2752/5677 [=============>................] - ETA: 7:26 - loss: 0.6874 - acc: 0.5411
2816/5677 [=============>................] - ETA: 7:16 - loss: 0.6872 - acc: 0.5437
2880/5677 [==============>...............] - ETA: 7:07 - loss: 0.6869 - acc: 0.5437
2944/5677 [==============>...............] - ETA: 6:56 - loss: 0.6862 - acc: 0.5442
3008/5677 [==============>...............] - ETA: 6:46 - loss: 0.6859 - acc: 0.5439
3072/5677 [===============>..............] - ETA: 6:37 - loss: 0.6856 - acc: 0.5439
3136/5677 [===============>..............] - ETA: 6:27 - loss: 0.6857 - acc: 0.5427
3200/5677 [===============>..............] - ETA: 6:17 - loss: 0.6858 - acc: 0.5416
3264/5677 [================>.............] - ETA: 6:07 - loss: 0.6859 - acc: 0.5404
3328/5677 [================>.............] - ETA: 5:58 - loss: 0.6863 - acc: 0.5400
3392/5677 [================>.............] - ETA: 5:49 - loss: 0.6867 - acc: 0.5395
3456/5677 [=================>............] - ETA: 5:40 - loss: 0.6866 - acc: 0.5394
3520/5677 [=================>............] - ETA: 5:31 - loss: 0.6863 - acc: 0.5415
3584/5677 [=================>............] - ETA: 5:21 - loss: 0.6861 - acc: 0.5419
3648/5677 [==================>...........] - ETA: 5:12 - loss: 0.6861 - acc: 0.5422
3712/5677 [==================>...........] - ETA: 5:02 - loss: 0.6858 - acc: 0.5428
3776/5677 [==================>...........] - ETA: 4:52 - loss: 0.6859 - acc: 0.5437
3840/5677 [===================>..........] - ETA: 4:43 - loss: 0.6861 - acc: 0.5430
3904/5677 [===================>..........] - ETA: 4:33 - loss: 0.6869 - acc: 0.5412
3968/5677 [===================>..........] - ETA: 4:23 - loss: 0.6863 - acc: 0.5423
4032/5677 [====================>.........] - ETA: 4:14 - loss: 0.6861 - acc: 0.5424
4096/5677 [====================>.........] - ETA: 4:04 - loss: 0.6859 - acc: 0.5442
4160/5677 [====================>.........] - ETA: 3:54 - loss: 0.6853 - acc: 0.5466
4224/5677 [=====================>........] - ETA: 3:44 - loss: 0.6846 - acc: 0.5478
4288/5677 [=====================>........] - ETA: 3:34 - loss: 0.6846 - acc: 0.5480
4352/5677 [=====================>........] - ETA: 3:25 - loss: 0.6849 - acc: 0.5480
4416/5677 [======================>.......] - ETA: 3:15 - loss: 0.6847 - acc: 0.5494
4480/5677 [======================>.......] - ETA: 3:05 - loss: 0.6851 - acc: 0.5487
4544/5677 [=======================>......] - ETA: 2:55 - loss: 0.6853 - acc: 0.5493
4608/5677 [=======================>......] - ETA: 2:45 - loss: 0.6848 - acc: 0.5512
4672/5677 [=======================>......] - ETA: 2:35 - loss: 0.6844 - acc: 0.5529
4736/5677 [========================>.....] - ETA: 2:26 - loss: 0.6843 - acc: 0.5530
4800/5677 [========================>.....] - ETA: 2:16 - loss: 0.6843 - acc: 0.5525
4864/5677 [========================>.....] - ETA: 2:06 - loss: 0.6845 - acc: 0.5526
4928/5677 [=========================>....] - ETA: 1:56 - loss: 0.6848 - acc: 0.5509
4992/5677 [=========================>....] - ETA: 1:46 - loss: 0.6848 - acc: 0.5515
5056/5677 [=========================>....] - ETA: 1:36 - loss: 0.6846 - acc: 0.5508
5120/5677 [==========================>...] - ETA: 1:26 - loss: 0.6841 - acc: 0.5520
5184/5677 [==========================>...] - ETA: 1:16 - loss: 0.6837 - acc: 0.5532
5248/5677 [==========================>...] - ETA: 1:06 - loss: 0.6839 - acc: 0.5524
5312/5677 [===========================>..] - ETA: 56s - loss: 0.6836 - acc: 0.5527 
5376/5677 [===========================>..] - ETA: 46s - loss: 0.6831 - acc: 0.5543
5440/5677 [===========================>..] - ETA: 36s - loss: 0.6831 - acc: 0.5542
5504/5677 [============================>.] - ETA: 26s - loss: 0.6832 - acc: 0.5547
5568/5677 [============================>.] - ETA: 16s - loss: 0.6833 - acc: 0.5548
5632/5677 [============================>.] - ETA: 6s - loss: 0.6829 - acc: 0.5552 
5677/5677 [==============================] - 913s 161ms/step - loss: 0.6828 - acc: 0.5556 - val_loss: 0.6820 - val_acc: 0.5721

Epoch 00010: val_acc improved from 0.57052 to 0.57211, saving model to /data/lyli/Stomach/stomach-model/Seq_feature_exact/window05/checkpoints/final_seq_model/stomach_seq_model_2_mer.hdf5
样本个数 789
样本个数 1578
WARNING:tensorflow:Entity <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f1c103a7d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TokenEmbedding.call of <keras_bert.layers.embedding.TokenEmbedding object at 0x7f1c103a7d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f1c10345ed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method PositionEmbedding.call of <keras_pos_embd.pos_embd.PositionEmbedding object at 0x7f1c10345ed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c100ece10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c100ece10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1bf86b6150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1bf86b6150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1c102cd050>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1c102cd050>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f17f04a52d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f17f04a52d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1bf86b64d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1bf86b64d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1bf864a450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1bf864a450>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1bf8625990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1bf8625990>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1bf8456fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1bf8456fd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1bf8286e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1bf8286e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1bf86a4210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1bf86a4210>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f17e048dd90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f17e048dd90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1bf81eefd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1bf81eefd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1bf83d29d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1bf83d29d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1bf8208610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1bf8208610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1bf81ee310>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1bf81ee310>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1bf8149d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1bf8149d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f174873c4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f174873c4d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1ab0780c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1ab0780c50>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f19d0223610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f19d0223610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f174873c650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f174873c650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c10423190>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c10423190>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1ab05f3950>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1ab05f3950>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1ab0600290>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1ab0600290>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c501b0b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1c501b0b10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1ab05f3d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1ab05f3d90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1ab0488650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1ab0488650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1ab028c790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1ab028c790>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1ab013b890>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1ab013b890>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1ab02ed610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1ab02ed610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1ab05e6a10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1ab05e6a10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1ab0281e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1ab0281e90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1a9876a410>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1a9876a410>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1a986418d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1a986418d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a98502650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a98502650>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1a9876a990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1a9876a990>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a98611610>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a98611610>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1a984742d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1a984742d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1a98309dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1a98309dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a982fa550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a982fa550>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1a982fa4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1a982fa4d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a9820ca10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a9820ca10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1a9813aed0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1a9813aed0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1a980bcf90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1a980bcf90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a98127c90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a98127c90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1a9814cb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1a9814cb90>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a706f4e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a706f4e10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1a70595150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1a70595150>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1a7043ba90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1a7043ba90>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a707cd0d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a707cd0d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1a70595d50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1a70595d50>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a703ac6d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a703ac6d0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1a7024edd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1a7024edd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1a7018d550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1a7018d550>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a687bb590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a687bb590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1a7024ead0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1a7024ead0>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a701c1c10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a701c1c10>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1a7018d4d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiHeadAttention.call of <keras_multi_head.multi_head_attention.MultiHeadAttention object at 0x7f1a7018d4d0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1a685c3dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ScaledDotProductAttention.call of <keras_self_attention.scaled_dot_attention.ScaledDotProductAttention object at 0x7f1a685c3dd0>>: AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a70063110>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a70063110>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1a7018d590>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method FeedForward.call of <keras_position_wise_feed_forward.feed_forward.FeedForward object at 0x7f1a7018d590>>: AttributeError: module 'gast' has no attribute 'Str'
WARNING:tensorflow:Entity <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a6860ee90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LayerNormalization.call of <keras_layer_normalization.layer_normalization.LayerNormalization object at 0x7f1a6860ee90>>: AttributeError: module 'gast' has no attribute 'Str'
01window_select.py:153: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`
  model = Model(input=[inputs_seq1], output=[seq_dense])

  64/1578 [>.............................] - ETA: 6:16
 128/1578 [=>............................] - ETA: 3:34
 192/1578 [==>...........................] - ETA: 2:38
 256/1578 [===>..........................] - ETA: 2:09
 320/1578 [=====>........................] - ETA: 1:49
 384/1578 [======>.......................] - ETA: 1:35
 448/1578 [=======>......................] - ETA: 1:24
 512/1578 [========>.....................] - ETA: 1:15
 576/1578 [=========>....................] - ETA: 1:08
 640/1578 [===========>..................] - ETA: 1:02
 704/1578 [============>.................] - ETA: 56s 
 768/1578 [=============>................] - ETA: 50s
 832/1578 [==============>...............] - ETA: 45s
 896/1578 [================>.............] - ETA: 41s
 960/1578 [=================>............] - ETA: 37s
1024/1578 [==================>...........] - ETA: 32s
1088/1578 [===================>..........] - ETA: 28s
1152/1578 [====================>.........] - ETA: 24s
1216/1578 [======================>.......] - ETA: 20s
1280/1578 [=======================>......] - ETA: 16s
1344/1578 [========================>.....] - ETA: 13s
1408/1578 [=========================>....] - ETA: 9s 
1472/1578 [==========================>...] - ETA: 5s
1536/1578 [============================>.] - ETA: 2s
1578/1578 [==============================] - 85s 54ms/step
loss: 0.67662567610372
acc: 0.5842839043554395
